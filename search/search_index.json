{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FORCE AI","text":"<p>FORCE AI (Fast Optimization for Resource-Constrained and Efficient AI Inference) is an open-source framework designed for educational purposes, demonstrating how to build an optimized AI accelerator for a specific model from scratch. This project covers the full development workflow, including model training and quantization, performance profiling, hardware architecture design, compiler construction, and runtime integration, helping learners understand and implement end-to-end AI inference acceleration.</p> <p>The framework is designed to be educational and extensible, with a clean and modular structure. It is suitable for course projects, self-learning, or as a foundation for advanced research. All tools and modules are based on open-source technologies, including PyTorch, TVM, Verilator, and ONNX. It also features a custom-designed power-of-2 symmetric quantization algorithm and a Python-based performance model, combining RTL simulation and hardware design space exploration to support a design-driven development approach for AI accelerators.</p>"},{"location":"#who-is-this-for","title":"Who is this for?","text":"<ul> <li>Beginners, graduate students, and senior undergraduates in the field of AI accelerators</li> <li>Developers interested in bridging AI inference and hardware design</li> <li>Learners who want to build their own accelerator environment at low cost</li> </ul>"},{"location":"#what-you-will-learn","title":"What you will learn","text":"<ul> <li>How to train and quantize neural network models for hardware deployment</li> <li>How to design and simulate a CNN accelerator based on the Eyeriss architecture</li> <li>How to perform RTL-level simulation and verification using Verilator</li> <li>How to build a simple inference runtime and use the TVM compiler for code generation</li> <li>How to derive hardware design specifications from front-end model requirements</li> </ul>"},{"location":"developers/","title":"Developer Guides","text":"<p>Warning</p> <p>This page is a work in progress. Please check back later for updates.</p>"},{"location":"quick-start/","title":"Quick Start","text":"<p>Warning</p> <p>This page is a work in progress. Please check back later for updates.</p>"},{"location":"2025sp/lab1/","title":"Lab 1 - AI Model Design and Quantization","text":""},{"location":"2025sp/lab1/#lab-11-ai-model-design","title":"Lab 1.1 - AI Model Design","text":""},{"location":"2025sp/lab1/#1-targeting-task-and-dataset","title":"1. Targeting Task and Dataset","text":"<p>In the labs of this course, we will perform an image classification task on the CIFAR-10 dataset. CIFAR-10 consists of 60,000 colored images (32\u00d732 pixels, RGB channels) across 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. It is split into 50,000 training and 10,000 test images.</p> <p>Many convolutional neural networks (CNNs) have been evaluated on CIFAR-10, making it a valuable dataset for learning image classification techniques.</p>"},{"location":"2025sp/lab1/#2-model-architecture-design","title":"2. Model Architecture Design","text":"<p>Below, we introduce several commonly used operators in convolutional neural networks (CNNs). In the assignment of this lab, you will be asked to design your own model architecture using these operators and apply it to the CIFAR-10 image classification task.</p> <p>In the following formulas, we present:</p> \\[ \\begin{align} N &amp;= \\text{batch size} \\\\ C_{\\text{in}} &amp;= \\text{number of input channels} \\\\ C_{\\text{out}} &amp;= \\text{number of output channels} \\\\ H &amp;= \\text{height of the tensor} \\\\ W &amp;= \\text{width of the tensor} \\end{align} \\]"},{"location":"2025sp/lab1/#convolution","title":"Convolution","text":"<p>Convolutions are widely used for feature extraction in computer vision tasks. A convolution operator can defined by its size, stride, padding, dilation, etc.</p> <p></p> <p>Here is the mathematical representation:</p> \\[ \\begin{align} \\text{out}(N_i, C_{\\text{out}, j}) &amp;= \\text{bias}(C_{\\text{out}, j}) + \\sum_{k=0}^{C_{\\text{in}}-1} \\text{weight}(C_{\\text{out}, j}, k) \\star \\text{input}(N_i, k) \\\\ \\end{align} \\] <p>In AlexNet, 11\u00d711, 5\u00d75, and 3\u00d73 Conv2D layers are used. However, using multiple convolution kernel sizes in a model increases the complexity of hardware implementation. Therefore, in this lab, we will use only 3\u00d73 Conv2D with a padding size of 1 and a stride of 1, following the approach in VGGNet. With these settings, the spatial dimensions of the output feature map will remain the same as those of the input feature map.</p>"},{"location":"2025sp/lab1/#linear","title":"Linear","text":"<p>Fully-connected layers, also known as linear layers or dense layers, connect every input neuron to every output neuron and are commonly used as the classifier in a neural network.</p> <p></p> \\[ \\mathbf{y} = \\mathbf{x} \\mathbf{W}^{T} + \\mathbf{b} \\]"},{"location":"2025sp/lab1/#rectified-linear-unit-relu","title":"Rectified Linear Unit (ReLU)","text":"<p>ReLU is an activation function, which sets all negative values to zero while keeping positive values unchanged. It introduces non-linearity to the model, helping neural networks learn complex patterns while mitigating the vanishing gradient problem compared to other activation functions like sigmoid, hyperbolic tangent, etc.</p> <p> {width=75%}</p> \\[ \\operatorname{ReLU}(x) = \\cases{ 0 \\text{, if } x &lt; 0 \\\\ x \\text{, otherwise} } \\]"},{"location":"2025sp/lab1/#max-pooling","title":"Max Pooling","text":"<p>Max pooling is a downsampling operation commonly used in convolutional neural networks (CNNs) to reduce the spatial dimensions of feature maps while preserving important features. In the following formulas, we present the typical 2D max pooling operation.</p> <p> {width=80%}</p> \\[ out(N_i, C_j, h, w) = \\max\\limits_{m=0,\\dots,k_H-1} \\max\\limits_{n=0,\\dots,k_W-1} \\text{input}(N_i, C_j, \\text{stride}[0] \\times h + m, \\text{stride}[1] \\times w + n) \\]"},{"location":"2025sp/lab1/#batch-normalization","title":"Batch Normalization","text":"<p>Batch Normalization (BN) is a technique used in deep learning to stabilize and accelerate training by normalizing the inputs of each layer. It reduces internal covariate shift, making optimization more efficient.</p> \\[ \\begin{align} y &amp;= \\frac{x - \\mathbb{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}} \\cdot \\gamma + \\beta \\\\ \\text{where} \\\\ \\mathbb{E}[\\cdot] &amp;= \\text{\"the mean\"} \\\\ \\text{Var}[\\cdot] &amp;= \\text{\"the variance\"} \\\\ \\gamma, \\beta &amp;= \\text{\"learnable parameters\"} \\in \\mathbb{R}^{C_\\text{out}} \\\\ \\epsilon &amp;= \\text{\"small constant to avoid division by zero\"} \\\\ \\end{align} \\] <p>The learnable parameters \\(\\gamma\\) and \\(\\beta\\) are updated during training but remains fixed during inference.</p> <ol> <li>During forward propagation of training: the mean and variance are calculated from the current mini-batch.</li> <li>During backward propagation of training: \\(\\gamma\\) and \\(\\beta\\) are updated with the gradients \\(\\frac{\\partial L}{\\partial \\gamma}\\) and \\(\\frac{\\partial L}{\\partial \\beta}\\) respectively</li> <li>During inference: \\(\\gamma\\) and \\(\\beta\\) are fixed</li> </ol> <p>As the following figure shows, the batch normalization are applied per channel. That is, the mean and variance are computed over all elements in the batch (\\(N\\)) and spatial dimension (\\(H\\) and \\(W\\)). Each channel has independent \\(\\gamma_c\\) and \\(\\beta_c\\) parameters.</p> <p></p>"},{"location":"2025sp/lab1/#3-model-training-and-hyperparameter-tuning","title":"3. Model Training and Hyperparameter Tuning","text":"<p>You can use these techniques to improve the accuracy and efficiency of model training. - Learning Rate Scheduling and Optimizer - Weight Initialization - Regularization (L1, L2) - Softmax and Cross Entropy Loss     - They are commonly used in image classification tasks.</p>"},{"location":"2025sp/lab1/#lab-12-ai-model-quantization","title":"Lab 1.2 - AI Model Quantization","text":""},{"location":"2025sp/lab1/#why-quantization","title":"Why Quantization?","text":"<p>In the beginning, we need to discuss the different data types that can be used for computation and their associated hardware costs.</p>"},{"location":"2025sp/lab1/#number-representation","title":"Number Representation","text":""},{"location":"2025sp/lab1/#integers","title":"Integers","text":"Integer Format Length (bit) Value Range INT32 32 -2,147,483,648 ~ 2,147,483,647 UINT32 32 0 ~ 4,294,967,295 INT16 16 -32768 ~ 32767 UINT16 16 0 ~ 65535 INT8 8 -128 ~ 127 UINT8 8 0 ~ 255"},{"location":"2025sp/lab1/#floating-point-numbers","title":"Floating Point Numbers","text":"Floating Point Format Length (bits) Exponent (E) Mantissa (M) Applications FP64 64 11 52 High precision computing, scientific simulations FP32 32 8 23 General computing, 3D rendering, machine learning TF32 32 8 10 NVIDIA proposed, AI training acceleration FP16 16 5 10 Low-power AI training and inference BF16 16 8 7 AI training, better compatibility with FP32 FP8-E4M3 8 4 3 Low-precision AI inference FP8-E5M2 8 5 2 Low-precision AI inference"},{"location":"2025sp/lab1/#hardware-energyarea-cost-on-different-numeric-operation","title":"Hardware Energy/Area Cost on Different Numeric Operation","text":"<ul> <li>When training a model in Python, we often use FP32 precision in default, as it provides a good performance and accuracy. However, we can observe that FP32 comes with a significant computational cost, making it less efficient in terms of hardware usage and power consumption.</li> <li>To reduce hardware costs (size of storage and computation), we can apply quantization, which converts high-precision numbers (e.g. FP32) into lower-precision formats (e.g. INT8).</li> </ul>"},{"location":"2025sp/lab1/#floating-point-arithmetic-is-more-computationally-expensive-due-to-the-overhead-of-mantissa-alignment-and-mantissa-multiplications","title":"Floating-point arithmetic is more computationally expensive due to the overhead of mantissa alignment and mantissa multiplications","text":""},{"location":"2025sp/lab1/#quantization-schemes","title":"Quantization Schemes","text":"<ul> <li>In the previous discussion, we introduced various data type formats and observed that high-precision computations are computationally expensive. To reduce this cost, we can apply quantization, reducing hardware complexity and improving efficiency.</li> <li>In this section, we will guide you through different quantization schemes, and delve into the calibration implementation. By the end, you will have a clear understanding of how to effectively apply PTQ to optimize model performance while maintaining accuracy.</li> </ul>"},{"location":"2025sp/lab1/#uniformnon-uniform","title":"Uniform/Non-uniform","text":""},{"location":"2025sp/lab1/#uniform-quantization-linear-quantization","title":"Uniform Quantization (Linear Quantization)","text":"<ul> <li>Uniform quantization maps input values to equally spaced discrete levels. </li> </ul> \\[ \\begin{align} q &amp;= \\mathbf{Q}(r) = \\text{clip}(\\left\\lfloor \\frac{r}{s} \\right\\rceil + z, q_\\min, q_\\max) \\\\ r &amp;\\approx \\mathbf{D}(q) = s(q - z)  \\\\ \\\\ \\text{where} \\\\ \\mathbf{Q} &amp;= \\text{\"the quantization operator\"}\\\\ \\mathbf{D} &amp;= \\text{\"the de-quantization operator\"}\\\\ q &amp;= \\text{\"quantized value\"}\\\\ r &amp;= \\text{\"real value\"}\\\\ s &amp;= \\text{\"the real difference between quantized steps\"} \\in \\mathbb{R} \\\\ z &amp;= \\text{\"the quantized value mapping to real number 0\"} \\in \\mathbb{Z} \\end{align} \\] <p>The precise definition of the scaling factor \\(s\\) and zero point \\(z\\) varies with which quantization scheme is utilzed.</p>"},{"location":"2025sp/lab1/#non-uniform-quantization-logarithmicpower-of-2-quantization","title":"Non-Uniform Quantization (Logarithmic/power-of-2 Quantization)","text":"<ul> <li>Non-uniform quantization maps input value to varying step sizes. </li> </ul> \\[ \\begin{align} q &amp;= \\mathbf{Q}(r) = \\text{sign}(r) \\cdot \\text{clip}(\\left\\lfloor -\\log_2{\\frac{|r|}{s}} \\right\\rceil + z, q_\\min, q_\\max) \\\\ r &amp;\\approx \\mathbf{D}(q) = s \\cdot 2^{z-q} \\end{align} \\]"},{"location":"2025sp/lab1/#symmetricasymmetric","title":"Symmetric/Asymmetric","text":""},{"location":"2025sp/lab1/#asymmetricaffine-uniform-quantiztaion","title":"Asymmetric/Affine Uniform Quantiztaion","text":"<ul> <li> <p>Asymmetric quantization allows a nonzero zero point to better represent skewed data distributions at the cost of additional processing. </p> </li> <li> <p>asymmetric: \\(\\beta \\ne -\\alpha\\)</p> </li> </ul> \\[ \\begin{align} s &amp;= \\frac{\\beta - \\alpha}{q_\\max - q_\\min} \\in \\mathbb{R} \\\\ z &amp;= q_\\min - \\left\\lfloor \\frac{\\alpha}{s} \\right\\rceil \\in \\mathbb{Z} \\end{align} \\]"},{"location":"2025sp/lab1/#symmetric-uniform-quantiztaion","title":"Symmetric Uniform Quantiztaion","text":"<ul> <li> <p>Symmetric quantization uses a zero-centered scale for positive and negative values </p> </li> <li> <p>symmetric: \\(\\beta = -\\alpha\\)</p> </li> </ul> \\[ \\begin{align} s &amp;= \\frac{2 \\max(|\\alpha|, |\\beta|)}{q_\\max - q_\\min} \\in \\mathbb{R} \\\\ z &amp;= \\left\\lceil \\frac{q_\\max + q_\\min}{2} \\right\\rceil = \\begin{cases} 0 \\text{, if signed} \\\\ 128 \\text{, if unsigned} \\end{cases} \\in \\mathbb{Z} \\end{align} \\]"},{"location":"2025sp/lab1/#comparison","title":"Comparison","text":"<p>Compared to asymmetric quantization, symmetric quantization is more hardware-friendly, which eliminates the cross terms of quantized matrix multiplication</p> <ol> <li>Faster Matrix Multiplication With symmetric quantization, all data is scaled using the same factor, and the zero-point is set to 0. This allows direct execution of integer-only operations: $$ \\begin{align} C_q &amp;= A_q \\times B_q \\end{align} $$ In contrast, with asymmetric quantization, matrix multiplication becomes more complex because the zero-point must be accounted for: $$ \\begin{align} C_q &amp;= (A_q-Z_A) \\times (B_q-Z_B) \\end{align} $$ This introduces additional subtraction operations, increasing the computational cost.</li> </ol>"},{"location":"2025sp/lab1/#clipping-range","title":"Clipping Range","text":"<ul> <li>clipping range \\(= [\\alpha, \\beta]\\)</li> <li>dynamic range \\(= [r_\\min, r_\\max]\\)</li> </ul>"},{"location":"2025sp/lab1/#min-max-clipping","title":"Min-Max Clipping","text":"<p>For min-max clipping, clipping range = dynamic range</p> \\[ \\begin{align} \\alpha &amp;= r_\\min \\\\ \\beta &amp;= r_\\max \\end{align} \\]"},{"location":"2025sp/lab1/#moving-average-clipping","title":"Moving Average Clipping","text":"\\[ \\begin{align} \\alpha_t &amp;= \\begin{cases}     r_\\min &amp;\\text{if } t=0 \\\\     c\\ r_\\min + (1-c) \\alpha_{t-1} &amp;\\text{otherwise} \\end{cases} \\\\ \\beta_t &amp;= \\begin{cases}     r_\\max &amp;\\text{if } t=0 \\\\     c\\ r_\\max + (1-c) \\beta_{t-1}  &amp;\\text{otherwise} \\end{cases} \\\\ \\end{align} \\]"},{"location":"2025sp/lab1/#percentile-clipping","title":"Percentile Clipping","text":"<p>Percentile</p> \\[ \\begin{align} \\alpha &amp;= P_{5}(r)\\\\ \\beta &amp;= P_{95}(r) \\end{align} \\]"},{"location":"2025sp/lab1/#reduced-rangefull-range","title":"Reduced Range/Full Range","text":"<p>For \\(b\\)-bit signed integer quantization</p> \\(q_\\min\\) \\(q_\\max\\) Full range \\(-2^{b-1}\\) \\(2^{b-1}-1\\) Reduce range \\(-2^{b-1}+1\\) \\(2^{b-1}-1\\) <p>For \\(b\\)-bit unsigned integer quantization</p> \\(q_\\min\\) \\(q_\\max\\) Full range \\(0\\) \\(2^b-1\\) Reduce range \\(0\\) \\(2^b-2\\) <p>For example, the integer representation of an 8-bit signed integer quantized number with reduce range is in the interval \\([-127, 127]\\).</p>"},{"location":"2025sp/lab1/#calibration-algorithms","title":"Calibration Algorithms","text":"<p>The process of choosing the input clipping range is known as calibration. The simplest technique (also the default in PyTorch) is to record the running minimum and maximum values and assign them to \\(\\alpha\\) and \\(\\beta\\). In PyTorch, Observer modules (code) collect statistics on the input values and calculate the qparams <code>scale</code> and <code>zero_point</code> . Different calibration schemes result in different quantized outputs, and it\u2019s best to empirically verify which scheme works best for your application and architecture. -- PyTorch</p>"},{"location":"2025sp/lab1/#weight-only-quantization","title":"Weight-only Quantization","text":"<p>In weight-only quantization, only weights are and quantized, while activations remain in full-precision (FP32).</p>"},{"location":"2025sp/lab1/#static-quantization","title":"Static Quantization","text":"<p>In static quantization approach, both weight's and activation's clipping range are pre-calculated and the resulting quantization parameters remain fixed during inference. This approach does not add any computational overhead during runtime.</p>"},{"location":"2025sp/lab1/#dynamic-quantization","title":"Dynamic Quantization","text":"<p>In dynamic quantization, activation's clipping range as well as the quantization parameters are dynamically calculated for each activation map during runtime. This approach requires run-time computation of the signal statistics (min, max, percentile, etc.) which can have a very high overhead.</p> Weight-only quantization Static quantization Dynamic quantization calibrate on weights before inference before inference before inference quantize on weights before inference before inference before inference calibrate on activations no before inference during inference quantize on activations no during inference during inference runtime overhead of quantization no low high"},{"location":"2025sp/lab1/#ptqqat","title":"PTQ/QAT","text":""},{"location":"2025sp/lab1/#ptq-post-training-quantization","title":"PTQ (Post-Training Quantization)","text":"<p> All the weights and activations quantization parameters are determined without any re-training of the NN model. In this assignment, we will use this method to perform quantization on our model.</p>"},{"location":"2025sp/lab1/#qat-quantization-aware-training","title":"QAT (Quantization-Aware Training)","text":"<p> Quantization can slightly alter trained model parameters, shifting them from their original state. To mitigate this, the model can be re-trained with quantized parameters to achieve better convergence and lower loss.</p>"},{"location":"2025sp/lab1/#straight-through-estimator-ste","title":"Straight-Through Estimator (STE)","text":"<p>In QAT, since quantization is non-differentiable, standard backpropagation cannot compute gradients. The STE in Quantization-Aware Training (QAT) allows gradients to bypass this step, enabling the model to be trained as if it were using continuous values while still applying quantization constraints.</p> <p></p>"},{"location":"2025sp/lab1/#quantization-errors","title":"Quantization Errors","text":"<p>A metric to evaluate the numerical error introduced by quantization.</p> \\[ \\mathbf{L}(r, \\hat r) \\text{ , where } r \\approx \\hat r = \\mathbf{D}(\\mathbf{Q}(r)) \\] <p>where \\(r\\) is the original tensor, \\(\\hat r = \\mathbf{D}(\\mathbf{Q}(r))\\) is the tensor after quantization and dequantization.</p>"},{"location":"2025sp/lab1/#mean-square-error-mse","title":"Mean-Square Error (MSE)","text":"<p>The most commonly-used metric for quantization error.</p> \\[ \\mathbf{L}(r, \\hat r) = \\frac{1}{N} \\sum_{i=1}^N (r_i - \\hat r_i)^2 \\]"},{"location":"2025sp/lab1/#other-quantization-error-metrics","title":"Other Quantization Error Metrics","text":"<ul> <li>Cosine distance</li> <li>Pearson correlation coefficient</li> <li>Hessian-guided metric</li> </ul>"},{"location":"2025sp/lab1/#fake-quantizationinteger-only-quantization","title":"Fake Quantization/Integer-only Quantization","text":""},{"location":"2025sp/lab1/#fake-quantization-simulated-quantization","title":"Fake Quantization (Simulated Quantization)","text":"<p>In simulated quantization, the quantized model parameters are stored in low-precision, but the operations (e.g. matrix multiplications and convolutions) are carried out with floating point arithmetic.</p> <p>Therefore, the quantized parameters need to be dequantized before the floating point operations</p>"},{"location":"2025sp/lab1/#integer-only-quantization","title":"Integer-only Quantization","text":"<p>In integer-only quantization, all the operations are performed using low-precision integer arithmetic.</p>"},{"location":"2025sp/lab1/#hardware-friendly-design","title":"Hardware-Friendly Design","text":""},{"location":"2025sp/lab1/#dyadic-quantization","title":"Dyadic Quantization","text":"<p>A type of integer-only quantization that all of the scaling factors are restricted to be dyadic numbers defined as:</p> \\[ s \\approx \\frac{b}{2^c} \\] <p>where \\(s\\) is a floating point number, and \\(b\\) and \\(c\\) are integers.</p> <p>Dyadic quantization can be implemented with only bit shift and integer arithmetic operations, which eliminates overhead of expensive dequantization and requantization.</p>"},{"location":"2025sp/lab1/#power-of-two-uniformscale-quantization-similar-to-dyadic-quantization","title":"Power-of-Two Uniform/Scale Quantization (Similar to Dyadic Quantization)","text":"<p>Same concept as dyadic quantization, we replace the numerator \\(b\\) with \\(1\\). This approach improves hardware efficiency since it further eliminates the need for the integer multiplier.</p> \\[ s \\approx \\frac{1}{2^c} \\] <p>:::warning Power-of-Two Uniform/Scale Quantization constrains the scaling factor to a power-of-two value, enabling efficient computation through bit shifts, while Power-of-Two (Logarithmic) Quantization directly quantizes data to power-of-two values, reducing multiplications to simple bitwise operations. :::</p>"},{"location":"2025sp/lab1/#derivation-of-quantized-mac","title":"Derivation of Quantized MAC","text":"<p>In order to simplify the hardware implementation, we use layerwise symmetric uniform quantization for all layers.</p> <p>Here are the data types for inputs, weights, biases, outputs, and partial sums:</p> input/output weight bias/psum Data type uint8 int8 int32 <p>Note that the scaling factor of bias is the product of input's scale and weight's scale. And rounding method is truncation instead of round-to-nearest.</p> \\[ \\tag{1} \\begin{align} \\bar x &amp;= \\text{clamp} \\left( \\left\\lfloor \\frac{x}{s_x} \\right\\rceil + 128, 0, 255 \\right) \\in \\mathbb{Z}_\\text{uint8}^{\\dim(x)} \\\\ \\bar w &amp;= \\text{clamp} \\left( \\left\\lfloor \\frac{w}{s_w} \\right\\rceil, -128, 127 \\right) \\in \\mathbb{Z}_\\text{int8}^{\\dim(w)} \\\\ \\bar b &amp;= \\text{clamp} \\left( \\left\\lfloor \\frac{b}{s_x s_w} \\right\\rfloor, -2^{31}, 2^{31}-1 \\right) \\in \\mathbb{Z}_\\text{int32}^{\\dim(b)} \\\\ \\bar y &amp;= \\text{clamp} \\left( \\left\\lfloor \\frac{y}{s_y} \\right\\rceil + 128, 0, 255 \\right) \\in \\mathbb{Z}_\\text{uint8}^{\\dim(y)} \\end{align} \\] <p>The notation \\(\\mathbb{Z}^N\\) denotes a vector space of dimension \\(N\\) where all elements (or components) are integers. See also Cartesian product.</p> <p>where the scaling factors are calaulated by</p> \\[ \\tag{2} \\begin{align} s_x = \\frac{2 \\max(|x_\\min|, |x_\\max|)|}{255} \\in \\mathbb{R}_\\text{float32} \\\\ s_w = \\frac{2 \\max(|w_\\min|, |w_\\max|)|}{255} \\in \\mathbb{R}_\\text{float32} \\\\ s_y = \\frac{2 \\max(|y_\\min|, |y_\\max|)}{255} \\in \\mathbb{R}_\\text{float32} \\end{align} \\] <p>The original values can be approximated by dequantizing the quantized numbers.</p> \\[ \\tag{3} \\begin{align} x &amp;\\approx s_x (\\bar x - 128) \\\\ w &amp;\\approx s_w \\bar w \\\\ b &amp;\\approx s_x s_w \\bar b \\\\ y &amp;\\approx s_y (\\bar y - 128) \\end{align} \\]"},{"location":"2025sp/lab1/#quantized-linear-layer-with-relu","title":"Quantized Linear Layer with ReLU","text":"<p>Rectified linear unit (ReLU) is one of the most commonly-used activation functions due to its simplicity.</p> \\[ \\text{ReLU}(x) = \\max(x, 0) = \\begin{cases} x, ~~\\text{if} ~ x &gt; 0 \\\\ 0, ~~\\text{otherwise} \\end{cases} \\] <p>Linear layer:</p> \\[ y_i = \\text{ReLU}(b_i + \\sum_j x_j \\cdot w_{ji}) \\] \\[ s_y (\\bar y_i - 128) = \\text{ReLU}(s_x s_w (\\bar b_i + \\sum_j (\\bar x_j - 128) \\cdot \\bar w_{ji})) \\] <p>The scaling factors \\(s_x\\), \\(s_w\\), and \\(s_y\\) are typically in \\([0, 1]\\), which doesn't affect the result of ReLU.</p> \\[ \\tag{5} \\bar y_i = \\underbrace{     \\frac{s_x s_w}{s_y}     \\overbrace{         \\text{ReLU}(\\bar b_i + \\sum_j (\\bar x_j - 128) \\cdot \\bar w_{ji})     }^{\\text{only int32 operations}}}_{\\text{float32 operations involved} } + 128 \\]"},{"location":"2025sp/lab1/#hardware-friendly-design_1","title":"Hardware-Friendly Design","text":""},{"location":"2025sp/lab1/#power-of-two-quantization","title":"Power-of-Two Quantization","text":"<p>With \\(b = 1\\) in dyadic quantization, we further get power-of-two quantization:</p> \\[ s \\approx \\frac{1}{2^c} = 2^{-c} \\text{ , where } c \\in \\mathbb{Z} \\] <p>The matrix multiplication can be approximated as:</p> \\[ \\tag{6} \\begin{align} \\bar y_i &amp;\\approx 2^{-(c_x + c_w - c_y)} \\text{ReLU}(\\bar b_i + \\sum_j (\\bar x_j - 128) \\cdot \\bar w_{ji}) + 128 \\\\ &amp;= \\left( \\text{ReLU}(\\bar b_i + \\sum_j (\\bar x_j - 128) \\cdot \\bar w_{ji}) \\gg (c_x + c_w - c_y) \\right) + 128 \\\\ \\end{align} \\] <p>We can use shifting to replace multiplication and division when applying a scaling factor.</p>"},{"location":"2025sp/lab1/#derivation-of-batch-normalization-folding","title":"Derivation of Batch Normalization Folding","text":"<p>During inference, batch normalization (BN) can be fused with Conv2d or Linear layers to improve inference efficiency, reduce memory access, and increase computational throughput. This also simplifies the hardware implementation in Lab3 by eliminating the need for separate BN computation. The derivation is as follows.</p> <p>Consider a batch normalization (BN) layer expressed by the following equation:</p> \\[ \\tag{1} z_c = \\frac{y_c - \\mu_c}{\\sqrt{\\sigma_c^2 + \\epsilon}} \\cdot \\gamma_c + \\beta_c \\] <p>where:</p> <ul> <li>\\(y_c \\in \\mathbb{R}^{H \\times W}\\): the \\(c\\)-th channel of the input tensor (assuming batch size \\(N = 1\\))</li> <li>\\(z_c \\in \\mathbb{R}^{H \\times W}\\): the \\(c\\)-th channel of the output tensor (assuming batch size \\(N = 1\\))</li> <li>\\(\\mu_c \\in \\mathbb{R}\\): mean value of input activations along the \\(c\\)-th channel</li> <li>\\(\\sigma_c^2 \\in \\mathbb{R}\\): variance of input activations along the \\(c\\)-th channel</li> <li>\\(\\gamma_c \\in \\mathbb{R}\\) and \\(\\beta_c \\in \\mathbb{R}\\): BN parameters for the \\(c\\)-th channel</li> <li>\\(\\epsilon \\in \\mathbb{R}\\): a small value for numerical stability during computation</li> </ul> <p>Expanding Eq. 1, we obtain:</p> \\[ z_c = \\left( \\frac{\\gamma_c}{\\sqrt{\\sigma_c^2 + \\epsilon}} \\right) \\cdot y_c + \\left( \\beta_c - \\frac{\\gamma_c \\mu_c}{\\sqrt{\\sigma_c^2 + \\epsilon}} \\right) \\] <p>Assuming that the numerical distribution during inference is the same as in the training set, the statistics \\(\\mu_c\\) and \\(\\sigma_c^2\\) obtained during training are considered fixed values during inference. These values can then be fused into the preceding Conv2d or Linear layer.</p> <p>For example, consider a Linear layer:</p> \\[ \\tag{2} y_c = b_c + \\sum_i x_i \\cdot w_{ic} \\] <p>where the output \\(y\\) is normalized by BatchNorm to obtain \\(z\\). Substituting Eq. 2 into Eq. 1:</p> \\[ \\begin{align} z_c &amp;= \\left( \\frac{\\gamma_c}{\\sqrt{\\sigma_c^2 + \\epsilon}} \\right) \\cdot \\left( b_c + \\sum_i x_i \\cdot w_{ic} \\right) + \\left( \\beta_c - \\frac{\\gamma_c \\mu_c}{\\sqrt{\\sigma_c^2 + \\epsilon}} \\right) \\\\ &amp;= \\left( \\sum_i x_i \\cdot \\frac{\\gamma_cw_{ic}}{\\sqrt{\\sigma_c^2 + \\epsilon}} \\right) + \\left( \\beta_c - \\frac{\\gamma_c \\mu_c}{\\sqrt{\\sigma_c^2 + \\epsilon}} + \\frac{\\gamma_c b_c}{\\sqrt{\\sigma_c^2 + \\epsilon}} \\right) \\\\ &amp;\\triangleq \\sum_i x_i \\cdot w_{ic}' + b_c' \\end{align} \\] <p>After rearranging, we observe that the Linear + BN operation can be expressed as a new Linear operation with updated weights and biases:</p> \\[ \\tag{3} \\begin{align} w_c' &amp;= \\frac{\\gamma_c}{\\sqrt{\\sigma_c^2 + \\epsilon}} \\cdot w_c \\\\ b_c' &amp;= \\frac{\\gamma_c}{\\sqrt{\\sigma_c^2 + \\epsilon}} (b_c - \\mu_c) + \\beta_c \\end{align} \\]"},{"location":"2025sp/lab1/#quantization-in-practice","title":"Quantization in Practice","text":"<p>In this section, we will demonstrate how to perform quantization with PyTorch framework with a simple yet comprehensive example.</p> <p></p> <p>Let's discuss the quantization process using PyTorch step by step:</p> <ol> <li>Calibration Data</li> <li>Pre-trained Model</li> <li>Customize Quantization Scheme</li> <li>Operator Fusion</li> <li>Insert Observer</li> <li>Calibration</li> <li>Quantization</li> </ol>"},{"location":"2025sp/lab1/#1-calibration-data","title":"1. Calibration Data","text":"<p>During calibration, only a small amount of data is required. Therefore, the batch size is set to 1 <pre><code>dataset = '{DATASET}'\nbackend = '{Quantization_scheme}'\nmodel_path = 'path/to/your/model'\n*_, test_loader = DATALOADERS[dataset](batch_size=1)\n</code></pre></p>"},{"location":"2025sp/lab1/#2-pre-trained-model","title":"2. Pre-trained Model","text":"<p>Load model weights trained from Lab 1.1.</p> <pre><code>model = network(in_channels, in_size).eval().cpu()\nmodel.load_state_dict(torch.load(model_path))\n</code></pre>"},{"location":"2025sp/lab1/#3-customize-quantization-scheme","title":"3. Customize Quantization Scheme","text":"<p>Configure Quantization - <code>model = tq.QuantWrapper(model)</code> Converts the model into a format suitable for quantization. <pre><code>model = tq.QuantWrapper(model)\nmodel.qconfig = CustomQConfig[{Your_Quantization_Scheme_in_CustomQConfig_class}].value\nprint(f\"Quantization backend: {model.qconfig}\")\n</code></pre></p> <ul> <li> <p><code>QConfig</code> qconfig.py<pre><code>class CustomQConfig(Enum):\n    POWER2 = torch.ao.quantization.QConfig(\n        activation=PowerOfTwoObserver.with_args(\n            dtype=torch.quint8, qscheme=torch.per_tensor_symmetric\n        ),\n        weight=PowerOfTwoObserver.with_args(\n            dtype=torch.qint8, qscheme=torch.per_tensor_symmetric\n        ),\n    )\n    DEFAULT = None\n</code></pre> The <code>torch.ao.quantization.QConfig</code> class helps define custom quantization schemes by specifying:</p> </li> <li> <p>How activations should be quantized</p> </li> <li>How weights should be quantized</li> </ul> <p>These are parameters tells your custom observer (<code>class PowerOfTwoObserver(...)</code>) how to calculate <code>scale</code> and <code>zero point</code>.</p> Parameter Description <code>dtype=torch.quint8</code> unsigned 8-bit quantization <code>dtype=torch.qint8</code> signed 8-bit quantization <code>qscheme=torch.per_tensor_symmetric</code> select symmetric quantization in one tensor <code>qscheme=torch.per_tensor_affine</code> select asymmetric quantization in one tensor"},{"location":"2025sp/lab1/#4-operator-fusion","title":"4. Operator Fusion","text":"<p>Operator fusion is a technique that combines multiple operations into a single efficient computation to reduce memory overhead and improve execution speed.</p> <p>Module fusion combines multiple sequential modules (eg: [Conv2d, BatchNorm, ReLU]) into one. Fusing modules means the compiler needs to only run one kernel instead of many; this speeds things up and improves accuracy by reducing quantization error. -- PyTorch</p> <p>Common fusions include: - Conv2D + BatchNorm + ReLU \u2192 Fused into a single Conv2D operation. - Conv2D + ReLU \u2192 Fused into a single Conv2D operation. - Linear + ReLU \u2192 Fused into a single linear transformation with an activation function.</p> <p>If you want to performed module fusion to improve performance, you should call the following API before calibration.</p> <pre><code>tq.fuse_modules(model: nn.Module, modules_to_fuse: list[str], inplace=False) -&gt; nn.Module\n</code></pre> <p>You can see the reference for more details.</p>"},{"location":"2025sp/lab1/#5-insert-observer","title":"5. Insert Observer","text":"<ul> <li><code>tq.prepare(model, inplace=True)</code> : Inserts fake quantization modules to track activations. <pre><code>tq.prepare(model, inplace=True)\n</code></pre></li> </ul>"},{"location":"2025sp/lab1/#6-calibration","title":"6. Calibration","text":"<p>Define calibration function first:</p> <p><pre><code>def calibrate(model, loader, device=DEFAULT_DEVICE):\n    model.eval().to(device)\n    for x, _ in loader:\n        model(x.to(device))\n        break\n</code></pre> Apply calibration by directly call the above function after inserting the observer via <code>tq.prepare</code>.</p> <pre><code>calibrate(model, test_loader, \"cpu\")\n</code></pre> <p>This runs one batch of data (previous step) through the model to collect activation statistics.</p>"},{"location":"2025sp/lab1/#7-quantization","title":"7. Quantization","text":"<p>Use <code>tq.convert(model.cpu(), inplace=True)</code> to convert the model into a fully-quantized model.</p> <pre><code>tq.convert(model.cpu(), inplace=True)\n</code></pre> <p>Finally, save your quantized model with given filename</p> <pre><code>save_model({Your Model}, \"filename.pt\")\n</code></pre>"},{"location":"2025sp/lab1/#reference","title":"Reference","text":"<ul> <li>A Survey of Quantization Methods for Efficient Neural Network Inference</li> <li>PyTorch Quantization</li> <li>Pooling Methods in Deep Neural Networks, a Review</li> <li>NVIDIA Deeplearning Performance Documents</li> </ul>"},{"location":"2025sp/lab1/#practice","title":"Practice","text":""},{"location":"2025sp/lab1/#1-train-a-vgg-like-model-using-cifar-10-dataset","title":"1. Train a VGG-like Model using CIFAR-10 dataset.","text":"<p>VGG is a classic CNN architecture used in computer vision. Compared to the previous CNNs, it only use 3x3 convolution, making it easy to be implmeneted and supported by existing and customized hardware accelerators.</p> <p>In this course, we are going to deploy a VGG-like model onto our custom hardware accelerator and complete an end-to-end inference of image recognition. In this lab, students are requested to implement a VGG-like model in PyTorch with only the following operators:</p> <ul> <li><code>Conv2d</code> with 3x3 kernel size, stride 1, and padding 1</li> <li><code>Linear</code></li> <li><code>ReLU</code></li> <li><code>MaxPool2d</code> with 2x2 kernel size and stride 2</li> <li><code>BatchNorm2d</code></li> </ul> Layer Type <code>Conv1</code> Conv2D (3 \u2192 64) <code>MaxPool</code> 2\u00d72 Pooling <code>Conv2</code> Conv2D (64 \u2192 192) <code>MaxPool</code> 2\u00d72 Pooling <code>Conv3</code> Conv2D (192 \u2192 384) <code>Conv4</code> Conv2D (384 \u2192 256) <code>Conv5</code> Conv2D (256 \u2192 256) <code>MaxPool</code> 2\u00d72 Pooling <code>Flatten</code> - <code>FC6</code> Linear (256*fmap_size\u00b2 \u2192 256) <code>FC7</code> Linear (256 \u2192 128) <code>FC8</code> Linear (128 \u2192 num_classes) <p>Students are required to design and train your model with only allowed operators using as few parameters as possible while ensuring the accuracy greater than 80%. You can use any training techniques and adjust the hyperparameters (e.g. learning rate tuning, optimizer, etc.) to achieve this goal.</p> <p>For full precision model, your model should achive the following metrics: - top-1 accuracy remains above 80% on the CIFAR-10 classification task</p>"},{"location":"2025sp/lab1/#2-quantize-the-vgg-like-model-as-int8-precision","title":"2. Quantize the VGG-like Model as INT8 Precision","text":"<p>Then, quantize the model to INT8 precision while preserving a high level of accuracy compared to the full-precision model.</p>"},{"location":"2025sp/lab1/#quantization-scheme","title":"Quantization scheme","text":"<p>Use the power-of-two uniform/scale, symmetric quantization we previously-mentioned to quantize your model. - Complete the QConfig observer setup for Post-Training Quantization (PTQ) calibration. - Reference: PyTorch QConfig documents</p> <p>For quantized model, your model should achive the following metrics:</p> <ul> <li>model size \\(&lt;\\) 4MB</li> <li>top-1 accuracy on CIFAR-10 \\(\\ge\\) 80%</li> <li>accuracy drop \\(\\le\\) 1% compared to your full-precision model</li> </ul>"},{"location":"2025sp/lab2/","title":"Lab 2 - Performance Modeling","text":""},{"location":"2025sp/lab2/#overview","title":"Overview","text":"<p>In this lab, we will perform workload analysis based on the model architecture from Lab 1, develop an analytical model to predict system performance, and investigate the optimal software and hardware configuration.</p>"},{"location":"2025sp/lab2/#lab-21-workload-analysis-and-network-parsing","title":"Lab 2.1 - Workload Analysis and Network Parsing","text":"<p>When implementing an AI accelerator, Workload Analysis and Network Parsing are critical steps, as they help designers understand the requirements of deep learning models and design efficient hardware architectures accordingly.</p> <p>In this lab, students are required to implement Network Parsing for PyTorch and ONNX models, transforming them into a format predefined by TAs for further analysis.</p>"},{"location":"2025sp/lab2/#workload-analysis-to-identify-the-performance-bottleneck","title":"Workload Analysis to Identify the Performance Bottleneck","text":"<p>The purpose of workload analysis is to understand the computational requirements and characteristics of the target application, in order to guide architecture design. The main task of this phase is to analyze the workload that the accelerator needs to execute, ensuring that the designed architecture aligns with the characteristics of the target application, and avoids designing hardware that is either unsuitable or over-engineered.</p> <p>In this phase, we will...</p> <ul> <li>Decide which computational patterns the AI accelerator should focus on, such as Conv acceleration or GEMM acceleration.</li> <li>Determine the architecture design, such as using systolic arrays or vector extensions, etc.</li> <li>Evaluating the memory architecture requirements, such as whether SRAM cache is needed or if there are constraints related to DRAM access.</li> </ul>"},{"location":"2025sp/lab2/#ratio-of-different-operations","title":"Ratio of Different Operations","text":"<p>Next, we take VGG-8 (5 layers of 3x3 Conv2d + 3 layers of Linear) as an example, and we perform profiling on the model's inference with PyTorch Profiler. Based on the CPU time, we list the top ten computations and obtain the following results:</p> Full-precision modelInt8-quantized model <pre><code>---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------\n                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------\n                  model_inference        12.90%     763.821us       100.00%       5.920ms       5.920ms             1\n                     aten::conv2d         0.39%      22.854us        67.44%       3.993ms     798.517us             5\n                aten::convolution         0.89%      52.872us        67.05%       3.970ms     793.947us             5\n               aten::_convolution         0.77%      45.697us        66.16%       3.917ms     783.372us             5\n                aten::thnn_conv2d         0.14%       8.547us        38.52%       2.281ms     570.170us             4\n       aten::_slow_conv2d_forward        34.62%       2.049ms        38.38%       2.272ms     568.034us             4\n         aten::mkldnn_convolution        26.46%       1.566ms        26.69%       1.580ms       1.580ms             1\n                 aten::batch_norm         0.19%      11.092us         6.30%     373.078us      74.616us             5\n     aten::_batch_norm_impl_index         0.31%      18.232us         6.11%     361.986us      72.397us             5\n          aten::native_batch_norm         5.10%     301.772us         5.74%     339.836us      67.967us             5\n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------\nSelf CPU time total: 5.920ms\n</code></pre> <pre><code>---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------\n                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------\n                  model_inference        30.99%     864.312us       100.00%       2.789ms       2.789ms             1\n           quantized::conv2d_relu        44.15%       1.231ms        46.34%       1.292ms     258.446us             5\n           quantized::linear_relu        10.34%     288.334us        10.90%     303.885us     151.943us             2\n                 aten::max_pool2d         0.23%       6.332us         4.05%     112.968us      37.656us             3\n       aten::quantized_max_pool2d         3.57%      99.674us         3.82%     106.636us      35.545us             3\n                quantized::linear         3.43%      95.745us         3.61%     100.594us     100.594us             1\n        aten::quantize_per_tensor         2.06%      57.541us         2.06%      57.541us      57.541us             1\n    aten::_empty_affine_quantized         1.48%      41.339us         1.48%      41.339us       3.180us            13\n                      aten::clone         1.12%      31.332us         1.34%      37.233us      18.617us             2\n                    aten::flatten         0.47%      13.105us         1.29%      35.889us      35.889us             1\n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------\nSelf CPU time total: 2.789ms\n</code></pre> <p>It can be observed that in the full-precision model, Conv2d accounts for more than 60% of the computation time, while in the quantized model, Conv2d still takes up over 40% of the CPU time. BatchNorm, Linear, and MaxPool consume relatively smaller portions of the time and are not the main computation bottlenecks. Based on this, we determined that the system design should focus on accelerating the convolutional layer (including Conv2d, BatchNorm, ReLU, and MaxPool) as the primary target.</p> <p>Tip</p> <p>If you have designed your own model architecture in Lab 1, you can also try running the above analysis and see what results you get.</p> <p>Common accelerators used for Conv2d have the following architectures:</p> <ul> <li>Systolic array (e.g. Google TPU)</li> <li>Eyeriss</li> <li>NVDLA</li> </ul> <p>In the following Labs, we will design the accelerator based on the Eyeriss architecture. Below is the system architecture diagram from the original Eyeriss paper.</p> <p></p> <p>Since the implementation of the Lab is for educational purposes, we will not be implementing the entire Eyeriss accelerator but will simplify it in line with the same principles. Overall, we will have a row-stationary dataflow PE array to perform Conv2d and ReLU computations. The parameters for BatchNorm can be pre-fused into the Conv2d weights and biases (the derivation process can be referenced in the Lab 1 lecture notes), so there will be no dedicated hardware support for BatchNorm. Finally, MaxPool is a memory-bound operation. By further integrating its dataflow with the Conv2d dataflow, we can reduce DRAM access and speed up the overall computation process. Therefore, we will design a module responsible for performing the MaxPool operation before the activation is written back to DRAM.</p> <p>In this assignment, you have to analyze your own model, and answer the questions in report.</p>"},{"location":"2025sp/lab2/#network-parsing-to-automate-the-analysis-workflow","title":"Network Parsing to Automate the Analysis Workflow","text":"<p>Since the hardware is designed by us and there are no existing performance evaluation tools to assist in the early stages of architecture design, we need to build our own performance model. This model will take a description of a model's computation as input and output the performance metrics that we care about.</p> <p>To better integrate with the design of the model algorithm and automate the process, while bridging the differences between various model formats, we will develop a network parser. This parser will extract the parameters needed for performance evaluation from different model weight formats (such as PyTorch and ONNX). This will facilitate performance analysis in the subsequent stages, allowing the decoupling of performance modeling and AI model design.</p>"},{"location":"2025sp/lab2/#defining-layer-information-class","title":"Defining Layer Information Class","text":"<p>First, we need to define the parameters required for the operators supported by the accelerator.</p> <p>In the <code>layer_info.py</code>, we provide a common interface through the <code>ShapeParam</code> to handle different shape-related parameters.</p> <pre><code>class ShapeParam(ABC):\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert the object's attributes to a dictionary.\"\"\"\n        return asdict(self)\n\n    @classmethod\n    def from_dict(cls, data: dict):\n        \"\"\"Create an instance of the class from a dictionary.\"\"\"\n        return cls(**data)\n</code></pre> <p><code>ShapeParam</code> is an abstract base class (ABC), which serves as the foundation for all shape parameter types. It provides two methods: * <code>to_dict(self)</code>: Converts the object's attributes into a dictionary. It uses <code>dataclasses.asdict()</code> to automatically convert all the fields of the dataclass into a dictionary format. * <code>from_dict(cls, data: dict)</code>: A class method that takes a dictionary and creates an instance of the class by unpacking the dictionary into the class constructor <code>(cls(**data))</code>.</p> <p>According to the original paper, the parameters for Conv2d are as follows:</p> <pre><code>@dataclass(frozen=True)\nclass Conv2DShapeParam(ShapeParam):\n    \"\"\"Follow the notation in the Eyeriss paper\"\"\"\n    N: int  # batch size\n    H: int  # input height\n    W: int  # input width\n    R: int  # filter height\n    S: int  # filter width\n    E: int  # output height\n    F: int  # output width\n    C: int  # input channels\n    M: int  # output channels\n    U: int = 1  # stride\n    P: int = 1  # padding\n</code></pre> <p>Besides inherited from <code>ShapeParam</code>, <code>Conv2DShapeParam</code> is also a frozen dataclass. The <code>@dataclass(frozen=True)</code> decorator automatically generates methods like <code>__init__</code>, <code>__repr__</code>, and <code>__eq__</code>, and the <code>frozen=True</code> option makes the dataclass immutable, meaning its attributes cannot be modified after initialization.</p> <p>BatchNorm and ReLU can be directly fused into Conv2d, so there is no need to define them separately. MaxPool2d cannot be fused with Conv2d as an operator, but it needs to be integrated into the dataflow of our custom DLA. Therefore, we define the information for MaxPool2d as follows:</p> <pre><code>@dataclass(frozen=True)\nclass MaxPool2DShapeParam(ShapeParam):\n    N: int  # batch size\n    kernel_size: int\n    stride: int\n</code></pre> <p>Additionally, we define <code>LinearShapeParam</code> as a dataclass that represents the shape parameters for a fully connected (linear) layer. However, our Eyeriss DLA will not directly support Linear layers. Instead, we will demonstrate CPU fallback support for operators that are not natively supported by our custom hardware through compiler in lab5.</p> <pre><code>@dataclass(frozen=True)\nclass LinearShapeParam(ShapeParam):\n    N: int  # batch size\n    in_features: int\n    out_features: int\n</code></pre> <p>Next, we will implement the network parser to extract the above information from the pre-trained model's weight files.</p>"},{"location":"2025sp/lab2/#parsing-pytorch-models","title":"Parsing PyTorch Models","text":"<p>To obtain the model's weight information, we can directly load the model and retrieve the weights. However, the activations are not stored in the PyTorch weight files, as they do not save the input and output activation sizes. These sizes can only be dynamically inferred during the model's computation process. Fortunately, PyTorch provides a built-in \"hook\" mechanism, which allows users to dynamically insert custom operations during the forward pass or backward pass. This enables us to capture and track the activation sizes during the computation.</p> <p>With hooks, we can dynamically inspect or manipulate input and output tensors without modifying the model's structure. They are commonly used for model debugging and visualization.</p> <p>There are several types of hooks in PyTorch: 1. Forward Hook     * Used to obtain the inputs and outputs of each layer during the forward pass.     * Triggered after the forward pass of a layer is completed. 2. Backward Hook     * Used to obtain the gradient values of inputs and outputs for each layer during the backward pass.     * Triggered after the backward pass of a layer is completed. 3. Gradient Hook     * Used to obtain the gradient values of each tensor during the backward pass.     * Triggered after each gradient computation is completed but before the gradient is updated to the <code>grad</code> attribute.</p> <p>Here\u2019s how we can use hook mechanism in PyTorch to capture activation sizes during the forward pass:</p>"},{"location":"2025sp/lab2/#step-1-instantiate-a-model","title":"Step 1. Instantiate a Model","text":"<p>First, we create a <code>SimpleModel</code> and instantiate it.</p> <pre><code>import torch\nimport torch.nn as nn\n\n\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super(SimpleModel, self).__init__()\n        self.conv = nn.Conv2d(3, 16, kernel_size=3)\n        self.relu = nn.ReLU()\n        self.fc = nn.Linear(16 * 30 * 30, 10)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.relu(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n\nmodel = SimpleModel()\n</code></pre>"},{"location":"2025sp/lab2/#step-2-define-forward-hook","title":"Step 2. Define Forward Hook","text":"<p>Next, we define the operation we want to insert, which must be defined as a function as the following prototype.</p> <pre><code>hook(module: nn.Module, inputs: torch.Tensor, output: torch.Tensor) -&gt; Optional[torch.Tensor]\n</code></pre> <p>In this example, <code>hook_fn</code> prints the basic information of the layer, and it will be triggered during the forward pass.</p> <pre><code>def hook_fn(module: nn.Module, inputs: torch.Tensor, output: torch.Tensor) -&gt; None:\n    print(f\"Layer: {module.__class__.__name__}\")\n    print(f\"Input Shape: {inputs[0].shape}\")\n    print(f\"Output Shape: {output.shape}\\n\")\n</code></pre>"},{"location":"2025sp/lab2/#step-3-register-forward-hook","title":"Step 3. Register Forward Hook","text":"<p>Then, we need to register the hook we defined with the <code>register_forward_hook</code> method into the modules we want to observe. Every call of <code>register_forward_hook</code> will return a handle of the hook inserted. We collect them with a list <code>hooks</code>, and we will remove them from the modules later.</p> <pre><code># Register the hook using register_forward_hook\nhooks = []\nfor module in model.modules():\n    hooks.append(module.register_forward_hook(hook_fn))\n</code></pre>"},{"location":"2025sp/lab2/#step-4-trigger-forward-pass","title":"Step 4. Trigger Forward Pass","text":"<p>Finally, we create a random input and pass it through the model, triggering the hook. During the model's execution, the hook for each layer will be triggered, thus executing <code>hook_fn</code>. After the forward pass, we can remove the hooks via the handles collected in <code>hooks</code>.</p> <pre><code># Create a input tensor\ndummy_input = torch.randn(1, 3, 32, 32)\n\n# Perform the forward pass, triggering the hook\nmodel(dummy_input)\n\n# Remove the hooks to prevent them from being triggered on every forward pass\nfor hook in hooks:\n    hook.remove()\n</code></pre> <p>The above code will output:</p> <pre><code>Layer: Conv2d\nInput Shape: torch.Size([1, 3, 32, 32])\nOutput Shape: torch.Size([1, 16, 30, 30])\n\nLayer: ReLU\nInput Shape: torch.Size([1, 16, 30, 30])\nOutput Shape: torch.Size([1, 16, 30, 30])\n\nLayer: Linear\nInput Shape: torch.Size([1, 14400])\nOutput Shape: torch.Size([1, 10])\n\nLayer: SimpleModel\nInput Shape: torch.Size([1, 3, 32, 32])\nOutput Shape: torch.Size([1, 10])\n</code></pre> <p>In this assignment, please implement your own <code>hook_fn</code> to capture the information of each layer in your custom VGG model. Then, populate the information into the previously defined <code>Conv2DShapeParam</code> and <code>MaxPool2DShapeParam</code> accordingly.</p>"},{"location":"2025sp/lab2/#parsing-onnx-models","title":"Parsing ONNX Models","text":"<p>ONNX (Open Neural Network Exchange) is an open standard that allows models to be exchanged between different deep learning frameworks (such as PyTorch and TensorFlow) and run on various inference engines (such as ONNX Runtime and TensorRT).</p>"},{"location":"2025sp/lab2/#convert-pytorch-models-to-onnx","title":"Convert PyTorch Models to ONNX","text":"<pre><code>import torch\nimport torch.onnx\n\nmodel = SimpleModel()\ndummy_input = torch.randn(1, 3, 32, 32)\ntorch.onnx.export(model, dummy_input, \"simple_model.onnx\")\n</code></pre>"},{"location":"2025sp/lab2/#run-inference-with-onnx-runtime","title":"Run Inference with ONNX Runtime","text":"<pre><code>import onnxruntime as ort\nsession = ort.InferenceSession(\"simple_model.onnx\")\nresult = session.run(None, {\"input\": input_data})\n</code></pre>"},{"location":"2025sp/lab2/#visualize-onnx-models","title":"Visualize ONNX Models","text":"<p>An ONNX model can be visualized by using Netron, and the result looks like this:</p> <ul> <li>full-precision model (FP32)     </li> <li>quantized model (INT8)     </li> </ul> <p>Students, try using Netron to visualize your own custom-designed model. Observe how the model architecture changes before and after quantization.</p>"},{"location":"2025sp/lab2/#parsing-onnx-models_1","title":"Parsing ONNX Models","text":"<p>ONNX cannot be used for training and requires the model to be trained in another framework first. Unlike PyTorch, ONNX stores the sizes of inputs and outputs.</p> <p>During the process of parsing an ONNX model, using <code>shape_inference</code> helps us obtain tensor shape information.</p>"},{"location":"2025sp/lab2/#step-1-use-shape_inference-to-infer-shapes","title":"Step 1. Use shape_inference to infer shapes","text":"<pre><code>import onnx\nfrom onnx import shape_inference\ndef _get_tensor_shape(model: onnx.ModelProto, tensor_name: str):\n    inferred_model = shape_inference.infer_shapes(model)\n</code></pre>"},{"location":"2025sp/lab2/#step-2-search-in-different-sections","title":"Step 2. Search in different Sections","text":"<pre><code>    # Search for the tensor with the given name\n    for value_info in inferred_model.graph.value_info:\n        if value_info.name == tensor_name:\n            return # return shape\n\n    # If not found; search the model's inputs\n    for input_info in inferred_model.graph.input:\n        if input_info.name == tensor_name:\n            return # return shape\n\n    # If still not found; search the model's outputs\n    for output_info in inferred_model.graph.output:\n        if output_info.name == tensor_name:\n            return # return shape\n</code></pre> <p>In this assignment, please implement a network parser for the ONNX model. Extract the relevant information from the model and fill it into the previously defined Layer Info classes.</p>"},{"location":"2025sp/lab2/#lab-22-analytical-model-for-eyeriss-dla","title":"Lab 2.2 - Analytical Model for Eyeriss DLA","text":""},{"location":"2025sp/lab2/#introduction-to-performance-modeling","title":"Introduction to Performance Modeling","text":"<ul> <li>Analytical modeling: Analytical modeling use mathematical equations to describe the performance characteristics of a hardware system. This method is suitable for simpler designs, but as system complexity increases, model accuracy may become limited.</li> <li>Simulation-based modeling: Simulation provides a more accurate modeling method with simulation software to model the behavior of the system, especially useful for evaluating complex architectures.<ul> <li>cycle-accurate simulation v.s. behavioral simulation</li> <li>full-system simulation v.s. intruction-level simulation</li> </ul> </li> <li>Statistical modeling: Statistical modeling gathers performance data under different conditions and uses statistical methods to create predictive models. When sufficient data is available, this method can achieve reasonable accuracy, but it may require extensive data and computational resources.</li> <li>Machine learning-based modeling: With advances in machine learning, using machine learning for performance modeling has become an emerging approach. For example, regression models or neural networks can predict the performance of specific hardware designs. This method is highly flexible and suitable for handling highly complex architectures but requires a large amount of training data.</li> </ul> <p>In this section, we will develop an analytical model for Eyeriss architecture to predict the system performance, and investigate the optimal software and hardware mapping configuration.</p>"},{"location":"2025sp/lab2/#hardware-parameters","title":"Hardware Parameters","text":"<p>The ultimate goal of the analytical model is to determine which implementation method to adopt by calculating the performance that different hardware configurations can provide. In the DLA, factors such as the size of the PE array, the scratch pad memory size within the PE, the size of the global buffer, and the bus bandwidth all affect the final performance that the hardware can deliver. Below, we will define these parameters as the <code>EyerissHardwareParam</code> data class.</p> <pre><code>@dataclass(frozen=True)\nclass EyerissHardwareParam:\n    pe_array_h: int\n    pe_array_w: int\n    ifmap_spad_size: int\n    filter_spad_size: int\n    psum_spad_size: int\n    glb_size: int\n    bus_bw: int\n    noc_bw: int\n</code></pre>"},{"location":"2025sp/lab2/#dataflow-mapping-parameters","title":"Dataflow Mapping Parameters","text":"<p>Eyeriss uses the row-stationary (RS) dataflow, which is defined by the following mapping parameters (see Table 2 in the paper). These parameters determine how the data is tiled, how it is mapped to the computational units on the hardware, and which are related to data reuse.</p> <pre><code>@dataclass\nclass EyerissMappingParam:\n    m: int  # number of ofmap channels stored in global buffer\n    n: int  # number of ofmaps/ifmaps used in a processing pass\n    e: int  # width of the PE set (strip-mined if nessary)\n    p: int  # number of filters processed by a PE set\n    q: int  # number of ifmap/filter channels processed by a PE set\n    r: int  # number of PE sets for different ifmap/filter channels\n    t: int  # number of PE sets for different filters\n</code></pre>"},{"location":"2025sp/lab2/#pe-set","title":"PE Set","text":"<p>Eyeriss employs a 2D array of PEs, each responsible for performing MAC operations. The PE array in Eyeriss is configurable. Thus, we can further partition the entire PE array into several PE sets to improve PE utilization rate and data reuse. A PE set is a logical grouping of PEs that are assigned to process a specific part of the convolution operation. since we apply the row-stationary dataflow, the height of a PE set is determined by the filter height R.</p> <p>For instance, we have a 6*8 PE array:</p> <p></p> <p>We assume that the height of the filters is 3; then, we can separate the PE set like this:</p> <p></p> <p>There're plenty of methods to assign these PE sets. In this example, e for a PE set is 4, which means that one PE set can process 4 rows of ofmap at a time. The values of m, n, p, and q are constrained by the on-chip memory (GLB or Spad). Notice that r \u00d7 t must equal 4 in this example. Either different PE sets process different channels, or different PE sets process different filters. There is no other choice.</p>"},{"location":"2025sp/lab2/#summary-of-notation","title":"Summary of Notation","text":""},{"location":"2025sp/lab2/#conv2d-shape-parameter","title":"Conv2D Shape Parameter","text":"Parameter Description N batch size H/W input height/width R/S filter height/width E/F output height/width C input channels M output channels U stride P padding"},{"location":"2025sp/lab2/#rs-dataflow-mapping-parameter","title":"RS Dataflow Mapping Parameter","text":"Parameter Description m number of ofmap channels stored in the global buffer n number of ofmaps/ifmaps used in a processing pass e width of the PE set (strip-mined if nessary) p number of filters processed by a PE set q number of ifmap/filter channels processed by a PE set r number of PE sets for different ifmap/filter channels t number of PE sets for different filters"},{"location":"2025sp/lab2/#tiling-strategy-and-loop-ordering","title":"Tiling Strategy and Loop Ordering","text":""},{"location":"2025sp/lab2/#visualization-for-conv2d-tiling","title":"Visualization for Conv2D Tiling","text":"<p>Below, we map a Conv2d operator to the Eyeriss DLA and label the Conv2d shape parameters and mapping parameters using the notation from the paper. We present the execution of Eyeriss tiling in each processing pass using a PPT to illustrate how data is processed in a single pass. The light blue-gray blocks represent the amount of data that can be processed in a single processing pass.</p> <ul> <li>Youtube Video</li> </ul> <ul> <li>Slides</li> </ul>"},{"location":"2025sp/lab2/#loop-nests-representation-for-conv2d-tiling","title":"Loop Nests Representation for Conv2D Tiling","text":"<p>Below is the representation of Conv2d tiling using loop nests, and how it is mapped to the PE array. We will explain each part step by step.</p> <pre><code>uint8 I[N][C][H][W]    # input activations\nint8  W[M][C][R][S]    # filter weights\nint32 B[M]             # bias\nuint8 O[N][M][E][F]    # output activations\n\nfor m_base in range(0, M, m):\nfor e_base in range(0, E, e):\nfor n_base in range(0, N, n):\nfor c_base in range(0, C, q*r):\nfor m_tile in range(0, m, p*t):\n    # move data from DRAM to GLB\n    dma(DRAM_TO_GLB, I, size=n*(q*r)*(U*(e-1)+R)*W)\n    dma(DRAM_TO_GLB, W, size=(p*t)*(q*r)*R*S)\n    dma(DRAM_TO_GLB, B, size=(p*t))\n\n    # processing pass in the PE array, each PE calculate a row of output\n\n    # PE sets in the PE array\n    parallel_for c_subtile in range(0, q*r, q):\n    parallel_for m_subtile in range(0, p*t, p):\n\n    # PEs within a PE set\n    parallel_for r_idx in range(0, R):\n    parallel_for e_offset in range(0, e):\n\n        # in a single PE\n        for n_offset in range(0, n):\n        for c_offset in range(0, q):\n        for m_offset in range(0, p):\n        for f_idx in range(0, F):\n            n_idx = n_base + n_offset\n            m_idx = m_base + m_tile + m_subtile + m_offset\n            e_idx = e_base + e_offset\n            c_idx = c_base + c_subtile + c_offset\n            h_idx = e_idx\n            w_idx = f_idx\n\n            if c_base == 0:\n                psum = B[m_idx]\n            else:\n                psum += \\\n                    I[n_idx][c_idx][h_idx][w_idx:w_idx+S] * \\\n                    W[m_idx][c_idx][r_idx][0:S]\n\n            O[n_idx][m_idx][e_idx][f_idx] = psum\n\n    # move data from GLB to DRAM\n    dma(GLB_TO_DRAM, O, size=n*m*e*F)\n</code></pre> <p>The above pseudocode contains two types of for loops. The <code>for</code> loop represents a \"temporal for,\" where the iterations correspond to the computation of different data at different time points by a single computational unit. The other loop, <code>parallel_for</code>, represents a \"spatial for,\" where at the same time point, different data is assigned to different computational units for processing.</p> <p>Different output rows (indexed by <code>e_offset</code>) will be distributed across different PEs for computation in a single processing pass, as represented below:</p> <pre><code># processing pass in the PE array, each PE calculate a row of output\nparallel_for e_offset in range(0, e):\n</code></pre> <p>In a single processing pass, each PE is responsible for computing one row. It will first iterate <code>F</code> times along the input/output width dimension, then iterate <code>p * t</code> elements along the output channel dimension, followed by iterating <code>q * r</code> elements along the input/filter channel dimension. Finally, it will iterate over the input/output batch size dimension, where we are currently assuming \\(N=n=1\\).</p> <pre><code># in a single PE\nfor n_offset in range(0, n):\nfor c_offset in range(0, q*r):\nfor m_offset in range(0, p*t):\nfor f_idx in range(0, F):\n</code></pre> <p>Each PE is responsible for performing channel-wise partial sum accumulation. Therefore, when <code>c_base == 0</code>, the bias can be pre-loaded from the GLB (Global Buffer) into the PE. For other cases, the product of the input and weight elements is accumulated into the partial sum (psum) scratch pad memory.</p> <pre><code>if c_base == 0:\n    psum = B[m_idx]\nelse:\n    psum += \\\n        I[n_idx][c_idx][h_idx][w_idx:w_idx+S] * \\\n        W[m_idx][c_idx][r_idx][0:S]\n</code></pre>"},{"location":"2025sp/lab2/#performance-metrics","title":"Performance Metrics","text":"<p>After understanding how Eyeriss performs the Conv2d operation, the next step is to evaluate the system's performance under different parameter configurations. Below, we list several performance metrics:</p> <ul> <li>memory related<ul> <li>GLB Usage</li> <li>GLB Access</li> <li>DRAM Access</li> </ul> </li> <li>compute related<ul> <li>MACs</li> <li>Latency</li> </ul> </li> <li>energy related<ul> <li>Power Consumption</li> </ul> </li> </ul>"},{"location":"2025sp/lab2/#glb-usage","title":"GLB Usage","text":"<p>The size required for the GLB (Global Buffer) depends on how much data is processed in a single processing pass, including the input feature map (8 bits per element), filter (8 bits per element), bias (32 bits per element), and partial sum (32 bits per element).</p> <p>In the upcoming lab, zero skipping is expected to be used when fetching the ifmap, so there is no need to consider the storage of padding.</p> <p>Below, we take the input feature map (ifmap) as an example:</p> \\[ \\text{GLB usage of ifmap} = n \\times qr \\times (U(e-1) + R) \\times W \\times 1 \\text{byte} \\]"},{"location":"2025sp/lab2/#dram-access-data-transfer-between-dram-and-glb","title":"DRAM Access (Data Transfer between DRAM and GLB)","text":"<p>The data used in a single processing pass is loaded from DRAM to the GLB once and written back from the GLB to DRAM once. Therefore, by dividing the total size of the entire layer by the amount of data processed in each processing pass, you can determine how many times DRAM needs to be accessed.</p> <p>Below, we take the input feature map (ifmap) as an example:</p> \\[ \\text{DRAM reads for ifmap} = \\overbrace{     \\left\\lceil\\frac{M}{m}\\right\\rceil }^\\text{refetch DRAM} \\times \\overbrace{     \\left\\lceil\\frac{E}{e}\\right\\rceil \\times     \\left\\lceil\\frac{N}{n}\\right\\rceil \\times     \\left\\lceil\\frac{C}{qr}\\right\\rceil }^\\text{num of tiles} \\times \\overbrace{     n \\times qr \\times (U(e-1) + R) \\times W \\times 1 \\text{byte} }^\\text{tile size} \\]"},{"location":"2025sp/lab2/#glb-access-data-transfer-between-glb-and-spads","title":"GLB Access (Data Transfer between GLB and Spads)","text":"<p>Below, we take the input feature map (ifmap) as an example:</p> \\[ \\text{GLB reads for ifmap} = \\overbrace{     \\left\\lceil\\frac{M}{m}\\right\\rceil }^\\text{refetch DRAM} \\times \\overbrace{     \\left\\lceil\\frac{E}{e}\\right\\rceil \\times     \\left\\lceil\\frac{N}{n}\\right\\rceil \\times     \\left\\lceil\\frac{C}{qr}\\right\\rceil }^\\text{num of tiles} \\times \\overbrace{     \\left\\lceil\\frac{m}{pt}\\right\\rceil }^\\text{reuse in GLB} \\times \\overbrace{     n \\times qr \\times (U(e-1) + R) \\times W \\times 1 \\text{byte} }^\\text{tile size} \\]"},{"location":"2025sp/lab2/#macs","title":"MACs","text":"<p>The number of MACs required by a Conv layer depends only on the shape of the layer.</p> \\[ \\text{MACs} = N \\times M \\times E \\times F \\times C \\times R \\times S \\]"},{"location":"2025sp/lab2/#latency","title":"Latency","text":"<p>Important</p> <p>The unit of latency here is cycles, and the number of DRAM/GLB accesses are bytes. Thus, the DRAM/GLB access time of the following equation should be cycles/byte.</p> <p>However, in the provided sample code, the units of <code>DRAM_ACCESS_TIME</code> and <code>GLB_ACCESS_TIME</code> are cycles/transaction, so they should be divided by <code>bus_bw</code> and <code>noc_bw</code> (unit: bytes/transcation), respectively.</p> \\[ \\begin{align} \\text{latency} &amp;= \\text{number of DRAM accesses} \\times \\text{DRAM access time} \\\\ &amp;+ \\text{number of GLB accesses} \\times \\text{SRAM access time} \\\\ &amp;+ \\text{PE array compute time} \\\\ &amp;+ \\text{ofmap size} \\times \\text{PPU compute time per element} \\end{align} \\] <p>The compute time of the PE array will partially overlap with the GLB access, but the exact ratio is difficult to estimate, so we will temporarily ignore it. As for the compute latency of the post-processing unit (PPU), it can be assumed to be:</p> \\[ \\text{PPU compute time per element} = \\cases{     5 \\text{ cycles, if MaxPool} \\\\     1 \\text{ cycle, otherwise} } \\]"},{"location":"2025sp/lab2/#power-and-energy-consumption","title":"Power and Energy Consumption","text":"<p>Given the following assumptions:</p> <ul> <li>Energy consumption per MAC: \\(E_\\text{MAC} = 2 \\text{ \u03bcJ}\\)</li> <li>Energy consumption per GLB access: \\(E_\\text{GLB} = 10 \\text{ \u03bcJ}\\)</li> <li>Energy consumption per DRAM access: \\(E_\\text{DRAM} = 200 \\text{ \u03bcJ}\\)</li> <li>Power consumption of leackage: \\(P_\\text{leakage} = 50 \\text{ \u03bcW}\\)</li> <li>clock rate: \\(200 \\text{ MHz}\\)</li> </ul> <p>We can get:</p> \\[ E_\\text{compute} = \\text{MACs} \\times E_\\text{MAC} = \\text{MACs} \\times 2 \\text{ \u03bcJ} \\] \\[ \\begin{align} E_\\text{memory} &amp;= (A_\\text{DRAM} \\times E_\\text{DRAM}) + (A_\\text{GLB} \\times E_\\text{GLB}) \\\\ &amp;= (A_\\text{DRAM} \\times 200 \\text{ \u03bcJ}) + (A_\\text{GLB} \\times 10 \\text{ \u03bcJ}) \\end{align} \\] \\[ T = \\frac{T_\\text{cycles}}{\\text{clock rate}} = \\frac{T_\\text{cycles}}{200 \\text{ MHz}} \\] <p>where \\(A_\\text{DRAM}\\) is number of DRAM accesses, \\(A_\\text{GLB}\\) is the number of GLB accesses, \\(T\\) is the latency per layer in seconds, and \\(T_\\text{cycles}\\) is the latency per layer in cycles.</p> <p>Therefore, the total energy and power consumption of a convolution layer are:</p> \\[ \\begin{align} E &amp;= E_\\text{compute} + E_\\text{memory} + (P_\\text{leakage}  \\times T) \\\\ &amp;= (\\text{MACs} \\times 2 \\text{ \u03bcJ}) + (A_\\text{DRAM} \\times 200 \\text{ \u03bcJ}) + (A_\\text{GLB} \\times 10 \\text{ \u03bcJ}) + (50 \\text{ \u03bcW} \\times \\frac{T_\\text{cycles}}{200 \\text{ MHz}}) \\\\ \\end{align} \\] \\[ \\begin{align} P &amp;= \\frac{E_\\text{compute} + E_\\text{memory}}{T} + P_\\text{leakage} \\\\ &amp;= \\frac{200 \\text{ MHz}}{T_\\text{cycles}} \\times [(\\text{MACs} \\times 2 \\text{ \u03bcJ}) + (A_\\text{DRAM} \\times 200 \\text{ \u03bcJ}) + (A_\\text{GLB} \\times 10 \\text{ \u03bcJ})] + 50 \\text{ \u03bcW} \\end{align} \\]"},{"location":"2025sp/lab2/#summary","title":"Summary","text":"<p>The above calculation method is only a rough estimate, helping us to have a quantitative basis before deciding on the system specifications. Although some factors have not been considered (e.g., memory read and write speeds may differ), it may not fully match the final hardware performance, but the trends it presents are still valuable for reference.</p> <p>In this assignment, we will ask students to calculate the following performance metrics and implement them in the analytical model:</p> <ul> <li>global buffer usage per pass<ul> <li>filter</li> <li>bias</li> <li>psum</li> </ul> </li> <li>global buffer accesses per layer<ul> <li>filter reads</li> <li>bias reads</li> <li>psum reads</li> <li>psum writes</li> </ul> </li> <li>DRAM accesses per layer<ul> <li>filter reads</li> <li>bias reads</li> <li>ofmap writes</li> </ul> </li> </ul>"},{"location":"2025sp/lab2/#implementation-of-analytical-model","title":"Implementation of Analytical Model","text":"<p>In this lab, we use a Python class to implement the analytical model of the Eyeriss DLA. Please refer to the <code>EyerissAnalyzer</code> class defined in the <code>lab2/src/analytical_model/eyeriss.py</code> file provided by the teaching assistant. Below, we will explain the API design and usage of the analytical model.</p>"},{"location":"2025sp/lab2/#softwarehardware-configuration","title":"Software/Hardware Configuration","text":"<pre><code>class EyerissAnalyzer:\n    def __init__(self, name: Optional[str], hardware_param: EyerissHardwareParam) -&gt; None:\n        pass\n\n    @conv_shape.setter\n    def conv_shape(self, conv_param: Conv2DShapeParam) -&gt; None:\n        pass\n\n    @maxpool_shape.setter\n    def maxpool_shape(self, maxpool_param: Optional[MaxPool2DShapeParam]) -&gt; None:\n        pass\n\n    @mapping.setter\n    def mapping(self, mapping_param: EyerissMappingParam) -&gt; None:\n        pass\n</code></pre> <p>When initializing the <code>EyerissAnalyzer</code>, you need to specify the <code>EyerissHardwareParam</code> (see Hardware Parameters). Then, based on the results from network parsing, input the information for the Conv2d and MaxPool2d layers into the analytical model (see Defining Layer Information). Finally, set the desired mapping parameters for analysis (see Dataflow Mapping Parameters). The numbers below are just examples, and you can try different settings.</p> <pre><code>eyeriss = EyerissAnalyzer(\n    name=\"Eyeriss\",\n    hardware_param=EyerissHardwareParam(\n        pe_array_h=6,\n        pe_array_w=8,\n        ifmap_spad_size=12,\n        filter_spad_size=48,\n        psum_spad_size=16,\n        glb_size=64 * 2**10,\n        bus_bw=4,\n        noc_bw=4,\n    ),\n)\neyeriss.conv_shape = Conv2DShapeParam(N=1, H=32, W=32, R=3, S=3, E=32, F=32, C=3, M=64, U=1)\neyeriss.maxpool_shape = MaxPool2DShapeParam(N=1, kernel_size=2, stride=2)\neyeriss.mapping = EyerissMappingParam(m=16, n=1, e=8, p=4, q=4, r=1, t=2)\n</code></pre>"},{"location":"2025sp/lab2/#performance-estimation","title":"Performance Estimation","text":"<p>In the previous Performance Metrics section, we mentioned several performance indicators that we care about. The analytical model is responsible for calculating these performance metrics based on the settings above. The <code>EyerissAnalyzer</code> provides the following APIs to perform this task:</p> <pre><code>@property\ndef glb_usage_per_pass(self) -&gt; Dict[str, int]:\n    return {\n        \"ifmap\": 0,\n        \"filter\": 0,\n        \"psum\": 0,\n        \"bias\": 0,\n        \"total\": 0,\n    }\n\n@property\ndef dram_access_per_layer(self) -&gt; Dict[str, int]:\n    return {\n        \"ifmap_read\": 0,\n        \"filter_read\": 0,\n        \"bias_read\": 0,\n        \"ofmap_write\": 0,\n        \"read\": 0,\n        \"write\": 0,\n        \"total\": 0,\n    }\n\n@property\ndef glb_access_per_layer(self) -&gt; Dict[str, int]:\n    return {\n        \"ifmap_read\": 0,\n        \"filter_read\": 0,\n        \"bias_read\": 0,\n        \"psum_read\": 0,\n        \"psum_write\": 0,\n        \"read\": 0,\n        \"write\": 0,\n        \"total\": 0,\n    }\n\n@property\ndef latency_per_layer(self) -&gt; int:\n    return 0\n\n@property\ndef macs_per_layer(self) -&gt; int:\n    return 0\n\n@property\ndef power_per_layer(self) -&gt; float:\n    return 0\n</code></pre> <p>After initializing the analytical model, you can obtain the system's performance metrics under the given settings. Here is an example:</p> <pre><code>print(f'maximum GLB usage: {eyeriss.glb_usage_per_pass[\"total\"]} bytes')\nprint(f'number of DRAM reads: {eyeriss.dram_access_per_layer[\"read\"]} bytes')\nprint(f'number of DRAM write: {eyeriss.dram_access_per_layer[\"write\"]} bytes')\nprint(f'number of GLB reads: {eyeriss.glb_access_per_layer[\"read\"]} bytes')\nprint(f'number of GLB write: {eyeriss.glb_access_per_layer[\"write\"]} bytes')\nprint(f'inference latency: {eyeriss.latency_per_layer} cycles')\n</code></pre> <p>The assignment will require students to implement getter functions for performance metrics based on the above API. Please refer to the code and the assignment instructions for further details.</p>"},{"location":"2025sp/lab2/#integration-with-network-parser","title":"Integration with Network Parser","text":"<p>Up to this point, the analytical model can evaluate performance based on the layer information and mapping parameters specified by the user, but it still requires manual input, which can be inconvenient. At this point, we can integrate the network parser we just implemented, directly load the pre-trained model, obtain the required information, and save it in a unified format.</p> <p>The following code is the structure for integrating the network parser with the analytical model. In the <code>for</code> loop, the parsed layer information is fed into the analytical model for computation, and finally, the results are saved as a <code>.csv</code> file.</p> <pre><code>def main() -&gt; None:\n    # Network parsing\n    model = load_model(...)\n    layers = parse_onnx(...)\n\n    # Hardware configuration\n    eyeriss = EyerissAnalyzer(...)\n    eyeriss.mapping = ...\n\n    # Workload mapping and performance estimation\n    for layer in range(len(layers)):\n        eyeriss.conv_shape = ...\n        eyeriss.maxpool_shape = ...\n        perf_report(eyeriss, output.csv)\n\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"2025sp/lab2/#lab-23-design-space-exploration-dse","title":"Lab 2.3 - Design Space Exploration (DSE)","text":"<p>After deciding on the target operators to accelerate and the hardware architecture, we need to perform design space exploration (DSE) to find the optimal parameter configuration. We will automate the process to list all possible configurations and select the best solution based on the optimization goals set by the user. In this lab, we will implement the <code>EyerissMapper</code> class and use exhaustive search to perform DSE.</p>"},{"location":"2025sp/lab2/#roofline-model","title":"Roofline Model","text":"<p>Fist of all, we will introduce the roofline model, which is an intuitive visual performance model used to provide performance estimates of a given workload running on a specific architecture, by showing inherent hardware limitations, and potential benefit and priority of optimizations.</p> <p>In the subsequent sections, we are going to learn how to formulate:</p> <ol> <li>the charateristics of a given workload/application</li> <li>the performance provided by hardware under some limitations</li> </ol>"},{"location":"2025sp/lab2/#workload-characteristics","title":"Workload Characteristics","text":"<p>For a given workload, let \\(W\\) represent the total computational requirement and \\(Q\\) the data transfer requirement. The operational intensity (also called arithmetic intensity) \\(I\\) characterizes the workload as follows:</p> \\[ I = \\frac{W}{Q} \\] <p>A workload with higher operational intensity relies more on computation, making it compute-bound. In such cases, optimizations should focus on accelerating computation, such as increasing the number of processing elements (PEs) to exploit parallelism. Conversely, a workload with lower operational intensity is memory-bound, and optimizations should aim to reduce memory traffic or increase the memory bandwidth.</p> <p>For example, consider matrix-matrix multiplication \\(C = AB\\), where \\(A \\in \\mathbb{R}^{m \\times n}\\), \\(B \\in \\mathbb{R}^{n \\times k}\\), and \\(C \\in \\mathbb{R}^{m \\times k}\\). The operational intensity is given by:</p> \\[ I_\\text{mm} = \\frac{\\text{num of MACs}}{\\text{num of data} \\times \\text{num of bytes per data}} = \\frac{mnk}{mn + nk + mk} = O(N) \\] <p>Similarly, for matrix-vector multiplication \\(y = A x\\), where \\(A \\in \\mathbb{R}^{m \\times n}\\), \\(x \\in \\mathbb{R}^{n \\times 1}\\), and \\(y \\in \\mathbb{R}^{m \\times 1}\\), the operational intensity is:</p> \\[ I_\\text{mv} = \\frac{mn}{mn + m + n} = O(1) \\] <p>With the asymptotic analysis, we observe that matrix-matrix multiplication is more compute-bound, whereas matrix-vector multiplication is more memory-bound as the problem size increases.</p>"},{"location":"2025sp/lab2/#hardware-limitations","title":"Hardware Limitations","text":"<p>For a given hardware architecture, the capacity of such a system is upper-bounded by two parameters, the peak performance of the processor and the peak bandwidth of the memory. The roofline is formulated as the following:</p> \\[ P = \\min(\\pi, ~ \\beta \\times I) \\] <p>where \\(P\\) is the attainable performance, \\(\\pi\\) is the peak performance, \\(\\beta\\) is the peak bandwidth, and \\(I\\) is the operational intensity.</p> <p>For the Eyeriss DLA in our lab, each PE complete a MAC in a cycle, so the peak performance \\(\\pi\\) is bounded by the number of PEs \\(N_\\text{PE}\\), that is:</p> \\[ \\pi \\le k \\times N_\\text{PE} \\] <p>where \\(k\\) is a constant used to make the unit consistent between the both sides of the inequality (e.g. \\(\\text{MACs}/\\text{cycle}\\) or \\(\\text{FLOPs}/\\text{second}\\)), which can be the number of MACs a PE can compute in a cycle or the number of arithmetic operations a PE can compute in a second.</p> <p>With the above formulation, we can plot the performance bound as follows:</p> <ul> <li>y axis: performance \\(P\\) (unit: FLOPS or MACs/cycle)</li> <li>x axis: operational intensity \\(I\\) (unit: FLOPs/byte or MACs/byte)</li> <li>black solid line: performance upper bound of a given hardware</li> <li>red dashed line: machine balance point, separating the memory-bound (left) and compute-bound (right) regions</li> </ul> <p></p> <p>For a given hardware architecture, different mapping parameter choices leads to different number of memory accesses, and hence affect the operational intensity (OI). The following figure shows that Mapping 1 (OI = 8 &lt; 12) is memory-bound, and Mapping 2 (OI = 18 &gt; 12) is compute-bound.</p> <p></p> <p>Changes in hardware parameters alter the capacity, leading to different rooflines. For the same workload (fixed OI = 16), it is compute-bound on Hardware 1 with 48 PEs, while it is memory-bound on Hardware 2 with 72 PEs.</p> <p></p> <p>We provide a Python script <code>roofline.py</code> in the project root to plot the roofline model. It supports the following functionalities:</p> <ul> <li>Generate multiple rooflines based on given peak performance and peak memory bandwidth</li> <li>Plot multiple workload points by specifying their operations intensities</li> </ul> <p>For example, you can define and plot a Roofline model by calling <code>plot_roofline</code> function:</p> <pre><code>peak_performance = 60.0\npeak_bandwidth = 6.0\n\nplot_roofline(\n    rooflines={\"Custom Hardware\": (peak_performance, peak_bandwidth)},\n    workloads={\"New Mapping\": 10.0},\n    filename=\"log/figure/custom.png\",\n)\n</code></pre> <p>[!Note] The above code was updated at 3/29 17:30</p> <p>Then, you can execute this script with:</p> <pre><code>python3 roofline.py\n</code></pre>"},{"location":"2025sp/lab2/#mapping-parameter-search","title":"Mapping Parameter Search","text":""},{"location":"2025sp/lab2/#search-space-construction","title":"Search Space Construction","text":"<p>First, we need to list all possible parameter configurations, as shown in the following code:</p> <pre><code>from itertools import product\n\ndef generate_mappings(self) -&gt; List[EyerissMappingParam]:\n    n_avaliable_list = [1]\n    p_available_list = self.p_avaliable()\n    q_available_list = self.q_avaliable()\n    e_available_list = self.e_available()\n    r_available_list = self.r_available()\n    t_available_list = self.t_available()\n    m_available_list = self.m_available()\n\n    candidate_solutions = product(\n        m_available_list,\n        n_avaliable_list,\n        e_available_list,\n        p_available_list,\n        q_available_list,\n        r_available_list,\n        t_available_list,\n    )\n    return candidate_solutions\n</code></pre> <p>Each of <code>*_available()</code> functions will return a list of integers, representing the set of possible values for each mapping parameter. For a detailed explanation of the approach, please refer to the example code provided by the teaching assistant in <code>analytical_model/mapper.py</code>. Then, use <code>itertools.product(...)</code> to compute the Cartesian product, which will give you a list of tuples, where each tuple represents a set of mapping parameters.</p>"},{"location":"2025sp/lab2/#search-space-pruning-by-constraints","title":"Search Space Pruning by Constraints","text":"<p>To reduce the complexity of subsequent hardware implementation, we will set certain parameter combinations that the hardware cannot support and remove them from the search space.</p> <pre><code>def validate(self, mapping) -&gt; bool:\n    m, n, e, p, q, r, t = mapping\n\n    # pq constraints\n    if p * q &gt; self.hardware.filter_spad_size // self.conv_analyzer.conv_shape.S:\n        return False\n\n    # e constraints\n    if (\n        e % self.hardware.pe_array_w != 0\n        and e != self.hardware.pe_array_w // 2\n        and self.conv_analyzer.conv_shape.E != e\n    ):\n        return False\n\n    # rt constraints\n    if (\n        r * t\n        != self.hardware.pe_array_h\n        * self.hardware.pe_array_w\n        // self.conv_analyzer.conv_shape.R\n        // e\n    ):\n        return False\n\n    # m constraints\n    if m % p != 0:\n        return False\n\n    return True\n\ncandidate_solutions = [sol for sol in candidate_solutions if validate(sol)]\n</code></pre>"},{"location":"2025sp/lab2/#design-point-evaluation","title":"Design Point Evaluation","text":"<p>The remaining parameter configurations are those that are still valid under our hardware constraints. Next, we can design our own scoring method, where a set of <code>EyerissMappingParam</code> is used as input, and it outputs a score that can be used for ranking. After scoring each <code>EyerissMappingParam</code> in the remaining <code>candidate_solutions</code>, we can sort them to find the best mapping parameters for the specified operator size and hardware configuration.</p> <p>The entire exploration process will be implemented in the <code>EyerissMapper.run()</code> method, which constructs the search space with <code>generate_mappings()</code>, calculate the performance metrics with <code>Eyeriss.Analyzer.summary()</code>, and finally return the best N solutions with the scoring function <code>EyerissMapper.evaluate()</code> defined by youself. Here is the function code:</p> <pre><code>from heapq import nlargest, nsmallest\n\ndef run(\n    self,\n    conv2d: Conv2DShapeParam,\n    maxpool: MaxPool2DShapeParam | None = None,\n    num_solutions: int = 1,\n) -&gt; list[AnalysisResult]:\n    self.analyzer.conv_shape = conv2d\n    self.analyzer.maxpool_shape = maxpool\n\n    results: list[AnalysisResult] = []\n\n    for mapping in self.generate_mappings():  # construct the search space for mapping parameters\n        self.analyzer.mapping = mapping\n        res = self.analyzer.summary  # project the configuration to performance metrics\n        results.append(res)\n\n    results = nlargest(num_solutions, results, key=self.evaluate)  # evaluate each design point\n    return results\n</code></pre> <p>In the assignment, students will need to design and implement your own scoring algorithm in <code>Eyeriss.evaluate()</code> to evaluate how good/bad a design point (a set of mapping parameters) is and explain why you chose this design in your report.</p> <pre><code>def evaluate(metrics: AnalysisResult) -&gt; int | float:\n    raise NotImplementedError\n\nsorted_solutions = sorted(candidate_solutions, key=scoring_function, reverse=True)\n# or\nbest_solution = max(candidate_solutions, key=scoring_function)\n# or\nresults = nlargest(num_solutions, results, key=self.evaluate)  # evaluate each design point\n</code></pre>"},{"location":"2025sp/lab2/#hardware-parameter-search","title":"Hardware Parameter Search","text":""},{"location":"2025sp/lab2/#introduction","title":"Introduction","text":"<p>So far, we have been able to compute the optimal dataflow mapping parameters for each convolution layer under a given hardware configuration. Next, we want <code>EyerissMapper</code> to automatically find the optimal hardware configuration for us using the same evaluation criteria. We can achieve this by slightly modifying the <code>EyerissMapper.run()</code> function to consider different hardware configurations as well. The modified code is as follows:</p> <pre><code>def run(\n    self,\n    conv2d: Conv2DShapeParam,\n    maxpool: MaxPool2DShapeParam | None = None,\n    num_solutions: int = 1,\n) -&gt; list[AnalysisResult]:\n    self.analyzer.conv_shape = conv2d\n    self.analyzer.maxpool_shape = maxpool\n\n    results: list[AnalysisResult] = []\n\n    for hardware in self.generate_hardware():  # Construct the search space for hardware parameters\n        self.analyzer.hardware = hardware      # Reconfigure the hardware parameters\n\n        for mapping in self.generate_mappings():\n            self.analyzer.mapping = mapping\n            res = self.analyzer.summary\n            results.append(res)\n\n    results = nlargest(num_solutions, results, key=self.evaluate)\n    return results\n</code></pre> <p>In this modified code, we added a loop to search for <code>EyerissHardwareParam</code> values and used <code>EyerissMapper.generate_hardware()</code> to construct the search space.</p>"},{"location":"2025sp/lab2/#design-space-construction-for-hardware-specification","title":"Design Space Construction for Hardware Specification","text":"<p>In the sample code provided by the TA, the <code>EyerissMapper.generate_hardware()</code> method has already been implemented. The method for generating the search space is similar to <code>EyerissMapper.generate_mappings()</code> from the previous section, but each hardware parameter currently has only one value. This means that the constructed search space <code>candidate_solutions</code> contains only a single element.</p> <pre><code>def generate_hardware(self) -&gt; list[EyerissHardwareParam]:\n    # Add more values to the following lists to explore more solutions\n    pe_array_h_list = [6]\n    pe_array_w_list = [8]\n    ifmap_spad_size_list = [12]\n    filter_spad_size_list = [48]\n    psum_spad_size_list = [16]\n    glb_size_list = [64 * 2**10]\n    bus_bw_list = [4]\n    noc_bw_list = [4]\n\n    candidate_solutions = product(\n        pe_array_h_list,\n        pe_array_w_list,\n        ifmap_spad_size_list,\n        filter_spad_size_list,\n        psum_spad_size_list,\n        glb_size_list,\n        bus_bw_list,\n        noc_bw_list,\n    )\n    candidate_solutions = [EyerissHardwareParam(*m) for m in candidate_solutions]\n    return candidate_solutions\n</code></pre> <p>For the assignment, students are required to modify this function based on the Roofline model analysis (compute-bound or memory-bound). They should add more candidate solutions or introduce constraints, explain the reasoning behind these modifications in their report, and use actual data to demonstrate how the newly added hardware configurations address performance bottlenecks.</p>"},{"location":"2025sp/lab2/#practice","title":"Practice","text":""},{"location":"2025sp/lab2/#prerequisites","title":"Prerequisites","text":"<ol> <li>Download the sample code and report template from Moodle and then decompress it.     <pre><code>unzip aoc2025-lab2.zip\n</code></pre></li> <li>Create and activate virtual environment for AOC labs.     <pre><code>conda create -n aoc python=3.10 -y\nconda activate aoc\n</code></pre></li> <li>Install dependency packages     <pre><code>cd aoc2025-lab2\npip install -r requirements.txt\n</code></pre></li> </ol> <p>Important</p> <p>Please ensure that your Conda environment uses a Python version greater than 3.10, as we rely on its new features (e.g., simplified type hints and <code>match-case</code> syntax) in this lab.</p> <p>After unzipped the file, the directory structure will look like below:</p> <pre><code>$PROJECT_ROOT\n\u251c\u2500\u2500 analytical_model\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 eyeriss.py\n\u2502   \u2514\u2500\u2500 mapper.py\n\u251c\u2500\u2500 layer_info.py\n\u251c\u2500\u2500 lib\n\u2502   \u251c\u2500\u2500 models\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 lenet.py\n\u2502   \u2502   \u251c\u2500\u2500 mlp.py\n\u2502   \u2502   \u251c\u2500\u2500 qconfig.py\n\u2502   \u2502   \u2514\u2500\u2500 vgg.py\n\u2502   \u2514\u2500\u2500 utils\n\u2502       \u251c\u2500\u2500 dataset.py\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 utils.py\n\u251c\u2500\u2500 main.py\n\u251c\u2500\u2500 network_parser\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 network_parser.py\n\u2502   \u2514\u2500\u2500 torch2onnx.py\n\u251c\u2500\u2500 onnx_inference.py\n\u251c\u2500\u2500 profiling.py\n\u251c\u2500\u2500 report.md\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 weights\n    \u251c\u2500\u2500 vgg8-power2.pt\n    \u2514\u2500\u2500 vgg8.pt\n</code></pre> <p>Note that the <code>lib/models</code> and <code>lib/utils</code> directories contains the code from lab1. If you have designed your own model architecture other than the VGG-8 provided by the TA and you want to perform lab2's analysis with your own model, please put it into <code>lib/models</code>.</p>"},{"location":"2025sp/lab2/#1-workload-analysis","title":"1. Workload Analysis","text":"<p>Use the <code>profiling.py</code> script provided in the lab analyze the computational breakdown of each operator in the model you used for Lab 1. To check the usage of <code>profiling.py</code>, run:</p> <pre><code>cd $PROJECT_ROOT\npython3 profiling.py -h\n</code></pre> <p>For example, to profile a full-precision model, execute:</p> <pre><code>python3 profiling.py $PATH_TO_FP32_MODEL\n</code></pre> <p>To profile a quantized model, remember to specify the quantization backend by the <code>--backend</code> or <code>-b</code> option.</p> <pre><code>python3 profiling.py $PATH_TO_INT8_MODEL -b power2\n</code></pre> <p>Your report should include:</p> <ul> <li>Architecture of your model you used</li> <li>Profiling results for the full-precision model</li> <li>Profiling results for the quantized model</li> <li>Analysis and comparison</li> </ul> <p>Either as screenshots or copied output text is acceptable.</p>"},{"location":"2025sp/lab2/#2-network-parser","title":"2. Network Parser","text":"<p>In Lab 5, we will use an ONNX-format model as input for the AI compiler. In this lab, we will learn how to convert a model into a format that an analytical model can process. The definition of that format is defined with Python dataclass and previously-mentioned in Defining Layer Information Class. The provided example code includes a script for converting PyTorch models to ONNX, and the lecture notes explain how to parse PyTorch models.</p> <p>Students are required to complete the following two functions defined in <code>network_parser/network_parser.py</code> and used to convert models into the format defined in <code>layer_info.py</code>:</p> <ol> <li>(10%) <code>parse_pytorch(model: nn.Module, input_shape: tuple[int]) -&gt; list[ShapeParam]</code></li> <li>(10%) <code>parse_onnx(model: onnx.ModelProto) -&gt; list[ShapeParam]</code></li> </ol> <p>You may create your own test program or function to verify the correctness of your implementation.</p>"},{"location":"2025sp/lab2/#3-analytical-model","title":"3. Analytical Model","text":"<p>Given a set of hardware and mapping parameters, we aim to estimate the performance achievable with this system configuration and the associated cost for each convolutional block (Conv2d+ReLU or Conv2d+ReLU+MaxPool2d).</p> <p>The metrics to evaluate are listed below, and some of those have been demonstrated by TA in the handout and sample code:</p> <ul> <li>GLB usage per processing pass (unit: bytes): including ifmap, filter, bias, and psum</li> <li>GLB access (unit: bytes): including ifmap read, filter read, bias read, psum read, psum write, and ofmap write</li> <li>DRAM access (unit: bytes): including ifmap read, filter read, bias read, and ofmap write</li> <li>number of MACs</li> <li>latency of inference (unit: cycles)</li> <li>power consumption (unit: \u03bcW)</li> <li>energy consumption (unit: \u03bcJ)</li> </ul> <p>To complete this assignment, students must:</p> <ol> <li>(15%) Compute the performance metrics using the notation from the Eyeriss paper and write down the equations in LaTeX format in your report.</li> <li>(15%) Implement the specified getter functions in the <code>EyerissAnalyzer</code> class in <code>analytical_model/eyeriss.py</code>. Students must strictly follow the API specifications defined in the lecture notes and sample code, including input and output data types for each function. Grading will be based on the success rate of automated tests, including edge case evaluations.</li> </ol> <p>Similarly, you can create your own test program or function to verify the correctness of your implementation.</p>"},{"location":"2025sp/lab2/#4-roofline-model","title":"4. Roofline Model","text":"<ol> <li>(5%) Derive the operational intensity of the 2D convolution kernel using the notation from the Eyeriss paper in LaTeX. Then, estimate its complexity with Big-O notation.</li> <li> <p>(5%) Estimate the operational intensity of the following Conv2D kernel size, plot this kernel onto the roofline model of peak performance 48 MACs/cycle and peak bandwidth 4 byte/cycle, and determine which type of performance bound such kernel is in your report.</p> N M C H/W R/S E/F P (padding) U (stride) 1 64 3 32 3 32 1 1 </li> </ol>"},{"location":"2025sp/lab2/#5-design-space-exploration","title":"5. Design Space Exploration","text":"<p>Consider an Eyeriss-based accelerator with the following configurations:</p> <ul> <li>6\u00d78 PE array</li> <li>64 KiB global buffer</li> <li>76 bytes of scratchpad memory per PE:</li> <li>12 bytes for ifmap</li> <li>48 bytes for filter</li> <li>16 bytes for psum</li> <li>4-byte bus and NoC bitwidth (transfers 4 bytes per cycle)</li> </ul> <p>Complete the following 3 tasks:</p> <ol> <li>Search Space Construction for Dataflow Mappings (10%)    Implement the <code>generate_mappings()</code> method in <code>EyerissMapper</code> (located in <code>analytical_model/mapper.py</code>) to generate candidate mapping parameters using the provided <code>*_available()</code> methods. Filter out invalid configurations using the <code>validate()</code> method.</li> <li>Evaluation Algorithm Design (10%) Develop an evaluation algorithm in the <code>evaluate()</code> method of <code>EyerissMapper</code> to assess the quality of a given mapping based on analytical model performance metrics.<ul> <li>Describe the design and logic of your algorithm in the report. (What performance metric(s) are considered, and why?)</li> <li>Integrate it with the analytical model and network parser for end-to-end analysis. (triggered by the <code>main.py</code>, which will be described latter)</li> <li>Include the generated CSV output file (<code>dse_mappings.csv</code>) in your submission, containing the mapping parameters and corresponding performance metrics for the top 3 candidate solutions of each Conv2D layer.</li> <li>List the best candidate solutions per layer selected by your algorithm in the report.</li> </ul> </li> <li>Search Space Expansion for Hardware Configurations (10%)    Modify the <code>generate_hardware()</code> method in <code>EyerissMapper</code> to expand the search space for hardware parameters based on Roofline model analysis (compute-bound vs. memory-bound).<ul> <li>Explain the rationale for your modifications in the report.</li> <li>Use actual data to demonstrate how your updated hardware configurations mitigate performance bottlenecks.</li> <li>Include the generated CSV output file (<code>dse_all.csv</code>) in your submission, containing the hardware parameters, mapping parameter, and corresponding performance metrics for the top 3 candidate solutions of each Conv2D layer.</li> </ul> </li> </ol> <p>These tasks do not have a single correct solution\u2014creativity is encouraged.</p> <p>A <code>main.py</code> script is provided in the project root, integrating the network parser, analytical model, and design space exploration. To view usage options, run:</p> <pre><code>python3 main.py -h\n</code></pre> <p>Basic case:</p> <pre><code>python3 main.py ./weights/vgg8-power2.pt`\n</code></pre> <p>More complex scenario:</p> <pre><code>python3 main.py -f onnx -o log/$(date -I) vgg-power2.onnx\n</code></pre>"},{"location":"2025sp/lab3/","title":"Lab 3 - Hardware Architecture Design","text":""},{"location":"2025sp/lab3/#overview","title":"Overview","text":"<p>In this lab, we will design an accelerator based on the Eyeriss paper. The goal is to implement the hardware architecture required to efficiently compute the VGG-8 model designed in Lab 1.</p> <p></p> <p>This includes key operations such as <code>2D Convolution</code> fused with <code>BatchNorm2D</code>, <code>Linear</code>, <code>ReLU</code>, <code>MaxPool</code>, and <code>Post-Quantization</code>.</p> <p>We will fuse <code>BatchNorm2d</code> into <code>Conv2D</code> during the software stage.</p> <p>For <code>Conv2D</code> computation, we will design an Eyeriss-based PE array to support <code>3\u00d73</code> convolution operations.</p> <p>Additionally, we will design a Post Processing Unit to handle <code>Maxpool</code>, <code>ReLU</code>, and <code>PostQuant</code> (We define post-quantization as the process where the quantized values are dequantized, processed, and then requantized, effectively outputting the ofmap).</p> <p>However, in Lab3, the <code>linear</code> operation will not be implemented.</p> <p>Students can try to think about how to fine-tune the Eyeriss architecture so that the PE array can also support linear operations. This contribution could serve as a breakthrough for the final project.</p>"},{"location":"2025sp/lab3/#eyeriss","title":"Eyeriss","text":"<p>In Lab 3, we will design our accelerator based on the Eyeriss paper.</p>"},{"location":"2025sp/lab3/#post-processing-unit","title":"Post Processing Unit","text":"<p>Besides <code>Conv2D</code> fused with <code>BatchNorm2D</code> and <code>linear</code> operations, we still need to calculate <code>ReLU</code> and <code>MaxPool</code> operations.</p> <p>We also need to perform post-quantized - Dequantize and requantize the activations - Dequantize the weight</p>"},{"location":"2025sp/lab3/#quantization","title":"Quantization","text":""},{"location":"2025sp/lab3/#quantization-method","title":"Quantization Method","text":"<p>In order to simplify the hardware implementation, we use layerwise symmetric uniform quantization for all layers in Lab 1.</p> <p>Refer to Lab1 HackMD for more details if unfamiliar with this quantization method.</p> <p>Here are the data types for inputs, weights, biases, outputs, and partial sums:</p> input/output activation weight bias/psum Data type uint8 int8 int32 <p>Note that: 1.  The scaling factor of bias is the product of input's scale and weight's scale. 2.  The rounding method is truncation instead of round-to-nearest.</p>"},{"location":"2025sp/lab3/#quantization_1","title":"Quantization","text":"\\[ \\tag{1} \\begin{align} \\bar x &amp;= \\text{clamp} \\left( \\left\\lfloor \\frac{x}{s_x} \\right\\rceil + 128, 0, 255 \\right) \\in \\mathbb{Z}_\\text{uint8}^{\\dim(x)} \\\\ \\bar w &amp;= \\text{clamp} \\left( \\left\\lfloor \\frac{w}{s_w} \\right\\rceil, -128, 127 \\right) \\in \\mathbb{Z}_\\text{int8}^{\\dim(w)} \\\\ \\bar b &amp;= \\text{clamp} \\left( \\left\\lfloor \\frac{b}{s_x s_w} \\right\\rfloor, -2^{31}, 2^{31}-1 \\right) \\in \\mathbb{Z}_\\text{int32}^{\\dim(b)} \\\\ \\bar y &amp;= \\text{clamp} \\left( \\left\\lfloor \\frac{y}{s_y} \\right\\rceil + 128, 0, 255 \\right) \\in \\mathbb{Z}_\\text{uint8}^{\\dim(y)} \\end{align} \\] <p>The notation \\(\\mathbb{Z}^N\\) denotes a vector space of dimension \\(N\\) where all elements (or components) are integers. See also Cartesian product.</p> <p>where the scaling factors are calaulated by</p> \\[ \\tag{2} \\begin{align} s_x = \\frac{2 \\max(|x_\\min|, |x_\\max|)|}{255} \\in \\mathbb{R}_\\text{float32} \\\\ s_w = \\frac{2 \\max(|w_\\min|, |w_\\max|)|}{255} \\in \\mathbb{R}_\\text{float32} \\\\ s_y = \\frac{2 \\max(|y_\\min|, |y_\\max|)}{255} \\in \\mathbb{R}_\\text{float32} \\end{align} \\]"},{"location":"2025sp/lab3/#dequantization","title":"Dequantization","text":"<p>The original values can be approximated by dequantizing the quantized numbers.</p> \\[ \\tag{3} \\begin{align} x &amp;\\approx s_x (\\bar x - 128) \\\\ w &amp;\\approx s_w \\bar w \\\\ b &amp;\\approx s_x s_w \\bar b \\\\ y &amp;\\approx s_y (\\bar y - 128) \\end{align} \\]"},{"location":"2025sp/lab3/#quantized-computation-operations","title":"Quantized Computation Operations","text":"<p>Quantized computation operations require dequantization of inputs to a higher-precision representation before performing arithmetic computations. Once the operations are completed, the results are requantized to restore them to the quantized format, ensuring that the output remains compatible as the input for the next layer, allowing seamless execution of subsequent computations.</p> <p>To enable hardware to perform computations directly on quantized bits, we factor out the scaling factor using the following mathematical transformation. This yields an equivalent expression where the scaling factor can be uniformly applied during post-processing, allowing computations to be carried out in <code>int8</code> format.</p>"},{"location":"2025sp/lab3/#quantized-scalar-multiplication","title":"Quantized Scalar Multiplication","text":"<p>With \\(\\dim(x) = \\dim(w) = \\dim(y) = 1\\) in Eq. 1, multiplication between two quantized numbers are:</p> \\[ y = x \\cdot w \\] \\[ s_y (\\bar y - 128) = s_x (\\bar x - 128) \\cdot s_w \\bar w \\] \\[ \\bar y = \\text{clanmp} \\left( \\frac{s_x s_w}{s_y} (\\bar x - 128) \\bar w + 128 , 0, 255\\right) \\]"},{"location":"2025sp/lab3/#quantized-matrix-multiplication","title":"Quantized Matrix Multiplication","text":"\\[ y_i = b_i + \\sum_j x_j \\cdot w_{ji} \\] <p>can be approximated as the following using Eq. 3</p> \\[ \\begin{align} s_y (\\bar y_i - 128) &amp;= s_x s_w \\bar b_i + \\sum_j s_x (\\bar x_j - 128) \\cdot s_w \\bar w_{ji} \\\\ &amp;= s_x s_w (\\bar b_i + \\sum_j (\\bar x_j - 128) \\cdot \\bar w_{ji}) \\end{align} \\] <p>then we get</p> \\[ \\tag{4} \\bar y_i = \\text{clamp} \\left( \\frac{s_x s_w}{s_y} (\\bar b_i + \\sum_j (\\bar x_j - 128) \\cdot \\bar w_{ji}) + 128, 0, 255 \\right) \\]"},{"location":"2025sp/lab3/#quantized-linear-layer","title":"Quantized Linear Layer","text":"\\[ y_i = b_i + \\sum_j x_j \\cdot w_{ji} \\] \\[ s_y (\\bar y_i - 128) = s_x s_w (\\bar b_i + \\sum_j (\\bar x_j - 128) \\cdot \\bar w_{ji}) \\] \\[ \\tag{5} \\bar y_i = \\text{clamp} \\left( \\underbrace{     \\frac{s_x s_w}{s_y}     \\overbrace{         (\\bar b_i + \\sum_j (\\bar x_j - 128) \\cdot \\bar w_{ji})     }^{\\text{only int32 operations}}}_{\\text{float32 operations involved} } + 128, 0, 255 \\right) \\]"},{"location":"2025sp/lab3/#quantized-convolution-layer","title":"Quantized Convolution Layer","text":"\\[ y_{m,~e,~f} = b_{m} + \\sum_c \\sum_r \\sum_s x_{c,~Ue+r,~Uf+s} \\cdot w_{m,~c,~r,~s}^T \\] \\[ \\tag{6} \\bar y_{m,~e,~f} = \\text{clamp} \\left( \\underbrace{     \\frac{s_x s_w}{s_y}     \\overbrace{         (\\bar b_m + \\sum_{c,~r,~s} (\\bar x_{c,~Ue+r,~Uf+s} - 128) \\cdot \\bar w_{m,~c,~r,~s}^T)     }^{\\text{only int32 operations}}}_{\\text{float32 operations involved} } + 128 , 0, 255\\right) \\]"},{"location":"2025sp/lab3/#hardware-friendly-quantization-design","title":"Hardware-Friendly Quantization Design","text":""},{"location":"2025sp/lab3/#power-of-two-approximation-for-scaling-factors","title":"Power-of-Two Approximation for Scaling Factors","text":"<p>We use dyadic quantization method in Lab1, so that we get the scaling factor:</p> \\[ s \\approx \\frac{b}{2^c} \\text{ , where } b, c \\in \\mathbb{Z} \\] <p>With \\(b = 1\\) in dyadic quantization, we further get power-of-two quantization:</p> \\[ s \\approx \\frac{1}{2^c} = 2^{-c} \\text{ , where } c \\in \\mathbb{Z} \\] <p>The quantized convolution operation can be approximated as:</p> \\[ \\tag{7} \\bar y_{m,~e,~f} = \\left(     (\\bar b_m + \\sum_{c,~r,~s} (\\bar x_{c,~Ue+r,~Uf+s} - 128) \\cdot \\bar w_{m,~c,~r,~s}^T)         \\gg \\frac{s_x s_w}{s_y} \\right) + 128 \\]"},{"location":"2025sp/lab3/#efficient-handling-for-zero-points","title":"Efficient Handling for Zero Points","text":"\\[ 128 = 1000\\_0000_2 \\] <p>Adding or subtracting 128 is equivalent to toggling the most significant bit (MSB) of an 8-bit unsigned integer, which can also be achieved by XORing 128 with that integer.</p> \\[ \\begin{align} \\bar x_j - 128 &amp;= \\bar x_j \\oplus 128 = \\{\\sim \\bar x[7],\\ \\bar x[6:0]\\} \\\\ \\bar y_j + 128 &amp;= \\bar y_j \\oplus 128 = \\{\\sim \\bar y[7],\\ \\bar y[6:0]\\} \\end{align} \\] \\[ \\text{where} \\oplus \\text{is the bitwise XOR operation.} \\] <p>The matrix multiplication can be further expressed as:</p> \\[ \\tag{8} \\bar y_{m,~e,~f} = \\text{clanmp} \\left[ \\left( \\left(     (\\bar b_m + \\sum_{c,~r,~s} (\\bar x_{c,~Ue+r,~Uf+s} \\oplus 128) \\cdot \\bar w_{m,~c,~r,~s}^T)         \\gg \\frac{s_x s_w}{s_y} \\right) \\oplus 128 \\right) ,0 , 255\\right] \\]"},{"location":"2025sp/lab3/#relu","title":"ReLU","text":"<p>ReLU is an abbreviation for Rectified Linear Unit. $$ \\operatorname{ReLU} = \\max(0, x) $$</p> <p>ReLU is the most commonly used nonlinear activation function in CNNs due to its computational efficiency, ability to mitigate the vanishing gradient problem, and promotion of sparsity in network activations.</p> <p>Unlike sigmoid<sup>[1]</sup>  or tanh<sup>[2]</sup>, ReLU involves only a simple thresholding operation, significantly reducing computational complexity. Its non-saturating property enables effective gradient propagation in deep networks, facilitating faster convergence. Additionally, ReLU induces sparsity by setting negative activations to zero, enhancing feature selectivity and regularization. See ReLU's wikipedia for more details.</p>"},{"location":"2025sp/lab3/#note","title":"Note","text":""},{"location":"2025sp/lab3/#1-sigmoid-function","title":"[1] sigmoid function","text":"<p>It is a smooth, S-shaped function that maps any real-valued input to the range \\((0,1)\\). The sigmoid function is particularly useful for probabilistic interpretations, as its output can be interpreted as a probability. It remains widely used in binary classification tasks and as a gating function in architectures such as LSTMs. $$ \\sigma = \\frac{1}{1+e^{-x}} $$</p>"},{"location":"2025sp/lab3/#2-tanh-function","title":"[2] tanh function","text":"<p>The hyperbolic tangent function <code>tanh</code> is a smooth, S-shaped function that maps any real-valued input to the range  \\((\u22121,1)\\). The function is odd-symmetric \\(tanh(\u2212x)=\u2212tanh(x)\\) and exhibits asymptotic behavior, approaching -1 as \\(x \\rightarrow -\\infty\\) and 1 as \\(x \\rightarrow \\infty\\). Unlike the sigmoid function, <code>tanh(x)</code> is zero-centered, meaning that its output is centered around 0, which often facilitates faster convergence in deep learning applications. Due to its saturating properties,  <code>tanh(x)</code> is prone to vanishing gradient issues when used in deep networks. Nevertheless, it remains widely employed in neural networks, particularly in recurrent architectures such as LSTMs and GRUs, where it serves as an activation function in hidden state updates. $$ \\tanh(x) = \\frac{e^x - e<sup>{-x}}{e</sup>x + e^{-x}} $$</p>"},{"location":"2025sp/lab3/#maxpool","title":"MaxPool","text":"<p>Max pooling is a widely used downsampling operation in CNNs that reduces spatial dimensions while retaining the most salient features. It is defined as:</p> \\[ y_{i,j} = \\max_{(m,n) \\in R_{i,j}} x_{m,n} \\] <p>where \\(R_{i,j}\\) represents the receptive field (e.g., a \\(k\\times k\\) region), and \\(x_{m,n}\\) are the input values within this region. Max pooling preserves the strongest activations by selecting the maximum value in each window, enhancing translational invariance and improving computational efficiency. It is commonly applied with a \\(2 \\times 2\\) window and a stride of 2, reducing feature map size while mitigating overfitting. However, it may discard finer details, making it less suitable for tasks requiring precise spatial information.</p> <p>for example:</p> \\[ X = \\begin{bmatrix} 1 &amp; 3 &amp; 2 &amp; 1 \\\\ 4 &amp; 8 &amp; 6 &amp; 2 \\\\ 3 &amp; 5 &amp; 7 &amp; 9 \\\\ 2 &amp; 4 &amp; 6 &amp; 8 \\\\ \\end{bmatrix} \\] <p>Resulting pooled output:</p> \\[ Y = \\begin{bmatrix} 8 &amp; 6 \\\\ 5 &amp; 9 \\\\ \\end{bmatrix} \\]"},{"location":"2025sp/lab3/#hardware-architecture","title":"Hardware Architecture","text":""},{"location":"2025sp/lab3/#controller","title":"Controller","text":"<p>The Controller in Eyeriss acts as the central unit that orchestrates the entire execution process. It ensures that different modules operate in a synchronized manner and that the correct data is processed at the right time. The key responsibilities of the Controller include:</p>"},{"location":"2025sp/lab3/#logic-control","title":"Logic Control","text":"<p>Managing the overall execution sequence of the hardware.</p>"},{"location":"2025sp/lab3/#module-activation","title":"Module Activation","text":"<p>Deciding when each module, such as the Processing Element (PE) Array and Post-Processing Unit (PPU), should start execution.</p>"},{"location":"2025sp/lab3/#data-routing","title":"Data Routing","text":"<p>Controlling the movement of data between memory, processing elements, and post-processing units.</p> <p>Assigning Tag IDs to allow data to be multicast to multiple designated PEs, ensuring that shared data is efficiently distributed across the array.</p>"},{"location":"2025sp/lab3/#glb-data-management","title":"GLB Data Management","text":"<ul> <li>Determining which data should be swapped in and out of GLB.</li> <li>Ensuring efficient data reuse by preloading required feature maps, weights, and intermediate results.</li> <li>Managing data transfer between external DRAM and GLB, optimizing memory bandwidth utilization.</li> </ul>"},{"location":"2025sp/lab3/#glb-memory-usage-calculation","title":"GLB Memory Usage Calculation","text":"<p>Calculation Order: NCHW</p> \\[ \\text{ifmap usage} = 1 * (q * r) * (e + R - 1) * (F + S - 1) * \\text{1 byte} \\] \\[ \\text{filter usage} = (p * t) * (q * r) * R * S* \\text{1 byte} \\] \\[ \\text{bias usage} = (p * t) * \\text{4 byte} \\] \\[ \\text{opsum usage} = 1 * m * e * F \\] \\[ \\text{GLB total size} = 64 \\text{ KiB} \\] \\[ \\tag{9} \\text{GLB total size} \\le \\text{ifmap usage} + \\text{filter usage} + \\text{bias usage} + \\text{opsum usage} \\] <p>Among these parameters, <code>p, q, r, t, e, R, S, F, GLB total size</code> are constants, allowing <code>m</code> to be calculated using Equation (9).</p>"},{"location":"2025sp/lab3/#axi-protocol","title":"AXI Protocol","text":""},{"location":"2025sp/lab3/#mmio-memory-mapped-io","title":"MMIO (Memory-Mapped I/O)","text":"<p>Memory-Mapped I/O (MMIO) is a common mechanism in ASIC and embedded system design that allows peripherals to be accessed using standard memory read and write instructions.</p> <p>In MMIO, peripheral registers are mapped to specific memory addresses, enabling the CPU to interact with devices such as DMA controllers, AI accelerators, and communication interfaces through load/store operations instead of special I/O instructions.</p> <p>This approach simplifies hardware design, improves data transfer efficiency. MMIO transactions are typically non-cacheable to ensure real-time data consistency, and memory barriers may be required to enforce execution order. This method is widely used in high-performance SoC architectures, where CPUs configure peripherals by writing to control registers, while peripherals report status and events through MMIO reads.</p>"},{"location":"2025sp/lab3/#memory-mapped-io-vs-port-mapped-io","title":"Memory-Mapped I/O v.s Port-Mapped I/O","text":"Feature MMIO PMIO Access Method Uses standard load/store instructions Uses dedicated I/O instructions Address Space Shares the same memory address space as RAM Uses a separate I/O address space Performance Faster Slower Use Cases DMA, AI Accelerator, GPU UART"},{"location":"2025sp/lab3/#axi-protocol_1","title":"AXI Protocol","text":""},{"location":"2025sp/lab3/#introduction","title":"Introduction","text":"<p>The AXI (Advanced eXtensible Interface) Protocol is a high-performance communication protocol within ARM's AMBA (Advanced Microcontroller Bus Architecture), widely used in SoC (System on Chip) designs to facilitate efficient data transfer between processors, memory controllers, and peripherals.</p>"},{"location":"2025sp/lab3/#five-independent-channels","title":"Five Independent Channels","text":"<p>AXI operates with five independent channels enabling simultaneous read and write transactions for improved performance. - AR (Address Read) - R (Read Data) - AW (Address Write) - W (Write Data) - and B (Write Response)</p>"},{"location":"2025sp/lab3/#brust-mode","title":"Brust Mode","text":"<p>The protocol supports burst-based transactions, allowing multiple data transfers per request, and offers three burst types: Fixed, Incrementing, and Wrapping, optimizing memory access efficiency. Key features include support for unaligned data transfers, separate read/write channels, and optional out-of-order execution.</p>"},{"location":"2025sp/lab3/#ready-valid-handshake","title":"Ready-Valid Handshake","text":"<p>The protocol uses handshake signals (VALID and READY) to synchronize data flow, ensuring reliable communication.</p>"},{"location":"2025sp/lab3/#conclusion","title":"Conclusion","text":"<p>AXI is highly scalable, supporting data widths from 32-bit to 512-bit, making it ideal for processor-memory interactions, DMA transfers, AI accelerators, GPUs, and FPGA designs. Its low-latency, high-throughput, and flexible nature makes it a preferred choice in modern embedded and high-performance computing systems.</p>"},{"location":"2025sp/lab3/#dma-direct-memory-access","title":"DMA (Direct Memory Access)","text":"<p>In ASIC design, the DMA directly writes data to the GLB (Global Buffer), providing efficient and low-latency memory access to optimize the data flow for local computation resources.</p> <p>This architecture is commonly used in AI accelerators, image processors, and deep learning inference engines, ensuring that compute units can stably retrieve data and enhance computational efficiency.</p> <p>The DMA controller supports high-throughput data transfer and accelerates data movement using AXI Burst Mode.</p> <p>Additionally, since the data is quantized to 8-bit, a specialized memory ordering is required when storing data in the GLB. Therefore, the DMA supports two modes for handling 8-bit and 32-bit data transfers separately.</p> <p>For more details, see the Wikipedia</p>"},{"location":"2025sp/lab3/#practice","title":"Practice","text":""},{"location":"2025sp/lab3/#data-transfer-by-handshake","title":"Data Transfer by Handshake","text":""},{"location":"2025sp/lab3/#what-is-handshake","title":"What is Handshake?","text":"<p>A handshake is a fundamental mechanism used in digital communication to ensure reliable data transfer between two parties. It establishes synchronization between the sender and receiver, ensuring that data is transmitted only when both sides are ready. Handshaking protocols are widely used in computer networks, peripheral interfaces, and bus protocols such as <code>AXI</code>, <code>SPI</code>, and <code>I\u00b2C</code>.</p> <p>In a typical handshake, control signals are exchanged between the sender and receiver before actual data transmission. This prevents data loss, ensures flow control, and optimizes system efficiency.</p>"},{"location":"2025sp/lab3/#ready-valid-handshake-common-in-axi-protocols","title":"Ready-Valid Handshake (Common in AXI Protocols):","text":"<p> - A widely used mechanism in hardware interfaces where the sender asserts a valid signal when data is available, and the receiver asserts a ready signal when it is ready to consume the data. - The transaction is completed when both valid and ready signals are asserted simultaneously. - In this lab, we use this for our handshake mechanism - For more details, please visit this website.</p>"},{"location":"2025sp/lab3/#eyeriss-pe-array","title":"Eyeriss PE Array","text":"<p>Tip</p> <p>You have to use <code>generate</code> block to generate PE, X-Bus, Y-Bus, MC.</p> <p>These are the shape parameters and mapping parameters defined by Eyeriss. They will be used directly in the following sections, so students must familiarize themselves with them to better understand the document.</p> <p>Shape Parameter</p> Parameter Description N batch size of 3D fmaps M # of 3D filters / # of ofmap channels C # of ifmap/filter channels H/W ifmap plane height/width R/S filter plane height/width E/F ofmap plane height/width <p>Mapping Parameter</p> Parameter Description m number of ofmap channels stored in the GLB n number of ifmaps used in a processing pass e width fo the PE set p number of filters processed by a PE set q number of channels processed by a PE set r number of PE sets that process different channels in the PE array t number of PE sets that process different filters in the PE array"},{"location":"2025sp/lab3/#testbench-guildline","title":"Testbench Guildline","text":"<ul> <li> <p>The testbench for this lab is written using Verilator.   For a detailed tutorial on Verilator, please refer to the Lab 0.4 HackMD handout.</p> </li> <li> <p>This testbench requires two additional files:</p> </li> <li> <p>define.svh <code>./include/define.svh</code>     This file contains macro definitions and is included during Verilator compilation, making the defined macros directly accessible.     If you need to add more macro definitions, you can modify this file accordingly.</p> </li> <li> <p>config.h <code>./testbench/config_*.h</code>     This file stores information about each test case, including test parameters and data locations, allowing the testbench to read the necessary data and perform complete computations.</p> </li> <li> <p>After running the simulation, you can obtain the waveform file at the following path <code>/wave/*.vcd</code> .     &gt; Refer to the\u00a0Lab 0.4 Verilator Tutorial\u00a0for instructions on how to open and inspect the waveform file to verify your design.</p> </li> </ul>"},{"location":"2025sp/lab3/#pe","title":"PE","text":""},{"location":"2025sp/lab3/#load-ifmap","title":"Load Ifmap","text":"<p>When loading the ifmap, you must first subtract the zero-point. Since we use uint8 symmetric quantization, the zero-point is 128 For details, refer to the Lab1 HackMD handout.</p> <p>You should adopt a more hardware-friendly approach to replace this subtraction. Otherwise, points may be deducted accordingly.</p>"},{"location":"2025sp/lab3/#activation-of-pe","title":"Activation of PE","text":"<p>The PE computes a 1-D convolution primitive. The PE waits for the testbench to assert <code>PE_en</code>. Once <code>PE_en</code> is high, <code>PE_config</code> becomes valid, and the PE reads its contents to determine the information for the current 1-D convolution computation. Afterward, the testbench starts sending and retrieving data through a handshake mechanism. The process continues until the computation is complete.</p> <p>The following is the definition of <code>PE_config</code></p> <p></p> <p><code>mode = 0</code> means <code>CONV</code> <code>mode = 1</code> means <code>FC Layer</code></p>"},{"location":"2025sp/lab3/#io-ports","title":"I/O ports","text":"<pre><code>module PE (\n    input clk,\n    input rst,\n    input PE_en,\n    input [`CONFIG_SIZE-1:0] i_config,\n    input [`DATA_BITS-1:0] ifmap,\n    input [`DATA_BITS-1:0] filter,\n    input [`DATA_BITS-1:0] ipsum,\n    input ifmap_valid,\n    input filter_valid,\n    input ipsum_valid,\n    input opsum_ready,\n    output logic [`DATA_BITS-1:0] opsum,\n    output logic ifmap_ready,\n    output logic filter_ready,\n    output logic ipsum_ready,\n    output logic opsum_valid\n);\n</code></pre>"},{"location":"2025sp/lab3/#testbench-behavior","title":"Testbench Behavior","text":"<p>If a 1-D convolution needs to be computed as shown below:</p>"},{"location":"2025sp/lab3/#pe-array","title":"PE Array","text":""},{"location":"2025sp/lab3/#io-ports_1","title":"I/O ports","text":"<pre><code>module PE_array #(\n    parameter NUMS_PE_ROW = `NUMS_PE_ROW,\n    parameter NUMS_PE_COL = `NUMS_PE_COL,\n    parameter XID_BITS = `XID_BITS,\n    parameter YID_BITS = `YID_BITS,\n    parameter DATA_SIZE = `DATA_BITS,\n    parameter CONFIG_SIZE = `CONFIG_SIZE\n)(\n    input clk,\n    input rst,\n\n    /* Scan Chain */\n    input set_XID,\n    input [`XID_BITS-1:0] ifmap_XID_scan_in,\n    input [`XID_BITS-1:0] filter_XID_scan_in,\n    input [`XID_BITS-1:0] ipsum_XID_scan_in,\n    input [`XID_BITS-1:0] opsum_XID_scan_in,\n    // output [XID_BITS-1:0] XID_scan_out,\n\n    input set_YID,\n    input [`YID_BITS-1:0] ifmap_YID_scan_in,\n    input [`YID_BITS-1:0] filter_YID_scan_in,\n    input [`YID_BITS-1:0] ipsum_YID_scan_in,\n    input [`YID_BITS-1:0] opsum_YID_scan_in,\n    // output logic [YID_BITS-1:0] YID_scan_out,\n\n    input set_LN,\n    input [`NUMS_PE_ROW-2:0] LN_config_in,\n\n    /* Controller */\n    input [`NUMS_PE_ROW*`NUMS_PE_COL-1:0] PE_en,\n    input [`CONFIG_SIZE-1:0] PE_config,\n    input [`XID_BITS-1:0] ifmap_tag_X,\n    input [`YID_BITS-1:0] ifmap_tag_Y,\n    input [`XID_BITS-1:0] filter_tag_X,\n    input [`YID_BITS-1:0] filter_tag_Y,\n    input [`XID_BITS-1:0] ipsum_tag_X,\n    input [`YID_BITS-1:0] ipsum_tag_Y,\n    input [`XID_BITS-1:0] opsum_tag_X,\n    input [`YID_BITS-1:0] opsum_tag_Y,\n\n    /* GLB */\n    input GLB_ifmap_valid,\n    output logic GLB_ifmap_ready,\n    input GLB_filter_valid,\n    output logic GLB_filter_ready,\n    input GLB_ipsum_valid,\n    output logic GLB_ipsum_ready,\n    input [DATA_SIZE-1:0] GLB_data_in,\n\n    output logic GLB_opsum_valid,\n    input GLB_opsum_ready,\n    output logic [DATA_SIZE-1:0] GLB_data_out\n\n);\n</code></pre>"},{"location":"2025sp/lab3/#insturction","title":"Insturction","text":"<pre><code>make pe%\n</code></pre>"},{"location":"2025sp/lab3/#testbench-behavior_1","title":"Testbench Behavior","text":"<p>If a 2-D convolution needs to be computed as shown below:</p> <p>In addtion to PE array, you need to implement the following submodules to construct it.</p>"},{"location":"2025sp/lab3/#gingon","title":"GIN/GON","text":""},{"location":"2025sp/lab3/#io-ports_2","title":"I/O ports","text":"<pre><code>module GIN (\n    input clk,\n    input rst,\n\n    // Slave SRAM &lt;-&gt; GIN\n    input GIN_valid,\n    output logic GIN_ready,\n    input [`DATA_BITS - 1:0] GIN_data,\n\n    /* Controller &lt;-&gt; GIN */\n    input [`XID_BITS - 1:0] tag_X,\n    input [`YID_BITS - 1:0] tag_Y,\n\n    /* config */\n    input set_XID,\n    input [`XID_BITS - 1:0] XID_scan_in,\n    input set_YID,\n    input [`YID_BITS - 1:0] YID_scan_in,\n\n    // Master GIN &lt;-&gt; PE\n    input [`NUMS_PE_ROW * `NUMS_PE_COL - 1:0] PE_ready,\n    output logic [`NUMS_PE_ROW * `NUMS_PE_COL - 1:0] PE_valid,\n    output logic [`DATA_BITS - 1:0] PE_data\n);\n</code></pre>"},{"location":"2025sp/lab3/#bus","title":"Bus","text":""},{"location":"2025sp/lab3/#io-ports_3","title":"I/O ports","text":"<pre><code> module GIN_Bus #(\n    parameter NUMS_SLAVE = `NUMS_PE_COL,\n    parameter ID_SIZE = `XID_BITS\n) (\n    input clk,\n    input rst,\n\n   // Master I/O\n    input [ID_SIZE-1:0] tag,\n    input master_valid,\n    input [`DATA_BITS-1:0] master_data,\n    output logic master_ready,\n\n   // Slave I/O\n    input [NUMS_SLAVE-1:0] slave_ready,\n    output logic [NUMS_SLAVE-1:0] slave_valid,\n    output logic [`DATA_BITS-1:0] slave_data,\n\n    // Config\n    input set_id,\n    input [ID_SIZE-1:0] ID_scan_in,\n    output logic [ID_SIZE-1:0] ID_scan_out\n );\n</code></pre>"},{"location":"2025sp/lab3/#multicast-controller","title":"Multicast Controller","text":""},{"location":"2025sp/lab3/#io-ports_4","title":"I/O ports","text":"<pre><code>module GIN_MulticastController #(\n    parameter ID_SIZE = `XID_BITS\n    )(\n    input clk,\n    input rst,\n\n    input set_id,\n    input [ID_SIZE - 1:0] id_in,\n    output reg [ID_SIZE - 1:0] id,\n\n    input [ID_SIZE - 1:0] tag,\n\n    input valid_in,\n    output logic valid_out,\n    input ready_in,\n    output logic ready_out\n);\n</code></pre>"},{"location":"2025sp/lab3/#testbench-behavior_2","title":"Testbench Behavior","text":"<p>The following slides illustrate how the testbench sends the config ID to the bus.</p>"},{"location":"2025sp/lab3/#instruction","title":"Instruction","text":"<p>There are six testbenches for different mapping parameter.</p> para array0 array1 array2 array3 array4 array5 F 16 32 16 16 16 16 e 8 8 8 4 4 16 p 4 4 4 4 4 4 q 4 3 4 4 4 4 r 1 1 2 2 1 1 t 2 2 1 2 4 1 <pre><code>make array%\n</code></pre>"},{"location":"2025sp/lab3/#post-processing-unit_1","title":"Post Processing Unit","text":"<p>In the post-processing unit (PPU), you must implement three operations: <code>PostQuant</code>, <code>ReLU</code>, and <code>MaxPool</code>. Whether <code>MaxPool</code> needs to be computed depends on <code>maxpool_en</code>.</p> <p>You can think of the PPU as an always-on module.</p>"},{"location":"2025sp/lab3/#postquant","title":"PostQuant","text":""},{"location":"2025sp/lab3/#scaling-factor","title":"Scaling Factor","text":"<p>Based on the result calculated in (8), we know that Quantized Convolution can be equivalently expressed by this equation.</p> <p>If \\(\\frac{s_x \\times s_w}{sy} = 2^n\\) the testbench will send the integer value <code>n</code> to the module.</p>"},{"location":"2025sp/lab3/#zero-point","title":"Zero Point","text":"<p>Opposite to the PE operation, when requantizing to uint8, you need to add the zero-point.</p> <p>Similarly, you cannot use an adder for this operation; instead, you must adopt a more hardware-friendly approach.</p> <p>Otherwise, points may be deducted accordingly.</p>"},{"location":"2025sp/lab3/#clamping","title":"Clamping","text":"<p>In post-quantization, after scaling and zero-point adjustment, the output values may exceed the valid range for<code>uint8</code> (i.e., <code>0</code> to <code>255</code>). Therefore, clamping is necessary to ensure that the final output remains within the acceptable range.</p> \\[ x = \\text{clamp} \\left( x, 0, 255 \\right) \\] <p>To implement hardware-friendly clamping, we avoid using explicit comparators or conditional branches, as they introduce extra logic and may increase latency. Instead, we leverage bitwise operation that are more suitable for synthesis in hardware.</p>"},{"location":"2025sp/lab3/#maxpool_1","title":"Maxpool","text":"<ul> <li>MaxPool is required the testbench will provide the first data sample when maxpool_init is asserted, followed by three additional data samples in consecutive cycles. The testbench will then read the output data in the next cycle.</li> <li>MaxPool is not required the testbench will read the output data in the cycle following the data input.</li> </ul>"},{"location":"2025sp/lab3/#io-ports_5","title":"I/O ports","text":"<pre><code>module PPU (\n    input clk,\n    input rst,\n    input [`DATA_BITS-1:0] data_in,\n    input [5:0] scaling_factor,\n    input maxpool_en,\n    input maxpool_init,\n    input relu_sel,\n    input relu_en,\n    output logic[7:0] data_out\n);\n</code></pre> <p><code>relu_sel</code> select whether to output the data from <code>post_quant</code> or <code>from max_pool</code> <code>*_en</code>  indicates whether the submodule is enabled.</p>"},{"location":"2025sp/lab3/#instruction_1","title":"Instruction","text":"<p>There are three testbenches.</p> <pre><code>make ppu%\n</code></pre>"},{"location":"2025sp/lab4/","title":"Lab 4 - Runtime and Performance Profiling","text":""},{"location":"2025sp/lab4/#overview","title":"Overview","text":"<p>In this lab, we will implement a software driver to enable integration and operation of the previously designed hardware accelerator with the host system. Through mechanisms such as memory-mapped I/O or interrupts, we establish control and data communication between the CPU and the accelerator. The driver acts as a critical interface layer between the runtime system and the hardware accelerator, exposing low-level register or memory access as high-level software APIs. The runtime, which orchestrates the overall inference flow, leverages the driver to configure the accelerator, transfer data, and retrieve results. This separation of concerns allows the runtime to focus on model-level control logic, while the driver handles hardware-specific interactions. This architecture enables us to evaluate the performance gains provided by the accelerator during real model inference. Additionally, we investigate the feasibility and effectiveness of optimization techniques for accelerating inference in the absence of hardware acceleration. Through these experiments and analyses, we aim to gain a comprehensive understanding of how hardware acceleration and optimization affect model inference performance under different scenarios, thereby providing practical insights for future system design and deployment.</p> <p></p>"},{"location":"2025sp/lab4/#lab-41-device-driver","title":"Lab 4.1 - Device Driver","text":"<p>Driver is a special type of software responsible for enabling the operating system to control hardware devices. It acts like a translator or bridge between the operating system and the hardware. The operating system itself does not directly manipulate hardware; instead, it uses drivers to instruct the hardware what to do.</p>"},{"location":"2025sp/lab4/#why-do-we-need-driver","title":"Why Do We Need Driver ?","text":""},{"location":"2025sp/lab4/#the-os-doesnt-understand-hardware-languages","title":"The OS Doesn\u2019t Understand Hardware Languages","text":"<p>Each hardware device (such as printers, keyboards, network cards) has its own specific control mechanisms. The operating system cannot directly handle these differences. Drivers are responsible for translating OS commands into signals that the hardware can understand, and then returning responses from the hardware back to the OS.</p>"},{"location":"2025sp/lab4/#supporting-various-hardware-with-one-os","title":"Supporting Various Hardware with One OS","text":"<p>With drivers, no matter which manufacturer produces the hardware, as long as a compatible driver exists, the OS can control the device. This means the OS does not need to be rewritten for each different hardware.</p>"},{"location":"2025sp/lab4/#software-perspective","title":"Software Perspective","text":"<ul> <li>A driver is a piece of low-level system software, usually running within the kernel space of the OS.</li> <li>It provides a standard interface (API) for applications or the OS to call (e.g., to read or send data).</li> <li>It also handles hardware interrupts, such as notifying the OS when \"data is ready\" or \"operation completed.\"</li> </ul>"},{"location":"2025sp/lab4/#hardware-perspective","title":"Hardware Perspective","text":"<ul> <li>The driver reads from and writes to control registers of the hardware (e.g., through memory-mapped I/O).</li> <li>It controls hardware behavior based on configurations (e.g., starting data transfer, resetting the device).</li> <li>It also facilitates data movement, such as transferring data from the hardware into main memory (RAM).</li> </ul>"},{"location":"2025sp/lab4/#communication-between-cpu-and-peripheral-device","title":"Communication between CPU and Peripheral Device","text":"<p>In modern computer systems, the CPU needs to exchange data with various peripheral devices, such as keyboards, network interfaces, graphics cards, and storage devices. These devices are not directly hardwired to the CPU for control. Instead, they are accessed through well-designed interface mechanisms, including: * Memory-Mapped I/O (MMIO) * Port-Mapped I/O (also known as I/O-mapped I/O) * Interrupt</p>"},{"location":"2025sp/lab4/#mmio-memory-mapped-io","title":"MMIO (Memory-Mapped I/O)","text":"<p>MMIO is a design method where hardware device registers are mapped into the system\u2019s memory address space. We use this method to control our DLA.</p> <ul> <li>Device registers are accessed using normal memory instructions (<code>load</code>, <code>store</code>).</li> <li>Example:     <pre><code>  *(volatile uint32_t*)0xFFFF0000 = 0x1; // write MMIO device\n</code></pre> <pre><code>  uint32_t value = *(volatile uint32_t*)0xFFFF0000; // read MMIO device\n</code></pre></li> </ul>"},{"location":"2025sp/lab4/#port-mapped-io-isolated-io","title":"Port-Mapped I/O (Isolated I/O)","text":"<p>Devices are controlled via a separate I/O address space using specific CPU instructions (e.g., <code>in</code>, <code>out</code> on x86).</p> <ul> <li>Registers are not part of memory space.</li> <li>Smaller address space (e.g., 64KB).</li> </ul>"},{"location":"2025sp/lab4/#interrupt","title":"Interrupt","text":"<p>Interrupts allow devices to asynchronously notify the CPU of events. The CPU pauses execution, runs an Interrupt Service Routine (ISR), then resumes. Benefits: - Avoids polling - Enables timely response to I/O events</p> <p>Typical Use Cases: - Timer ticks - I/O completion (e.g., DMA, keyboard) - Network packet arrival</p>"},{"location":"2025sp/lab4/#software-hardware-co-design-framework","title":"Software-Hardware Co-design Framework","text":"<p>In real embedded systems, software (runtime) typically interacts with hardware through Memory-Mapped I/O (MMIO) or device drivers, enabling indirect access to registers or peripherals. However, in this lab, we do not have access to actual hardware. Instead, we use Verilator to translate Verilog RTL into cycle-accurate C++ models, generating a Verilated model for simulation.</p> <p>The issue arises because Verilated models expose only low-level signal interfaces (e.g., <code>.clk</code>, <code>.rst</code>, <code>.data_in</code>, <code>.data_out</code>), unlike real hardware that provides register-based MMIO access. As a result, in order to control the Verilated hardware, the runtime must directly manipulate the internal C++ objects generated by Verilator. This tight coupling requires the runtime to conform to the structure and naming conventions of the Verilator testbench, which is not modular and hinders portability.</p> <p>To address this problem, we introduce the Hardware Abstraction Layer (HAL) in this lab.</p>"},{"location":"2025sp/lab4/#purpose-and-benefits-of-hal","title":"Purpose and Benefits of HAL","text":"<ul> <li> <p>Hardware Interface Abstraction   HAL wraps the Verilated model and exposes an interface that resembles real hardware, such as <code>read_reg(addr)</code> and <code>write_reg(addr, value)</code>. This allows the runtime to remain agnostic to whether it is interacting with an RTL simulation or actual FPGA hardware.</p> </li> <li> <p>Unified Runtime Codebase   With HAL in place, a single runtime implementation can be reused across both simulation and real hardware environments without any modification.</p> </li> <li> <p>Modularity via Runtime Library   On top of HAL, a Runtime Library can be built to provide high-level operations (e.g., launching computation, setting parameters, fetching results). This allows the main application to focus purely on control logic, improving software modularity, maintainability, and portability.</p> </li> </ul> <p>Question</p> <p>Can we record the hardware runtime information like cycles, memory access time?</p> <p>Yes, we provide info counter in the HAL.</p>"},{"location":"2025sp/lab4/#hal-info-counter","title":"HAL info counter","text":"<p>The runtime_info structure is designed to record execution-related metrics during hardware simulation or emulation. It provides useful insights into the system's behavior, performance, and memory access patterns. Below is a detailed explanation of each field:</p> include/hal/hal.hpp<pre><code>/**\n * @struct runtime_info\n * @brief Stores runtime performance metrics for a hardware simulation or\n * execution.\n *\n * This structure holds information about execution cycles, elapsed time, and\n * memory operations during a hardware simulation or a specific computation.\n */\nstruct runtime_info {\n    uint32_t elapsed_cycle;  ///&lt; Number of cycles elapsed during execution.\n    uint32_t elapsed_time;   ///&lt; Total elapsed time (e.g., in nanoseconds).\n    uint32_t memory_read;    ///&lt; Total number of memory read operations.\n    uint32_t memory_write;   ///&lt; Total number of memory write operations.\n};\n</code></pre> <p>Warning</p> <p>The HAL counters do not simulate software-level counters; instead, they function more like debug counters that are typically embedded in hardware during design for debugging and analysis purposes. However, in our hardware implementation, we did not allocate dedicated registers for such counters. Instead, we supplement these counters within the HAL, allowing the driver to access them during simulation or emulation.</p> <p>Question</p> <p>Our simulated CPU uses 64-bit addresses, but our memory-mapped peripherals (MMIO) only occupy a small 32-bit region.</p> <ul> <li> <p>How do we bridge the address space between the CPU and simulated devices?</p> </li> <li> <p>Why can't we directly access <code>ARADDR_M</code> in a 64-bit simulation environment?</p> </li> </ul>"},{"location":"2025sp/lab4/#hal-simple-mmu-mapping-32-bit-axi-addresses-in-a-64-bit-simulation","title":"HAL Simple MMU: Mapping 32-bit AXI Addresses in a 64-bit Simulation","text":"<p>In our simulation environment, the host system uses a 64-bit virtual memory space, while the AXI bus operates with 32-bit addresses. This mismatch can cause address mapping issues, as AXI requests may not directly correspond to valid host memory addresses.</p> <p>To resolve this, we implement a simple MMU mechanism within the HAL. By capturing the high 32 bits of the HAL instance\u2019s address (<code>vm_addr_h</code>), we can reconstruct valid 64-bit addresses by combining them with 32-bit AXI addresses:</p> src/hal/hal.cpp<pre><code>/* HAL Constructor */\nHardwareAbstractionLayer::HardwareAbstractionLayer(uint32_t baseaddr,\n                                                   uint32_t mmio_size) {\n    this-&gt;vm_addr_h = ((uint64_t)(this) &amp; 0xffffffff00000000);\n    this-&gt;baseaddr = baseaddr;\n    this-&gt;mmio_size = mmio_size;\n</code></pre> <p></p> <p>Warning</p> <p>If the mapping address crosses the 32-bit peripheral space boundary, invalid access may occur. This happens because the host's 64-bit address space cannot safely simulate memory outside the defined 32-bit region.</p> <p></p> <pre><code>If need, the MMU eed to be optimized in the future,  but in this lab, 4GB address space is enough for simulations.\n</code></pre>"},{"location":"2025sp/lab4/#support-mmio-write-in-hal-with-axi-interface","title":"Support MMIO write in HAL with AXI interface","text":"src/hal/hal.cpp<pre><code>bool HardwareAbstractionLayer::memory_set(uint32_t addr, uint32_t data) {\n    if (device == NULL) {\n        fprintf(stderr, \"[HAL] device is not init yet.\\n\");\n    }\n</code></pre> <p>When a write request is issued via HAL, the first step is to verify whether the provided address falls within the valid MMIO (Memory-Mapped I/O) region. This region starts at <code>baseaddr</code> and spans a range defined by <code>mmio_size</code>, meaning a valid address must lie within the interval <code>[baseaddr, baseaddr + mmio_size]</code>. If the address fails this check (i.e., it is out of bounds), HAL will immediately return false, indicating that the operation is invalid and halting any further processing.</p> src/hal/hal.cpp<pre><code>#ifdef DEBUG\n    fprintf(stderr, \"[HAL memory_set] (0x%08x) 0x%08x \\n\", addr, data);\n#endif\n    if (addr &lt; baseaddr || addr &gt; baseaddr + mmio_size) {\n#ifdef DEBUG\n        fprintf(stderr,\n                \"[HAL ERROR] address 0x%08x is not in device MMIO range.\\n\",\n                addr);\n#endif\n        return false;\n    }\n</code></pre> <p>Following a successful address check, HAL proceeds with the AXI4 protocol, using three separate channels to complete a full write transaction. The first is the Address Write (AW) channel. HAL sets the target address into <code>AWADDR_S</code> and asserts <code>AWVALID_S</code> to signal that a valid address is being presented. The system then waits for the counterpart (typically an interconnect or slave device) to assert <code>AWREADY_S</code>, indicating readiness to accept the address. Once <code>AWREADY_S</code> is high, HAL advances one clock cycle and de-asserts <code>AWVALID_S</code>, completing the address phase.</p> src/hal/hal.cpp<pre><code>    // send write address\n    device-&gt;AWID_S = 0;\n    device-&gt;AWADDR_S = addr;\n    device-&gt;AWLEN_S = 0;    // unused\n    device-&gt;AWSIZE_S = 0;   // unused\n    device-&gt;AWBURST_S = 0;  // unused\n    device-&gt;AWVALID_S = 1;  // valid\n    device-&gt;eval();\n\n    // wait for ready (address)\n    while (!device-&gt;AWREADY_S) {\n        clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n    }\n    clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n    device-&gt;AWVALID_S = 0;\n</code></pre> <p>After sending the address, HAL transitions to the data write phase using the Write Data (W) channel. The data is placed in <code>WDATA_S</code>, and <code>WVALID_S</code> is asserted to indicate the data is valid. Similar to the address phase, HAL waits for WREADY_S to be asserted by the receiver, signaling that it is ready to accept the data. At that point, HAL advances one clock cycle and de-asserts <code>WVALID_S</code>, marking the completion of the data transmission.</p> src/hal/hal.cpp<pre><code>    // send write data\n    device-&gt;WDATA_S = data;\n    device-&gt;WSTRB_S = 0;   // unused\n    device-&gt;WLAST_S = 1;   // single shot, always the last one\n    device-&gt;WVALID_S = 1;  // valid\n    device-&gt;eval();\n\n    // wait for ready (data)\n    while (!device-&gt;WREADY_S) {\n        clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n    }\n    clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n    device-&gt;WVALID_S = 0;\n</code></pre> <p>Once the data is successfully sent, the final step is to receive the write response through the B (Write Response) channel. HAL first asserts <code>BREADY_S</code> to indicate that it is ready to receive a response, then waits for <code>BVALID_S</code> to be asserted by the receiver. Once <code>BVALID_S</code> goes high, HAL reads the value of <code>BRESP_S</code> to determine whether the write was successful. If the response is <code>AXI_RESP_OKAY</code>, the operation is considered successful and HAL returns true; otherwise, it returns false.</p> src/hal/hal.cpp<pre><code>    // wait for write response\n    device-&gt;BREADY_S = 1;\n    device-&gt;eval();\n    while (!device-&gt;BVALID_S) {\n        clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n    }\n    clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n    device-&gt;BREADY_S = 0;\n\n    int resp = device-&gt;BRESP_S;\n    clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n    return resp == AXI_RESP_OKAY;\n}\n</code></pre>"},{"location":"2025sp/lab4/#support-mmio-read-in-hal-with-axi-interface","title":"Support MMIO read in HAL with AXI interface","text":"src/hal/hal.cpp<pre><code>bool HardwareAbstractionLayer::memory_get(uint32_t addr, uint32_t &amp;data) {\n    if (device == NULL) {\n        fprintf(stderr, \"[HAL] device is not init yet.\\n\");\n    }\n</code></pre> <p>When a read request is issued through the HAL, the first step is to verify whether the target address falls within the valid MMIO (Memory-Mapped I/O) region. If the address is out of bounds, the HAL immediately returns false, indicating that the operation is invalid and that no further steps will be taken.</p> src/hal/hal.cpp<pre><code>#ifdef DEBUG\n    fprintf(stderr, \"[HAL memory_get] (0x%08x) \\n\", addr);\n#endif\n    if (addr &lt; baseaddr || addr &gt; baseaddr + mmio_size) {\n#ifdef DEBUG\n        fprintf(stderr,\n                \"[HAL ERROR] address 0x%08x is not in device MMIO range.\\n\",\n                addr);\n#endif\n        return false;\n    }\n</code></pre> <p>If the address passes the check, the HAL proceeds to carry out the read transaction following the AXI4 protocol, which involves a three-phase operation. The first phase uses the AR (Address Read) channel. The HAL sets the target read address to <code>ARADDR_S</code> and asserts <code>ARVALID_S</code>, signaling that a valid read request is being issued. At this point, the system waits for the receiving end (such as an interconnect or slave device) to assert <code>ARREADY_S</code>, indicating readiness to accept the address. Once <code>ARREADY_S</code> is high, the HAL advances one clock cycle and de-asserts <code>ARVALID_S</code>, completing the address transmission.</p> src/hal/hal.cpp<pre><code>    // send read address\n    device-&gt;ARID_S = 0;\n    device-&gt;ARADDR_S = addr;\n    device-&gt;ARLEN_S = 0;    // unused\n    device-&gt;ARSIZE_S = 0;   // unused\n    device-&gt;ARBURST_S = 0;  // unused\n    device-&gt;ARVALID_S = 1;  // valid\n\n    // wait for ready (address)\n    do {\n        clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n    } while (!device-&gt;ARREADY_S);\n    device-&gt;ARVALID_S = 0;\n</code></pre> <p>After the address phase is completed, the HAL transitions to the data reception phase via the R (Read Data) channel. It first asserts <code>RREADY_S</code> to indicate that it is ready to receive data. When the slave asserts <code>RVALID_S</code>, signaling that valid data is available, the HAL immediately reads the value on <code>RDATA_S</code> and de-asserts <code>RREADY_S</code>, completing the data transfer.</p> <p>src/hal/hal.cpp<pre><code>    // wait for valid (data)\n    device-&gt;RREADY_S = 1;\n    do {\n        clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n    } while (!device-&gt;RVALID_S);\n    device-&gt;RREADY_S = 0;\n</code></pre> Finally, the HAL examines the contents of <code>RRESP_S</code>to determine the status of the read operation. If the response is <code>AXI_RESP_OKAY</code>, the read is considered successful, and the retrieved data is stored in the specified variable. The function then returns true. Otherwise, if the response indicates an error, the HAL returns false, signaling that the read operation failed.</p> src/hal/hal.cpp<pre><code>    // get read data\n    data = device-&gt;RDATA_S;\n    int resp = device-&gt;RRESP_S;\n    clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n    return resp == AXI_RESP_OKAY;\n}\n</code></pre> <p>We\u2019ve now covered the HAL info counter and the simple MMU. With this knowledge, we\u2019re ready to integrate them into the DMA handler.</p>"},{"location":"2025sp/lab4/#support-dma-read-in-hal-with-axi-interface","title":"Support DMA read in HAL with AXI interface","text":"<p>In the DMA read handling routine provided by our HAL, the process begins by retrieving the read address and burst length from the master interface. Specifically, the address is obtained from <code>ARADDR_M</code> and the burst length is retrieved from <code>ARLEN_M</code>, which determines the number of data beats to be transferred in this burst operation (for a total of <code>len + 1</code> beats, as the AXI burst length is zero-based).</p> <p>To initiate the transaction, the HAL asserts <code>ARREADY_M</code> to complete the address handshake on the AR channel. After simulating a clock cycle to reflect the timing behavior of the interface, <code>ARREADY_M</code> is de-asserted, signaling that the address phase is complete.</p> src/hal/hal.cpp<pre><code>void HardwareAbstractionLayer::handle_dma_read() {\n    // get read address\n    uint32_t *addr;\n    addr = (uint32_t *)(vm_addr_h | device-&gt;ARADDR_M);\n    uint32_t len = device-&gt;ARLEN_M;\n    device-&gt;ARREADY_M = 1;\n    clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n    device-&gt;ARREADY_M = 0;\n    clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n\n#ifdef DEBUG\n    fprintf(stderr, \"[HAL handle_dma_read] addr = %p, len = %d \\n\", addr,\n            len + 1);\n#endif\n</code></pre> <p>Once the read address is acknowledged, the HAL proceeds to send data through the R channel in a burst manner. For each data beat, the corresponding memory content is fetched from the emulated memory <code>(*(addr + i))</code> and sent through <code>RDATA_M</code>. The data response is always set to <code>AXI_RESP_OKAY</code>, and <code>RID_M</code> is set to 0 by default. Before sending each beat, the HAL simulates a memory access delay by incrementing <code>info.elapsed_cycle</code> and <code>info.elapsed_time</code> accordingly.</p> src/hal/hal.cpp<pre><code>    // send read data (increase mode, burst_size 32bits)\n    device-&gt;RID_M = 0;  // default\n    device-&gt;RRESP_M = AXI_RESP_OKAY;\n\n    for (int i = 0; i &lt;= len; i++) {\n        device-&gt;RDATA_M = *(addr + i);           // send read data\n        info.elapsed_cycle += MEM_ACCESS_CYCLE;  // simulate memory access delay\n        info.elapsed_time += MEM_ACCESS_CYCLE * CYCLE_TIME;\n\n#ifdef DEBUG\n        fprintf(stdout, \"[HAL handle_dma_read] addr = %p, data = %08x \\n\",\n                addr + i, *(addr + i));\n#endif\n        device-&gt;RLAST_M = i == len;  // the last one\n        device-&gt;RVALID_M = 1;\n        device-&gt;eval();\n</code></pre> <p>During the burst, <code>RVALID_M</code> is asserted to indicate that data is valid, and the system waits until the DMA master sets <code>RREADY_M</code>, signaling it is ready to accept the data. Only then is the clock advanced and the next beat prepared. On the final beat of the burst, <code>RLAST_M</code> is asserted to indicate the end of the transfer.</p> src/hal/hal.cpp<pre><code>        // wait DMA ready for next data\n        while (!device-&gt;RREADY_M) {\n            clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n        }\n        clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n        device-&gt;RVALID_M = 0;\n        clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n    }\n    device-&gt;eval();\n</code></pre> <p>At the end of the transaction, the HAL updates the internal counter info.memory_read to reflect the total number of bytes read. This is computed as <code>(len + 1) * sizeof(uint32_t)</code>, and is used for performance monitoring and profiling purposes, such as bandwidth analysis or simulation statistics.</p> src/hal/hal.cpp<pre><code>    // count memory access\n    info.memory_read += sizeof(uint32_t) * (len + 1);\n}\n</code></pre>"},{"location":"2025sp/lab4/#support-dma-write-in-hal-with-axi-interface","title":"Support DMA write in HAL with AXI interface","text":"<p>In the DMA write handling routine provided by our HAL, the process begins by retrieving the write address and burst length from the master interface. Specifically, the address is obtained from <code>AWADDR_M</code> and the burst length is retrieved from <code>AWLEN_M</code>, which determines the number of data beats to be transferred in this burst operation (for a total of <code>len + 1</code> beats, as the AXI burst length is zero-based).</p> <p>To initiate the transaction, the HAL asserts <code>AWREADY_M</code> to complete the address handshake on the AW channel. After simulating a clock cycle to reflect the timing behavior of the interface, <code>AWREADY_M</code> is de-asserted, signaling that the address phase is complete.</p> src/hal/hal.cpp<pre><code>void HardwareAbstractionLayer::handle_dma_write() {\n    // get address\n    uint32_t *addr;\n    addr = (uint32_t *)(vm_addr_h | device-&gt;AWADDR_M);\n    uint32_t len = device-&gt;AWLEN_M;\n    device-&gt;AWREADY_M = 1;\n    clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n    device-&gt;AWREADY_M = 0;\n    clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n\n#ifdef DEBUG\n    fprintf(stderr, \"[HAL handle_dma_write] addr = %p, len = %d \\n\", addr,\n            len + 1);\n#endif\n</code></pre> <p>Once the write address is acknowledged, the HAL proceeds to receive data through the W channel in a burst manner. For each data beat, the corresponding data is obtained from <code>WDATA_M</code> and written into the emulated memory <code>(*(addr + i))</code>. Before processing each beat, the HAL simulates a memory access delay by incrementing <code>info.elapsed_cycle</code> and <code>info.elapsed_time</code> accordingly.</p> src/hal/hal.cpp<pre><code>    // recv write data (increase mode, burst_size 32bits)\n    device-&gt;RID_M = 0;  // default\n\n    for (int i = 0; i &lt;= len; i++) {\n        *(addr + i) = (uint32_t)device-&gt;WDATA_M;  // recv write data\n        info.elapsed_cycle += MEM_ACCESS_CYCLE;  // simulate memory access delay\n        info.elapsed_time += MEM_ACCESS_CYCLE * CYCLE_TIME;\n\n#ifdef DEBUG\n        fprintf(stdout, \"[HAL handle_dma_write] addr = %p, data = %08x \\n\",\n                addr + i, *(addr + i));\n#endif\n        device-&gt;WREADY_M = 1;\n        device-&gt;eval();\n</code></pre> <p>During the burst, <code>WREADY_M</code> is asserted to indicate that the HAL is ready to accept data. The system then waits until the DMA master sets <code>WVALID_M</code>, signaling the data is valid and ready to be written. Once valid data is detected, the clock is advanced and <code>WREADY_M</code> is de-asserted to complete the handshake for that beat.</p> src/hal/hal.cpp<pre><code>        // wait DMA valid for next data\n        while (!device-&gt;WVALID_M) {\n            clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n        }\n        clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n        device-&gt;WREADY_M = 0;\n        clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n    }\n    device-&gt;eval();\n</code></pre> <p>At the end of the data transfer, the HAL sends a write response by asserting <code>BVALID_M</code> and setting <code>BRESP_M</code> to <code>AXI_RESP_OKAY</code>, indicating a successful write operation. <code>BID_M</code> is set to 0 by default. The system then waits until the DMA master asserts <code>BREADY_M</code> to acknowledge the response, after which the response phase is completed and <code>BVALID_M</code> is de-asserted.</p> src/hal/hal.cpp<pre><code>    // recv write response\n    device-&gt;BID_M = 0;\n    device-&gt;BRESP_M = AXI_RESP_OKAY;\n    device-&gt;BVALID_M = 1;\n    device-&gt;eval();\n    while (!device-&gt;BREADY_M) {\n        clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n    };\n    clock_step(device, ACLK, info.elapsed_cycle, info.elapsed_time);\n    device-&gt;BVALID_M = 0;\n    device-&gt;eval();\n</code></pre> <p>Finally, the HAL updates the internal counter <code>info.memory_write</code> to reflect the total number of bytes written. This is computed as <code>(len + 1) * sizeof(uint32_t)</code>, and is used for performance monitoring and profiling purposes, such as bandwidth analysis or simulation statistics.</p> src/hal/hal.cpp<pre><code>    // count memory access\n    info.memory_write += sizeof(uint32_t) * (len + 1);\n}\n</code></pre> <p>The above describes the basic design concept and implementation of the HAL. For more detailed design information and parameters, you can refer to and explore the files located at <code>include/hal/hal.hpp</code> and <code>/src/hal/hal.cpp</code>.</p>"},{"location":"2025sp/lab4/#lab-42-performance-profiling-and-optimization","title":"Lab 4.2 - Performance Profiling and Optimization","text":""},{"location":"2025sp/lab4/#why-we-need-cpu-performance-profiling","title":"Why we need CPU performance profiling ?","text":"<p>In many embedded systems, edge devices, or environments lacking dedicated accelerators such as GPUs or NPUs, developers are often faced with the challenge of running AI models using only CPUs. Given the computationally intensive nature and frequent memory access patterns of AI models, performance can degrade significantly without proper analysis and optimization. Therefore, uncovering potential performance bottlenecks and applying compiler-level optimizations are crucial steps to ensure efficient AI inference on CPU-only platforms. This lecture introduces the use of performance analysis tools such as Valgrind and Cachegrind, along with common CPU-level optimization techniques, to help developers effectively deploy AI applications even in resource-constrained environments without hardware accelerators.</p>"},{"location":"2025sp/lab4/#performance-measurement-tool","title":"Performance Measurement Tool","text":""},{"location":"2025sp/lab4/#valgrind","title":"Valgrind","text":"<p>Valgrind is an open-source debugging and performance analysis tool for Linux, licensed under the GPL. Its suite of tools enables automated detection of memory management and threading issues, significantly reducing the time spent on debugging and enhancing program stability. Additionally, Valgrind provides in-depth profiling capabilities to optimize application performance.</p>"},{"location":"2025sp/lab4/#cachegrind","title":"Cachegrind","text":"<p>Cachegrind is a cache profiling tool that simulates the behavior of the I1 (instruction cache), D1 (data cache), and L2 caches in a CPU, providing precise identification of cache misses in code. It records the number of cache misses, memory accesses, and executed instructions at the source code level, offering detailed analysis at the function, module, and program-wide levels. Supporting programs written in any language, Cachegrind provides comprehensive profiling insights, though it operates at a runtime overhead of approximately 20 to 100 times slower than native execution.</p>"},{"location":"2025sp/lab4/#cachegrind-usage-guide","title":"Cachegrind Usage Guide","text":"<ol> <li>start profiling a program using Cachegrind, run:</li> </ol> <pre><code>valgrind --tool=cachegrind ./your_program\n</code></pre> <ol> <li>The output file cachegrind.out.\\&lt;pid&gt; contains detailed cache statistics. To generate a human-readable report, use:</li> </ol> <pre><code>cg_annotate cachegrind.out.*\n</code></pre>"},{"location":"2025sp/lab4/#advanced-configuration-customizing-cache-parameters","title":"Advanced Configuration - Customizing Cache Parameters","text":"<p>To simulate different CPU cache configurations, use <code>--I1</code>, <code>--D1</code>, and <code>--L2</code> options: <pre><code>valgrind --tool=cachegrind --I1=&lt;size&gt;,&lt;assoc&gt;,&lt;line_size&gt; --D1=&lt;size&gt;,&lt;assoc&gt;,&lt;line_size&gt; --L2=&lt;size&gt;,&lt;assoc&gt;,&lt;line_size&gt; ./your_prog\n</code></pre></p>"},{"location":"2025sp/lab4/#cpu-optimization","title":"CPU Optimization","text":""},{"location":"2025sp/lab4/#compiler-optimization-levels-gcc-clang-examples","title":"Compiler Optimization Levels (GCC / Clang Examples)","text":"Level Description <code>-O0</code> No optimization. Suitable for debugging; preserves source code structure. <code>-O1</code> Enables basic optimizations. Safe and stable. <code>-O2</code> Moderate optimizations. Suitable for general use cases. <code>-O3</code> Aggressive optimizations including loop unrolling and vectorization."},{"location":"2025sp/lab4/#common-optimization-techniques","title":"Common Optimization Techniques","text":"Technique Description Loop Unrolling Expands loop bodies to reduce control overhead and improve instruction-level parallelism. SIMD Vectorization Uses SIMD instructions to process multiple data elements in parallel. Common Subexpression Elimination Eliminates redundant computations by reusing previously computed expressions. Constant Folding / Propagation Computes constant expressions at compile time and substitutes their values. Loop-Invariant Code Motion Moves calculations that do not change within a loop to outside the loop. Memory Allocation &amp; Alignment Using <code>malloc</code> can improve performance by ensuring better cache locality and memory alignment, especially for large or frequently accessed data."},{"location":"2025sp/lab4/#examples","title":"Examples","text":"<p>Loop Unrolling</p> BeforeAfter <pre><code>for (int i = 0; i &lt; 4; i++) {\n    sum += a[i];\n}\n</code></pre> <p>This loop adds one element per iteration and includes overhead from loop control (increment, comparison).</p> <pre><code>sum += a[0];\nsum += a[1];\nsum += a[2];\nsum += a[3];\n</code></pre> <p>By unrolling the loop, we reduce control instructions and increase instruction-level parallelism (ILP), improving CPU pipeline efficiency.</p> <p>SIMD Vectorization</p> <p>Compiler optimize <code>-O3</code>\uff0c the compiler automatically convert this loop into SIMD instructions. Or</p> <pre><code>#include &lt;immintrin.h&gt;\n__m128 vb = _mm_loadu_ps(b); // load 4 float number\n__m128 vc = _mm_loadu_ps(c);\n__m128 va = _mm_add_ps(vb, vc); // SIMD ADD\n_mm_storeu_ps(a, va);          // store\n</code></pre> <p>SIMD processes multiple elements in parallel, reducing the number of arithmetic and memory access instructions.</p> <p>Common Subexpression Elimination</p> BeforeAfter <pre><code>int x = 3 * 4;         // Constant Folding\nconst int a = 5;\nint y = a + 2;         // Constant Propagation\n</code></pre> <p>The expression <code>(x + 2)</code> is calculated twice \u2014 a redundant computation.</p> <pre><code>int x = 12;\nint y = 7;\n</code></pre> <p>The optimized version computes the common expression only once, reducing ALU usage.</p> <p>Constant Folding / Propagation</p> BeforeAfter <pre><code>int y = (x + 2) * (x + 2);\n</code></pre> <p>The compiler can compute these values at compile time and replace them with constants.</p> <pre><code>int t = x + 2;\nint y = t * t;\n</code></pre> <p>Reduces runtime computation and generates simpler machine code.</p> <p>Loop-Invariant Code Motion</p> BeforeAfter <pre><code>for (int i = 0; i &lt; n; i++) {\n    y[i] = x[i] * a * b;\n}\n</code></pre> <p>The expression <code>a * b</code> is invariant across iterations and gets recalculated unnecessarily.</p> <pre><code>int t = a * b;\nfor (int i = 0; i &lt; n; i++) {\n    y[i] = x[i] * t;\n}\n</code></pre> <p>Moves constant computation outside the loop, reducing the number of multiplications.</p> <p>Memory Allocation &amp; Alignment</p> <pre><code>int* data = malloc(1024 * sizeof(int));\n</code></pre> <p>Default <code>malloc</code> alignment may be suboptimal for cache usage and SIMD.</p> <pre><code>posix_memalign((void**)&amp;data, 64, 1024 * sizeof(int));\n</code></pre> <p>Aligning to 64 bytes improves cache line utilization and enables efficient SIMD access \u2014 especially beneficial for large data structures like tensors and matrices.Most modern CPUs use a 64-byte cache line size, so aligning memory to 64-byte boundaries ensures that data fits neatly within a single cache line, avoiding splits across multiple lines and reducing cache misses.</p>"},{"location":"2025sp/lab4/#notes-and-caveats","title":"Notes and Caveats","text":"<ul> <li>Optimizations may make debugging more difficult (e.g., variables may be inlined or eliminated).</li> <li>Programs with undefined behavior may yield unexpected results under optimization.</li> </ul>"},{"location":"2025sp/lab4/#dla-info-record-for-performance-profiling","title":"DLA info Record for Performance Profiling","text":"<p>From the above introduction to the HAL, we can see that the info struct contains four counters. We have therefore exposed functionality to read and reset these counters at the DLA driver level.</p> src/eyeriss/dla/hardware_dla.cpp<pre><code>struct runtime_info get_runtime_info() { return hal.get_runtime_info(); }\nvoid reset_runtime_info() { hal.reset_runtime_info(); }\n</code></pre> src/eyeriss/dla/runtime_dla.cpp<pre><code>void dla_reset_runtime_info() { reset_runtime_info(); }\n</code></pre> <p>Additionally, we provide an API that exports DLA computation information along with the counters recorded by the HAL into a CSV file, facilitating further analysis.</p> src/eyeriss/dla/runtime_dla.cpp<pre><code>void create_dla_info_to_csv(const char *filename) {\n    fprintf(stdout, \"Creating dla info file: %s\\n\", filename);\n    FILE *file = fopen(filename, \"w\");\n    if (!file) {\n        fprintf(stderr, \"Create DLA info file failed.\\n\");\n        return;\n    }\n    fprintf(file,\n            \"Operation,Cycles,Time(ns),Memory read,Memory \"\n            \"write,m,e,p,q,r,t,PAD,U,R,S,C,M,W,H\\n\");\n    fclose(file);\n}\n\nvoid dump_dla_info_to_csv(const char *filename, const char *operation_name,\n                          // mapping parameter\n                          uint32_t m, uint32_t e, uint32_t p, uint32_t q,\n                          uint32_t r, uint32_t t,\n                          // shape parameter\n                          uint32_t PAD, uint32_t U, uint32_t R, uint32_t S,\n                          uint32_t C, uint32_t M, uint32_t W, uint32_t H) {\n    FILE *file = fopen(filename, \"a\");\n    struct runtime_info info = get_runtime_info();\n    fprintf(file, \"%s,\", operation_name);        // Operation\n    fprintf(file, \"%10d,\", info.elapsed_cycle);  // Cycles\n    fprintf(file, \"%10d,\", info.elapsed_time);   // Time (ns)\n    fprintf(file, \"%10d,\", info.memory_read);    // Memory read\n    fprintf(file, \"%10d,\", info.memory_write);   // Memory write\n    fprintf(file, \"%d,%d,%d,%d,%d,%d,\", m, e, p, q, r, t);\n    fprintf(file, \"%d,%d,%d,%d,%d,%d,%d,%d\\n\", PAD, U, R, S, C, M, W, H);\n    fclose(file);\n}\n</code></pre> <p>Tip</p> <p>If the simulation program is compiled with the DLA_INFO macro defined, this profiling feature will be enabled.</p> <p>src/eyeriss/dla/runtime_dla.cpp<pre><code>void dla_init() {\n#ifdef DLA_INFO\n    fprintf(stdout, \"DLA runtime info logging enabled.\\n\");\n    dla_reset_runtime_info();\n    create_dla_info_to_csv(DLA_INFO_CSV);\n#endif\n    hal_init();\n}\n</code></pre> The compilation usage will be mentioned in the homework section.</p>"},{"location":"2025sp/lab4/#practice","title":"Practice","text":"<p>In this lab, you learned about the HAL and basic concepts about device driver. Now it is your turn to implement the driver to support some operations of the DLA and the CPU.</p> <p>In addition, you will implement several APIs to support operations commonly-used in CNNs such as convolution, maxpooling, relu, matmul, etc.</p>"},{"location":"2025sp/lab4/#prerequisites","title":"Prerequisites","text":"<ol> <li>Download the sample code and report template from Moodle and then decompress it.     <pre><code>unzip aoc2025-lab4.zip\n</code></pre></li> <li>Check Verilator version in this lab     We are using Verilator 5.030. You can run the following command to verify it:     <pre><code>verilator --version\n</code></pre>     It will show that.     <pre><code>verilator 5.030\n</code></pre></li> </ol>"},{"location":"2025sp/lab4/#directory-structure","title":"Directory Structure","text":"<p>After unzipped the file downloaded from Moodle, the directory structure will look like below:</p> <pre><code>StudentID_lab4\n\u251c\u2500\u2500 hardware (DLA IP from verilator)\n\u2502   ..........\n\u2502\n\u251c\u2500\u2500 include\n\u2502   \u251c\u2500\u2500 eyeriss\n\u2502   \u2502   \u2514\u2500\u2500 runtime.h\n\u2502   \u2514\u2500\u2500 hal\n\u2502       \u251c\u2500\u2500 axi.hpp\n\u2502       \u2514\u2500\u2500 hal.hpp\n\u251c\u2500\u2500 src\n\u2502   \u251c\u2500\u2500 eyeriss\n\u2502   \u2502   \u251c\u2500\u2500 cpu\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 improve\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 hardware_cpu.c\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 hardware_cpu.h\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 runtime_cpu.c\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 original\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 hardware_cpu.c\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 hardware_cpu.h\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 runtime_cpu.c\n\u2502   \u2502   \u2514\u2500\u2500 dla\n\u2502   \u2502       \u251c\u2500\u2500 hardware_dla.cpp\n\u2502   \u2502       \u251c\u2500\u2500 hardware_dla.h\n\u2502   \u2502       \u2514\u2500\u2500 runtime_dla.cpp\n\u2502   \u2514\u2500\u2500 hal\n\u2502       \u2514\u2500\u2500 hal.cpp\n\u251c\u2500\u2500 test\n\u2502    \u251c\u2500\u2500 cpu\n\u2502    \u2502   \u251c\u2500\u2500 data.h\n\u2502    \u2502   \u251c\u2500\u2500 main.cpp\n\u2502    \u2502   \u2514\u2500\u2500 Makefile\n\u2502    \u2514\u2500\u2500 dla\n\u2502        \u251c\u2500\u2500 dla0\n\u2502        \u2502   \u251c\u2500\u2500 data.h\n\u2502        \u2502   \u251c\u2500\u2500 main.cpp\n\u2502        \u2502   \u2514\u2500\u2500 Makefile\n\u2502        \u251c\u2500\u2500 dla1\n\u2502        \u2502   \u251c\u2500\u2500 data.h\n\u2502        \u2502   \u251c\u2500\u2500 main.cpp\n\u2502        \u2502   \u2514\u2500\u2500 Makefile\n\u2502        \u251c\u2500\u2500 dla2\n\u2502        \u2502   \u251c\u2500\u2500 data.h\n\u2502        \u2502   \u251c\u2500\u2500 main.cpp\n\u2502        \u2502   \u2514\u2500\u2500 Makefile\n\u2502        \u251c\u2500\u2500 dla3\n\u2502        \u2502   \u251c\u2500\u2500 main.cpp\n\u2502        \u2502   \u2514\u2500\u2500 Makefile\n\u2502        \u251c\u2500\u2500 makefile\n\u2502        \u2514\u2500\u2500 Makefile\n\u2514\u2500\u2500 report.md\n</code></pre>"},{"location":"2025sp/lab4/#1-dla-driver","title":"1. DLA driver","text":"<p>In Lab 3, you have already implemented the complete PE-array architecture and the PPU. Now, in this lab, the TAs will provide you with the entire accelerator IP. The complete architecture is shown in the diagram below, which includes sub-modules such as the controller, global buffer, DMA, and MMIO AXI interface. We have already used Verilator to convert it into a C++ library and connected it to the HAL. Your task for this lab is to implement the DLA driver on top of the HAL.</p> <p></p>"},{"location":"2025sp/lab4/#mmio-register-configuration","title":"MMIO register configuration","text":"<p>The following is the MMIO configuration of the accelerator when mapped into memory space. It includes the memory information it needs to operate on, as well as the computation parameters. It is important to note that the enable register should only be set after all parameters have been properly configured. This ensures the accelerator correctly reads the parameters before starting. Therefore, when implementing the driver, you must take the order of register writes into account.</p> <p>We mount the DLA on 0x10040000 ~ 0x10041000 of the system address space (in <code>src/eyeriss/dla/hardware_dla.h</code>)</p> src/eyeriss/dla/hardware_dla.h<pre><code>/* ========================= DLA Register Base Address &amp; Size ========================= */\n#define DLA_MMIO_BASE_ADDR  0x10040000   ///&lt; Base address of the DLA MMIO registers.\n#define DLA_MMIO_SIZE 0x1000  ///&lt; Size of the DLA register memory map.\n</code></pre> <p>The following MMIO registers are all 32-bit (4 bytes) wide. Each address represents the starting location of the corresponding data.</p> Address Offset Name Description <code>0x0</code> <code>enable</code> DLA enable with operation config <code>0x4</code> <code>mapping_param</code> Mapping Parameter <code>0x8</code> <code>shape_param1</code> Shape Parameter <code>0xc</code> <code>shape_param2</code> Shape Parameter 2 <code>0x10</code> <code>ifmap_addr</code> Input feature map address (Starting address in DRAM) <code>0x14</code> <code>filter_addr</code> Filter address (Starting address in DRAM) <code>0x18</code> <code>bias_addr</code> Bias address (Starting address in DRAM) <code>0x1c</code> <code>ofmap_addr</code> Output feature map address (Starting address in DRAM) <code>0x20</code> <code>GLB_filter_addr</code> Global buffer filter address (Starting address in GLB) <code>0x24</code> <code>GLB_opsum_addr</code> Global buffer output sum address <code>0x28</code> <code>GLB_bias_addr</code> Global buffer bias address <code>0x2c</code> <code>ifmap_len</code> Input feature map length <code>0x30</code> <code>ofmap_len</code> Output feature map length <p>You can see the C MACRO define in <code>src/eyeriss/dla/hardware_dla.h</code></p> src/eyeriss/dla/hardware_dla.h<pre><code>/* ========================= DLA Register Offsets ========================= */\n#define DLA_ENABLE_OFFSET 0x0  ///&lt; Offset for enabling/disabling the DLA.\n#define DLA_MAPPING_PARAM_OFFSET   0x4  ///&lt; Offset for setting mapping parameters.\n#define DLA_SHAPE_PARAM1_OFFSET    0x8  ///&lt; Offset for shape parameters (filter, channel).\n#define DLA_SHAPE_PARAM2_OFFSET    0xc  ///&lt; Offset for shape parameters (input size, padding).\n#define DLA_IFMAP_ADDR_OFFSET      0x10   ///&lt; Offset for input feature map address.\n#define DLA_FILTER_ADDR_OFFSET     0x14  ///&lt; Offset for filter weights address.\n#define DLA_BIAS_ADDR_OFFSET       0x18    ///&lt; Offset for bias values address.\n#define DLA_OPSUM_ADDR_OFFSET      0x1c   ///&lt; Offset for output sum buffer address.\n#define DLA_GLB_FILTER_ADDR_OFFSET 0x20  ///&lt; Offset for global filter weights address.\n#define DLA_GLB_OFMAP_ADDR_OFFSET  0x24  ///&lt; Offset for global output feature map address.\n#define DLA_GLB_BIAS_ADDR_OFFSET   0x28                           ///&lt; Offset for global bias values address.\n#define DLA_IFMAP_LEN_OFFSET       0x2c  ///&lt; Offset for input activation length.\n#define DLA_OFMAP_LEN_OFFSET       0x30  ///&lt; Offset for output activation length.\n\n#define DLA_UNDEFINED  ///&lt; Placeholder for undefined registers.\n</code></pre> <p>The details of the bitwise configuration for the first four MMIO registers are as follows. Then, you have to implement them in <code>src/eyeriss/dla/hardware_dla.c</code></p>"},{"location":"2025sp/lab4/#1-dla-enable-with-operation-config","title":"1. DLA enable with operation config","text":"<p>Note</p> <p>The <code>enable</code> register should be the last one when setting MMIO registers.</p> <p></p> <ul> <li>operation<ul> <li><code>0</code> for CONV.</li> <li><code>1</code> (Reserved, this operation did not implemented in the ASIC, you can extend the operation like FCN in the future).</li> </ul> </li> <li>relu, maxpool, en<ul> <li><code>0</code> for disable.</li> <li><code>1</code> for enable.</li> </ul> </li> <li>scale<ul> <li>Quantize and requantization scaling factor represented as a power of two.</li> </ul> </li> </ul>"},{"location":"2025sp/lab4/#2-mapping-parameter-mapping_param-please-refer-to-the-paper-for-the-definition","title":"2. Mapping Parameter (mapping_param) - Please refer to the paper for the definition.","text":""},{"location":"2025sp/lab4/#3-shape-parameter-shape_param1-please-refer-to-the-paper-for-the-definition","title":"3. Shape Parameter (shape_param1) - Please refer to the paper for the definition.","text":"<ul> <li><code>PAD = 1</code>: Only padding of size 1 is supported. Other padding sizes will not be implemented in this lab.</li> </ul>"},{"location":"2025sp/lab4/#4-shape-parameter-2-shape_param2-please-refer-to-the-paper-for-the-definition","title":"4. Shape Parameter 2 (shape_param2) - Please refer to the paper for the definition.","text":"<p>Note: Ensure to account for padding when calculating width (W) and height (H) before writing the value to the register. Add <code>2 * padding</code> to both <code>W</code> and <code>H</code>, then apply the necessary bitwise operations.</p> <p>Note</p>"},{"location":"2025sp/lab4/#memory-write-operation","title":"Memory Write Operation","text":""},{"location":"2025sp/lab4/#using-the-reg_write-function","title":"Using the <code>reg_write</code> Function","text":"<p>In this assignment, you are required to use the function <code>reg_write(uint32_t offset, uint32_t value);</code> provided in <code>hardware_dla.h</code> to write values to a specific memory location.</p>"},{"location":"2025sp/lab4/#function-prototype","title":"Function Prototype","text":"<pre><code>void reg_write(uint32_t offset, uint32_t value);\n</code></pre>"},{"location":"2025sp/lab4/#parameters","title":"Parameters","text":"<ul> <li>offset: The offset of the target memory location (relative to the DLA base address).</li> <li>value: The value to be written.</li> </ul>"},{"location":"2025sp/lab4/#functionality","title":"Functionality","text":"<p>Writes <code>value</code> to the memory location corresponding to <code>DLA_MMIO_BASE_ADDR + offset</code>.</p>"},{"location":"2025sp/lab4/#a-function-call-based-runtime-api-for-common-dnn-operations","title":"A function call-based runtime API for common DNN operations","text":"<p>After completing the low-level MMIO configuration driver, we need to implement appropriate computation APIs for the operations supported by the DLA.</p>"},{"location":"2025sp/lab4/#configuration-for-glb-memory-allocation","title":"Configuration for GLB memory allocation","text":"<p>\"The size of the GLB is configured to 64 KB.\"</p> <p></p> <p>Students can implement the design based on the illustration provided in this diagram. The size of each block can be computed based on the shape parameters and mapping parameters, following the methodology used in the previous lab.</p> <ul> <li>In the file <code>src/eyeriss/dla/runtime/dla.cpp</code>, you need to implement the <code>qconv2d_relu_maxpool</code> and <code>qconv2d_relu</code> functions to correctly activate the DLA. Before configuring the MMIO registers, you must determine the optimal <code>m</code> parameter in the runtime to maximize GLB utilization. As you learned in Lab 2, the value of <code>m</code> affects the size of the output feature map (ofmap) and bias stored in the GLB. Therefore, you need to calculate the largest power-of-two value for <code>m</code> that fits within the available GLB space.</li> <li>The buffer allocation for the input feature map (ifmap) within the GLB does not need to consider padding, as its required space can be fully determined by the mapping and shape parameters.The DLA controller is aware of the padding size and includes corresponding logic to handle it, so this aspect does not need to be explicitly considered. While the previous analysis provides valuable design insights, there may be slight discrepancies during implementation due to various trade-offs.</li> </ul> src/eyeriss/dla/runtime_dla.cpp<pre><code>int qconv2d_relu_maxpool(\n    uint8_t *input_in_DRAM, int8_t *filter_in_DRAM, uint8_t *opsum_in_DRAM,\n    int32_t *bias, uint32_t ofmap_len, uint32_t ifmap_len, uint32_t filter_len,\n    // mapping parameter\n    uint32_t m, uint32_t e, uint32_t p, uint32_t q, uint32_t r, uint32_t t,\n    // shape parameter\n    uint32_t PAD, uint32_t U, uint32_t R, uint32_t S, uint32_t C, uint32_t M,\n    uint32_t W, uint32_t H,\n    uint32_t scale) {  // int32_t scale_factor: merge ifmap and weight and ofmap\n    // scale bit-shift\n\n#ifdef DLA_INFO\n    dla_reset_runtime_info();\n#endif\n    // Calculate m for GLB memory allocation\n    /*! &lt;&lt;&lt;========= Implement here =========&gt;&gt;&gt;*/\n\n\n    // call lower setting functions\n    /*! &lt;&lt;&lt;========= Implement here =========&gt;&gt;&gt;*/\n\n    wait_for_interrupt();\n    dla_stop();\n#ifdef DLA_INFO\n    dump_dla_info_to_csv(DLA_INFO_CSV, \"qconv2d_relu_maxpool\", m, e, p, q, r, t,\n                         PAD, U, R, S, C, M, W, H);\n#endif\n    return 0;\n};\n</code></pre> <p>Info</p> <p>To better understand the source file, you may start by reading the corresponding header file.</p>"},{"location":"2025sp/lab4/#a-different-glb-configuration-from-the-previous","title":"A different GLB configuration from the previous","text":"<p>One key difference in this assignment compared to the previous one is <code>the number of bias values</code> stored in the <code>GLB</code>. In this case, the number of bias values in GLB is <code>m</code>, and storing <code>m</code> biases in the GLB rather than <code>p\u202f\u00d7\u202ft</code> can reduce the number of DRAM accesses, saving handshake time. This change does not affect the number of opsums. Another is the space occupied by the ifmap does not need to account for <code>padding</code>, as it can be determined solely by the mapping parameters and shape parameters.</p>"},{"location":"2025sp/lab4/#dla-testbench-user-guide","title":"DLA Testbench user guide","text":"<p>Note: The implementation of <code>hal.cpp</code> is also required. There are four testcase (dla0 ~ dla3)</p> Testbench test API Note dla0 qconv2d_relu dla1 qconv2d_relu_maxpool dla2 qconv2d_relu_maxpool dla3 qconv2d_relu_maxpool, qconv2d_relu_maxpool_cpu you need to implement original cpu version first in HW4.2 <p>Run the testbench separately.</p> <pre><code>cd test/dla/dla&lt;testcase&gt;\nmake test\n</code></pre> <p>For more Makefile usage</p> <pre><code>cd test/dla/dla&lt;testcase&gt;\nmake &lt;usage&gt;\n</code></pre> <p>We provide a counter in the HAL to record DLA information, which will be dumped into a <code>.csv</code> file when the user enables it by running</p> <pre><code>make all DLA_INFO=1\n</code></pre> <p>Run all testbench without any configurations</p> <pre><code>cd test/dla\nmake test\n</code></pre> <p>Please be patient, as the simulation could take some time to complete. Make sure to take a screenshot of the simulation output if your test passes.</p>"},{"location":"2025sp/lab4/#2-cpu-runtime-api","title":"2. CPU Runtime API","text":"<p>Since there are still some operations that are not supported by the accelerator, it is necessary to implement such operations purely running on the CPU, namely CPU fallback support. Additionally, in certain embedded systems where accelerators are absent, AI model inference must be performed using only the CPU. This motivates us to develop such a library.</p> <p>We\u2019ve learned about memory hierarchy in computer organization, which helps us optimize for memory cache when performing CPU-only computations. Therefore, we need to implement basic algorithms as well as cache-optimized versions, and use tools like Valgrind to measure the effectiveness of these optimizations.</p>"},{"location":"2025sp/lab4/#implementation-requirements","title":"Implementation requirements","text":"<p>Complete the blank sections in the following files:</p> <ul> <li><code>/src/eyeriss/cpu/improve/hardware_cpu.c</code></li> <li><code>/src/eyeriss/cpu/original/hardware_cpu.c</code></li> </ul> <p>Makefiles are also provided under the corresponding directories to compile the library, execute the program, and analysis the performance.</p> <ul> <li> <p>Original Version Implementation</p> <ul> <li>In <code>/src/eyeriss/cpu/original/hardware_cpu.c</code>, the <code>conv</code> and <code>conv_maxpooling</code> functions must be implemented using 6 nested for loops to fully cover the computation across output channel, output height, output width, input channel, filter height, and filter width.</li> <li>The <code>linear</code> and <code>linear_relu</code> functions must be implemented using 2 nested for loops, corresponding to matrix-vector multiplication (output dimension \u00d7 input dimension).</li> <li>Each operation contributes 7.5 points to the overall score.</li> </ul> </li> <li> <p>Improved Version Implementation     You can use loop tiling or any method you want to reduse the cache miss rate and cycles, improve program without complier optimize. We will use cachegrind to simulate the limited cache size.</p> </li> </ul>"},{"location":"2025sp/lab5/","title":"Lab 5 - AI Compiler","text":""},{"location":"2025sp/lab5/#overview","title":"Overview","text":"<p>In this lab, we will implement an AI compiler flow that bridges high-level machine learning models with low-level C code deployment, targeting CPU execution. By integrating TVM, a machine learning compiler framework, with a code generator and pattern fusion engine, we create an end-to-end system capable of compiling models like PyTorch into optimized C code. The compiler serves as a crucial translation and optimization layer, converting models from intermediate representations (Relay) into fused, low-level operator sequences suitable for embedded or resource-constrained environments.</p> <p>The code generation runtime, which controls this compilation pipeline, leverages modular passes such as operator fusion, pattern recognition, and C function call generation to produce executable code that mimics real hardware accelerator deployment. This abstraction allows us to study inference performance and correctness in a software-only context, simulating hardware-constrained environments.</p> <p>Moreover, the lab guides us through implementing key components such as Relay operator fusion, model traversal, and shape-aware codegen logic. Through these implementations, we investigate how compiler-level optimizations\u2014like operator fusion and quantization-aware generation\u2014impact inference performance and portability. Ultimately, this lab equips us with practical insights into how AI compilers serve as a critical infrastructure in deploying deep learning models across heterogeneous platforms.</p>"},{"location":"2025sp/lab5/#lab-50-enviroment-setup","title":"Lab 5.0 - Enviroment Setup","text":"<p>In the upcoming assignment, TVM will be required. Please follow the instructions below to set up the environment.</p> <p>Set up the basic environment and download the required configurations.</p> <pre><code>conda create -n tvm-lab\nconda activate tvm-lab\nconda install python=3.11\nconda install llvmdev=18 -c conda-forge\n</code></pre> <p>[!Warning] If you encounter an issue where Conda is not found, it means Conda has not been installed. The installation guide for Miniconda can be found in Lab 0.</p> <p>We need the CPU version of <code>torchvision</code>, and the TVM version is already installed on the server. Please use the following command to install it to your local conda environment.</p> <pre><code>pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cpu\npip3 install onnx\npip3 install graphviz\npip3 install -e /opt/tvm/python\n</code></pre> <p>After completing the above steps, extract the files from Moodle and place them on the server.</p>"},{"location":"2025sp/lab5/#lab-51-introduction-to-ai-compiler","title":"Lab 5.1 - Introduction to AI Compiler","text":"<p>AI compilers enable the deployment of models from high-level frameworks like TensorFlow and PyTorch onto various hardware platforms such as CPUs, GPUs, and AI accelerators by transforming high-level code into low-level executable code.</p>"},{"location":"2025sp/lab5/#tvm","title":"TVM","text":"<p>One such compiler is TVM, an open-source machine learning compiler framework designed to optimize and deploy deep learning models efficiently across diverse hardware targets. TVM automates the process of translating models into optimized code tailored to specific hardware architectures.</p> <p></p>"},{"location":"2025sp/lab5/#bring-your-own-codegen-byoc","title":"Bring Your Own Codegen (BYOC)","text":"<p>The compilation process begins by converting models from TensorFlow or PyTorch into an Intermediate Representation (IR). In the high-level IR, computations are structured as a computation graph, where each node represents an operation (e.g., matrix multiplication, convolution). This graph is then progressively optimized through multiple stages. Finally, TVM\u2019s code generation (codegen) module translates the optimized IR into low-level C code or other backend-specific code for execution on the target hardware.</p> <p>For more information about BYOC, see How to Bring Your Own Codegen to TVM</p> <p></p> <p>Question: What will Relay look like?</p> <p>After being converted by TVM, the high-level Relay representation will look like this.</p> <pre><code>def @main(%input: Tensor[(1, 3, 32, 32), float32] /* ty=Tensor[(1, 3, 32, 32), float32] span=/quant/QuantizeLinear.input:0:0 */) -&gt; Tensor[(1, 10), float32] {\n  %0 = qnn.quantize(%input, 0.015625f /* ty=float32 span=/quant/Constant_1:0:0 */, 128 /* ty=int32 span=/quant/QuantizeLinear:0:0 */, out_dtype=\"uint8\", axis=1) &gt;/* ty=Tensor[(1, 3, 32, 32), uint8] span=/quant/QuantizeLinear:0:0 */;\n  %1 = cast(%0, dtype=\"uint8\") /* ty=Tensor[(1, 3, 32, 32), uint8] span=/module/conv1/0/Cast:0:0 */;\n  %2 = qnn.dequantize(%1, 0.015625f /* ty=float32 span=/module/conv1/0/Constant:0:0 */, 128 /* ty=int32 span=/module/conv1/0/DequantizeLinear:0:0 */, out_dtype=\"float32\", axis=1) /* ty=Tensor[(1, 3, 32, 32), float32] span=/module/conv1/0/DequantizeLinear:0:0 */;\n  %3 = qnn.dequantize(meta[relay.Constant][0] /* ty=Tensor[(64, 3, 3, 3), int8] span=/module/conv1/0/Constant_2:0:0 */, meta[relay.Constant][1] /* ty=Tensor[(1), float32] span=/module/conv1/0/Constant_3:0:0 */, meta[relay.Constant][2] /* ty=Tensor[(1), int32] span=/module/conv1/0/DequantizeLinear_1:0:0 */, out_dtype=\"float32\", axis=1) /* ty=Tensor[(64, 3, 3, 3), float32] span=/module/conv1/0/DequantizeLinear_1:0:0 */;\n  ...\n</code></pre> <p>It will precisely record each execution step along with detailed information, such as input shape, data type, and more. Once the Relay representation is obtained, optimizations can begin.</p> <p>In this lab, our objective is to take the input Relay, apply fusion techniques to combine specific operators, and generate a fused Relay. Subsequently, we will perform code generation on the fused Relay to produce the output files marked in green, located along the designated path on the right.</p> <p>In typical scenarios, TVM's C code generator is implemented as a C++ class that must be registered within TVM's function registry. After registration, TVM needs to be rebuilt in order to trigger the customized code generator through the Python API using <code>relay.build()</code>. However, TVM also offers an alternative design that allows implementing the code generator directly in Python. In this case, the function can be registered using a decorator. It is important to note that such functions must take a Relay model as input and return a string containing the generated code in either C++ or C.</p> <p>According to the BYOC (Bring Your Own Codegen) framework, in order to produce an executable as part of the standard TVM compilation flow, the custom code generator must conform to the DLPack specification, and data transmission must utilize DLTensor. However, since our approach focuses on an end-to-end code generation flow, we bypass TVM\u2019s generated output files. Instead, we directly invoke our code generator to produce both the model\u2019s C source code and the corresponding binary weight data.</p> <p></p>"},{"location":"2025sp/lab5/#lab-52-optimization","title":"Lab 5.2 - Optimization","text":""},{"location":"2025sp/lab5/#operator-fusion","title":"Operator Fusion","text":"<p>In Lab 4, we implemented the runtime API for the CPU and the driver for the DLA. It's important to note that the operations of these APIs are not purely single operations. Instead, they function more like fused operators within a single function, such as <code>conv2d_relu</code>, <code>conv2d_relu_maxpool</code>, and so on. To handle this, we use TVM to automatically detect patterns from the Relay model graph and fuse these patterns into a single representative node, called a Composite. Next, we annotate these nodes for the specific target (or compiler). Finally, we merge these compiler regions to obtain the Fused Relay model, which is then used by our customized code generator.</p> <p>Fusing multiple operators helps reduce memory accesses, thereby minimizing data movement and improving performance.</p> <p>Based on the <code>merge_composite_pass</code> function in <code>fuse.py</code>, we need to create a pattern_table to identify subgraphs that match specific patterns. Therefore, our goal here is to properly construct the pattern_table.</p> <p>aoc2025-lab5/StudentID_lab5/Python/utils/fuse.py<pre><code>@register_pattern_table(COMPILER_NAME)\ndef  pattern_table():\n    acc_pattern_tables = [\n        (f\"{COMPILER_NAME}.qconv2d_relu_maxpool\", fuse_conv2d_bias_add_relu_max_pool2d()),\n        (f\"{COMPILER_NAME}.qconv2d_relu\", fuse_conv2d_bias_add_relu()),\n        (f\"{COMPILER_NAME}.qlinear_relu\", fuse_dense_add_relu()),\n        (f\"{COMPILER_NAME}.qlinear\", fuse_dense_add()),\n        (f\"{COMPILER_NAME}.flatten\", fuse_flatten()),\n        (f\"{COMPILER_NAME}.quantize\", quantize()),\n        (f\"{COMPILER_NAME}.dequantize\", dequantize()),\n    ]\n    return acc_pattern_tables\n\n# Define the fusion function\ndef merge_composite_pass(mod):\n    with tvm.transform.PassContext(opt_level=3):\n        model_progress = dict()\n        model_progress['origin'] = mod\n        model_progress['MergeComposite'] = transform.MergeComposite(pattern_table())(model_progress['origin'])\n        model_progress['AnnotateTarget'] = transform.AnnotateTarget([COMPILER_NAME])(model_progress['MergeComposite'])\n        model_progress['MergeCompilerRegions'] = transform.MergeCompilerRegions()(model_progress['AnnotateTarget'])\n        model_progress['PartitionGraph'] = transform.PartitionGraph()(model_progress['MergeCompilerRegions'])\n    return (model_progress, model_progress['PartitionGraph'])\n</code></pre> To complete the pattern_table, we need to implement several fusion functions defined within it.</p> <p>Here, let's use <code>fuse_conv2d_bias_add_relu</code> as an example to explain.</p> aoc2025-lab5/StudentID_lab5/Python/utils/fuse.py<pre><code>def fuse_conv2d_bias_add_relu():\n    # Define the pattern for the operations to be fused\n    i = dfp.wildcard()  # Input\n    w = dfp.wildcard()  # Weight\n    b = dfp.wildcard()  # Bias\n    dequantized_i = dfp.is_op(\"qnn.dequantize\")(i,dfp.wildcard(),dfp.wildcard())\n    dequantized_w = dfp.is_op(\"qnn.dequantize\")(w,dfp.wildcard(),dfp.wildcard())\n    dequantized_b = dfp.is_op(\"qnn.dequantize\")(b,dfp.wildcard(),dfp.wildcard())\n    conv2d_op = dfp.is_op(\"nn.conv2d\")(dequantized_i,dequantized_w)\n    bias_add_op = dfp.is_op(\"nn.bias_add\")(conv2d_op, dequantized_b)\n    relu_op = dfp.is_op(\"nn.relu\")(bias_add_op)\n    quantize_op = dfp.is_op(\"qnn.quantize\")(relu_op,dfp.wildcard(),dfp.wildcard())\n    cast_op = dfp.is_op(\"cast\")(quantize_op)  # Assuming requantize is a cast operation\n    return cast_op\n</code></pre> <p>Using <code>fuse_conv2d_bias_add_relu()</code> as an example, note that wildcards in TVM represent patterns that can match any Relay expression.</p> <p>First, we use wildcards to represent the input, weight, and bias tensors.</p> <p>Next, we sequence the operations in the following order: <code>conv2d</code>, followed by <code>bias_add</code>, and then <code>relu</code>.</p> <p>After applying these operations, we quantize the result back to the original data type. Finally, we cast the data to meet the hardware requirements before returning the final output.</p> <ul> <li>The following diagram illustrates the structure of the pattern:</li> </ul> <p></p> <p>Following the fusion and annotation of the model subgraph, the subsequent step involves generating customized C code aligned with the target ASIC driver and its corresponding API.</p>"},{"location":"2025sp/lab5/#lab-53-integration","title":"Lab 5.3 - Integration","text":"<p>To aid understanding, the diagram below depicts the full function call path for the code generation and data generation process as implemented in this lab.</p>"},{"location":"2025sp/lab5/#overview-of-c-codegen-in-python-version","title":"Overview of C codegen in Python version","text":"<p>In this lab, we implement a lightweight C code generator using Python, allowing for faster prototyping without the need to recompile TVM. This Python-based flow simplifies the integration of customized code generation logic while maintaining compatibility with the Relay model structure.</p> <p>The codegen process is organized into three core components under <code>/Python/utils/*.py</code></p> <ul> <li><code>codegen.py</code>: Responsible for generating the full C source code required for model inference. This includes emitting function declarations and layer-wise computations, as well as embedding model weights.</li> <li><code>datagen.py</code>: Handles the transformation of sample input datasets into a runtime-friendly binary format. A lightweight header is added to assist with input parsing during execution.</li> <li><code>note.py</code>: Serves as a configuration and pattern-matching module. It defines wildcard variable names for pattern recognition and maps fused composite functions to their corresponding C code templates.</li> </ul> <p>This modular design not only increases code readability and reusability but also separates concerns clearly, making it easier to maintain and extend the system for different target hardware or model structures.</p> <p></p>"},{"location":"2025sp/lab5/#tvm-relay-external-codegen-c-backend-walkthrough-codegenpy","title":"TVM Relay External Codegen: C Backend Walkthrough - <code>codegen.py</code>","text":"<p>This part provides an explanation of the implementation for an external code generator in TVM targeting C. It demonstrates how to lower a Relay composite function into C code and generate a runtime-loadable module.</p>"},{"location":"2025sp/lab5/#module-overview-and-imports","title":"Module Overview and Imports","text":"<pre><code>import tvm\nimport os\nimport numpy as np\n\nfrom .fuse import COMPILER_NAME\nfrom .fuse import pattern_table\nfrom .note import *\n</code></pre> <p>Here, standard TVM libraries are imported, along with local modules for pattern matching and annotations. The <code>COMPILER_NAME</code> defines the custom compiler target name used by TVM.</p>"},{"location":"2025sp/lab5/#data-structures-output-and-data","title":"Data Structures: Output and Data","text":""},{"location":"2025sp/lab5/#output","title":"Output","text":"<pre><code>class Output(dict):\n    ...\n</code></pre> <p><code>Output</code> is used to store metadata for generated buffers or variables, such as name, data type, copy requirements, and size.</p>"},{"location":"2025sp/lab5/#data","title":"Data","text":"<pre><code>class Data(dict):\n    ...\n</code></pre> <p><code>Data</code> stores information about constants, including their data content and structural metadata.</p>"},{"location":"2025sp/lab5/#abstract-codegen-base-class","title":"Abstract Codegen Base Class","text":"<pre><code>class CodegenCBase(ABC):\n    ...\n</code></pre> <p>This base class provides shared logic for all C-based code generators:</p> <ul> <li>Code string construction with indentation</li> <li>Scope management for nested code</li> <li>Wrapper function generation (<code>generate_backend_c_func</code>)</li> <li>Runtime entry point generation (<code>jit_impl</code>)</li> </ul> <p>The output consists of both an internal kernel function and a wrapper conforming to TVM's external runtime interface.</p>"},{"location":"2025sp/lab5/#codegenc-core-relay-to-c-lowering","title":"CodegenC: Core Relay to C Lowering","text":"<pre><code>class CodegenC(CodegenCBase):\n    ...\n</code></pre> <p>This class handles the traversal of the Relay IR and emits C code. It supports common Relay expressions including <code>Call</code>, <code>Var</code>, <code>Tuple</code>, <code>Constant</code>, and <code>TupleGetItem</code>.</p>"},{"location":"2025sp/lab5/#visiting-expressions","title":"Visiting Expressions","text":"<pre><code>    def visit_expr(self, node):\n        if isinstance(node, Var):\n            return self.visit_var(node)\n        elif isinstance(node, Tuple):\n            return self.visit_tuple(node)\n        elif isinstance(node, TupleGetItem):\n            return self.visit_tuple_get_item(node)\n        elif isinstance(node, Constant):\n            return self.visit_constant(node)\n        elif isinstance(node, Call):\n            return self.visit_call(node)\n        else:\n            return self.visit_expr_default(node)\n</code></pre> <p>Becuase python did not support Overloadding, Relay expressions are dispatched to appropriate handlers depending on their type. For instance:</p> <ul> <li><code>visit_var</code>: Registers variable inputs.</li> <li><code>visit_constant</code>: Processes embedded tensor data.</li> <li><code>visit_call</code>: Handles composite patterns such as convolutions.</li> </ul>"},{"location":"2025sp/lab5/#visiting-call-nodes-composite-operators","title":"Visiting Call Nodes (Composite Operators)","text":"<pre><code>def visit_call(self, call):\n    ...\n</code></pre> <p>This is the main entry point for lowering composite functions into backend-specific C calls. Key steps include:</p> <ol> <li>Check if the call is part of a known composite pattern.</li> <li>Collect and process function arguments.</li> <li>Allocate an output buffer for results.</li> <li>Populate a config dictionary with all necessary parameters.</li> <li>Emit a function call using a registered generator function.</li> </ol> <pre><code>def visit_call(self, call):\n        composite_name = call.op.attrs[\"Composite\"]\n        func_name = composite_name.replace(\".\",\"_\")\n        in_shape = self.get_shape(call.args[0].checked_type)\n\n        if composite_name in PATTERN_TABLE:\n            print(\"[composite trace]\", composite_name, in_shape)\n        else:\n            raise RuntimeError(\"Unrecognized composite\")\n</code></pre> <p>First, obtain which Composite function this Call belongs to. Next, replace \".\" with \"_\" in composite_name to convert it into a C-compatible function name. Finally, retrieve the input shape of this Call.And check whether it exists in our pattern_table to prevent encountering unsupported functions.</p> <p>Next, iterate through all parameters of the Call, checking whether each parameter is a constant (Constant) or a variable (Variable). Store these parameters for later use in generating C code.</p> <p>Please complete the implementation of the trace parts, ensuring that operator information is correctly extracted and the corresponding C code is generated.</p> <pre><code>        # ------------------------------------------------------------\n        # TODO 1: Trace parameters\n        # ------------------------------------------------------------\n        # For each argument in call.args, determine:\n        #   - its mapped name in tvm_auto_args_NOTES[func_name]\n        #   - whether it's a Constant or not\n        #   - if not a constant, use `self.visit_expr(arg)` to visit it\n        # Then fill the `parameters` dict like:\n        #   parameters[\"input\"] = (value, is_const)\n        #\n        # Hint:\n        #   - Use zip(call.args, tvm_auto_args_NOTES[func_name])\n        #   - Use isinstance(arg, Constant)\n        parameters = dict()\n\n        raise NotImplementedError(\"Please implement argument tracing into the 'parameters' dictionary.\")\n\n        # fetch function generator\n        func_gen = tvm_c_func_call_gen.get(func_name,None)\n        if not func_gen:\n            return parameters[\"input\"] # if function generator not exist, just bypass the input.\n</code></pre> <p>Next, set up the output buffer and store the parameters and values into the config for easy access later. Note that buf_idx should be incremented by 1 each time to ensure there are no duplicate buffers. Once the buffers are set, use get_conv_info(call) to retrieve the configuration and store it in the config.</p> <pre><code>        # output buffer\n        # ------------------------------------------------------------\n        # TODO 2: Create output buffer\n        # ------------------------------------------------------------\n        # You need to:\n        #   - Generate a new buffer name using `BUF_PREFIX` and self.buf_idx\n        #   - Get the output buffer size: self.get_size(call)\n        #   - Get the output buffer dtype: self.get_dtype_string(call.checked_type)\n        #\n        # You should generate a line like:\n        #   float* out_0 = (float*)malloc(size * 4);\n        #\n        # Output:\n        #   - out      -&gt; output buffer name\n        #   - out_size -&gt; total number of elements\n        #   - dtype    -&gt; C-style data type\n        raise NotImplementedError(\"Please implement output buffer allocation logic.\")\n\n\n        ### gether the parameter that we need.\n        # conv2d Op info\n        if \"conv2d\" in func_name:\n            config = self.get_conv_info(call)\n            config[\"C\"] = in_shape[1]\n            config[\"H\"] = in_shape[2]\n            config[\"W\"] = in_shape[3]\n        else:\n            config = dict()\n\n        # wildcard info\n        for k in [\"input\",\"weight\",\"bias\"]:\n            # default\n            config[k] = None\n            config[f\"{k}_len\"] = None\n            config[f\"{k}_dtype\"] = None\n            # get parameter\n            param = parameters.get(k, None)\n            if param == None:\n                continue\n            # unpack\n            p,is_const = param\n            if p == None:\n                continue\n            # if it is constant, now can visit it\n            if is_const:\n                p = self.visit_constant(p)[0]\n\n            config[k] = p.name\n            config[f\"{k}_len\"] = p.size\n            config[f\"{k}_dtype\"] = p.dtype\n\n        config[\"output\"] = out\n        config[\"output_len\"] = out_size\n\n        # convert quntize scale\n        for k,(v,is_const) in parameters.items():\n            if \"scale\" in k and is_const:\n                n = v.data.numpy()\n                config[k] = n[0] if n.ndim == 1 else n\n\n        # malloc output buffer\n        buf_create = f\"{dtype}* {out} = ({dtype}*)malloc({out_size * 4});\"\n        self.ext_func_body.append(buf_create)\n\n        # generate c function\n        self.ext_func_body.append(\"\".join(func_gen(config)))\n\n        # free input buffer\n        p,_ = parameters[\"input\"]\n        if BUF_PREFIX in p.name:\n            buf_free = f\"free({p.name});\"\n            self.ext_func_body.append(buf_free)\n\n        output = Output(name=out, dtype=dtype, need_copy=True, size=out_size)\n        return [output]\n</code></pre>"},{"location":"2025sp/lab5/#extracting-convolution-information","title":"Extracting Convolution Information","text":"<p>To extract convolution-related parameters (e.g., padding, kernel size, and number of output channels), the <code>get_conv_info</code> function performs a Breadth-First Search (BFS) over the body of the composite function. It starts by initializing a <code>conv2d_info</code> dictionary with default values and adds the top-level operation (<code>call.op.body</code>) to the <code>op_list</code>. As it iterates through this list, it checks if each node is a Call and whether its operator name is \"<code>nn.conv2d</code>\". If a convolution is found, it extracts and stores the relevant attributes\u2014such as <code>padding</code>, <code>channels</code>, and <code>kernel_size</code>\u2014into the conv2d_info dictionary. This approach ensures that deeply nested operators inside fused composite patterns are correctly identified and their parameters recorded for downstream code generation.</p> <pre><code>    def get_conv_info(self,call):\n        op_list = [call.op.body,]\n\n        conv2d_info = {\n            \"m\":\"DEFAULT_m\",\n            \"e\":\"DEFAULT_e\",\n            \"p\":\"DEFAULT_p\",\n            \"q\":\"DEFAULT_q\",\n            \"r\":\"DEFAULT_r\",\n            \"t\":\"DEFAULT_t\",\n            \"U\":1\n        }\n\n        # BFS\n        while len(op_list) &gt; 0:\n            op = op_list.pop(0)\n\n            # ------------------------------------------------------------\n            # TODO: Extract conv2d attributes from the op node\n            # ------------------------------------------------------------\n            # If the op is nn.conv2d:\n            #   - Extract and store:\n            #       - padding -&gt; conv2d_info[\"PAD\"]\n            #       - channels -&gt; conv2d_info[\"M\"]\n            #       - kernel size (0 and 1) -&gt; conv2d_info[\"R\"], conv2d_info[\"S\"]\n            #   - Also set conv2d_info[\"m\"] = conv2d_info[\"M\"]\n            #\n            # Hint:\n            #   - Use op.op.name to check for \"nn.conv2d\"\n            #   - Use op.attrs[\"padding\"], op.attrs[\"channels\"], etc.\n            #   - You can assume padding and kernel_size are lists/tuples.\n            #\n            # Example:\n            #   conv2d_info[\"PAD\"] = op.attrs[\"padding\"][0]\n            # When done, remove the following line\n            raise NotImplementedError(\"You need to extract attributes for nn.conv2d node.\")\n\n            for node in op.args:\n                if isinstance(node, Call):\n                    op_list.append(node)\n\n        return conv2d_info\n</code></pre> <p>This is essential for generating hardware-aware kernel configurations.</p>"},{"location":"2025sp/lab5/#code-emission-and-constant-handling","title":"Code Emission and Constant Handling","text":"<pre><code>def create_data_reference(self, symbol, const_id, cn):\n    ...\n</code></pre> <p>Each constant is given a unique name and stored for emission into <code>weight.c</code> and <code>weight.h</code>.</p> <p>The <code>visit_constant</code> method also returns an <code>Output</code> object representing the constant as a pointer for runtime use.</p>"},{"location":"2025sp/lab5/#csourcecodegen-final-module-construction","title":"CSourceCodegen: Final Module Construction","text":"<pre><code>class CSourceCodegen(CSourceModuleCodegenBase):\n    ...\n</code></pre> <p>This class wraps everything and produces:</p> <ul> <li><code>code.c</code>: Generated logic for the Relay function.</li> <li><code>weight.c</code>/<code>weight.h</code>: Definitions and initializations for constant data.</li> <li><code>weight.bin</code>: Serialized tensor weights.</li> </ul>"},{"location":"2025sp/lab5/#generating-the-function","title":"Generating the Function","text":"<pre><code>def gen_c_func(self, func):\n    ...\n</code></pre> <p>Calls into <code>CodegenC</code> to perform expression traversal and lower the function. The resulting C source code is appended to the <code>code_stream</code>.</p>"},{"location":"2025sp/lab5/#generating-weight-files","title":"Generating Weight Files","text":"<pre><code>def gen_weight(self, const_vars):\n    ...\n</code></pre> <p>Iterates through all constants, exports their data to:</p> <ul> <li>C array declarations (<code>weight.c</code>)</li> <li>Header file references (<code>weight.h</code>)</li> <li>A binary blob file for deployment (<code>weight.bin</code>)</li> </ul> <p>Special handling is included for DLA platforms that require channel padding to multiples of 4.</p>"},{"location":"2025sp/lab5/#tvm-registration","title":"TVM Registration","text":"<pre><code>@registry.register_func(f\"relay.ext.{COMPILER_NAME}\")\ndef DLA_compiler(ref):\n    ...\n</code></pre> <p>This function is registered under the name <code>relay.ext.&lt;compiler&gt;</code> and invoked automatically when TVM detects a composite pattern assigned to the external compiler.</p>"},{"location":"2025sp/lab5/#output-files","title":"Output Files","text":"<p>Upon execution, the following files are written to <code>./output/</code>:</p> <ul> <li><code>code.c</code>: Core runtime logic</li> <li><code>weight.c</code>: Constant data arrays</li> <li><code>weight.h</code>: Header declarations for constants</li> <li><code>weight.bin</code>: Serialized constant tensors for loading at runtime</li> </ul> <pre><code>self.dump_code(self.weight_c_stream.getvalue(), \"weight\", \"c\")\n</code></pre>"},{"location":"2025sp/lab5/#dataset-preprocessing-for-c-deployment-datagenpy","title":"Dataset Preprocessing for C Deployment - <code>datagen.py</code>","text":"<p>In this section, we walk through the implementation of <code>datagen.py</code>, which prepares dataset samples for use in embedded C environments. Its primary function is to retrieve images from a test dataset and convert them into <code>.h</code> (C header) or <code>.bin</code> (binary) files, making them portable for inference testing on microcontrollers or other low-level platforms.</p>"},{"location":"2025sp/lab5/#data-processing-class-overview","title":"Data Processing Class Overview","text":"<p>We start by defining a class called <code>Dataset_Generator</code>, which handles the core data preprocessing logic. Its responsibilities include:</p> <ul> <li>Loading the test dataset</li> <li>Extracting a specified number of test samples per category</li> <li>Generating C-style <code>.h</code> files</li> <li>Generating binary <code>.bin</code> files</li> </ul> <pre><code>class Dataset_Generator(object):\n    def __init__(self, source, root=\"data\", eval_transform=None):\n        self.root = root\n        self.eval_transform = eval_transform\n        self.test_dataset = source(\n            root=self.root,\n            train=False,\n            download=True,\n            transform=self.eval_transform\n        )\n        self.testloader = DataLoader(self.test_dataset, batch_size=1, num_workers=1, shuffle=False)\n        self.classes = []\n</code></pre>"},{"location":"2025sp/lab5/#fetching-data-by-class","title":"Fetching Data by Class","text":"<p>The <code>fetch_data()</code> method extracts a fixed number of samples (<code>num_data_per_class</code>) from each category in the test dataset and organizes them into a dictionary for later processing.</p> <pre><code>    def fetch_data(self, num_data_per_class):\n        self.classes = self.test_dataset.classes\n        data_dict = dict()\n        for idx, c in enumerate(self.classes):\n            data_dict[c] = []\n            for img, y in self.testloader:\n                if idx == y:\n                    data_dict[c].append(img)\n                if len(data_dict[c]) &gt;= num_data_per_class:\n                    break\n        return data_dict\n</code></pre>"},{"location":"2025sp/lab5/#generating-bin-files","title":"Generating .bin Files","text":"<p>The <code>gen_bin()</code> method saves the extracted image data to a <code>.bin</code> file in a format suitable for embedded inference. The binary structure includes:</p> <ol> <li>Total number of classes</li> <li>Each class name and its length (encoded in UTF-8)</li> <li>Number of samples per class</li> <li>Flattened image size</li> <li>The actual image data in <code>float32</code> format</li> </ol> <pre><code>    def gen_bin(self, output_path, num_data_per_class=10):\n        data_dict = self.fetch_data(num_data_per_class=num_data_per_class)\n\n        with open(output_path, \"wb\") as f:\n            num_classes = len(self.classes)\n            f.write(struct.pack(\"I\", num_classes))  # Total number of classes\n\n            for class_name in self.classes:\n                encoded_name = class_name.encode('utf-8')\n                name_length = len(encoded_name)\n                f.write(struct.pack(\"I\", name_length))\n                f.write(encoded_name)\n\n            first_data_shape = list(data_dict.values())[0][0].shape\n            flattened_size = np.prod(first_data_shape)\n            f.write(struct.pack(\"I\", num_data_per_class))\n            f.write(struct.pack(\"I\", flattened_size))\n\n            for values in data_dict.values():\n                for value in values:\n                    np_array = value.numpy().astype('float32')\n                    f.write(np_array.tobytes())\n</code></pre>"},{"location":"2025sp/lab5/#tvm-function-call-generation-for-quantized-inference-notepy","title":"TVM Function Call Generation for Quantized Inference - <code>note.py</code>","text":"<p>This module automates the generation of C-style function call code for the TVM (Tensor Virtual Machine) compiler. It targets quantized deep learning models and supports operations such as:</p> <ul> <li>Quantized Convolution (<code>QConv2D</code>)</li> <li>Quantized Linear Layer (<code>QLinear</code>)</li> <li>Quantization / Dequantization</li> <li>Variants like ReLU and MaxPooling fused with other ops</li> </ul>"},{"location":"2025sp/lab5/#tvm_auto_args_notes-parameter-schemas-for-tvm-operations","title":"<code>tvm_auto_args_NOTES</code>: Parameter Schemas for TVM Operations","text":"<pre><code>tvm_auto_args_NOTES = {\n    f\"{COMPILER_NAME}_qconv2d_relu_maxpool\": [\n        \"input\",\n        \"input_scale\",\n        \"input_zero_point\",\n        \"weight\",\n        \"weight_scale\",\n        \"weight_zero_point\",\n        \"bias\",\n        \"bias_scale\",\n        \"bias_zero_point\",\n        \"dequantize_scale\",\n        \"dequantize_zero_point\",\n        \"quantize_zero_point\",\n    ],\n    # Additional entries go here\n}\n</code></pre> <p>This dictionary defines the expected argument names for each supported TVM function. These names will be used for:</p> <ul> <li>Identifying input/output tensors</li> <li>Associating quantization parameters (scales, zero points)</li> <li>Mapping configurations for code generation</li> </ul> <p>For example, the <code>qconv2d_relu_maxpool</code> function requires:</p> <ul> <li>Input, Weights, Biases (quantized)</li> <li>Quantization/Dequantization scales and zero points</li> <li>Dequantize/Quantize metadata for rescaling output</li> </ul>"},{"location":"2025sp/lab5/#convert_log-helper-for-quantization-scaling","title":"<code>convert_log</code>: Helper for Quantization Scaling","text":"<pre><code>def convert_log(x):\n    return -int(math.log2(x))\n</code></pre> <p>This helper function calculates <code>-log2(x)</code> as an integer. It\u2019s commonly used to transform floating-point scaling factors into integer log scale shifts, which are more efficient for fixed-point inference on hardware accelerators or microcontrollers.</p>"},{"location":"2025sp/lab5/#tvm_c_func_call_gen-c-code-generators-per-operator","title":"<code>tvm_c_func_call_gen</code>: C Code Generators per Operator","text":"<pre><code>tvm_c_func_call_gen = {\n    f\"{COMPILER_NAME}_qconv2d_relu_maxpool\": lambda config: f\"\"\"\n#ifndef CPU_ONLY\n  qconv2d_relu_maxpool(\n#else\n  qconv2d_relu_maxpool_cpu(\n#endif\n    {config[\"input\"]}, {config[\"weight\"]}, {config[\"output\"]},\n    {config[\"bias\"]}, {config[\"output_len\"]}, {config[\"input_len\"]}, {config[\"weight_len\"]},\n#ifndef CPU_ONLY\n    // Mapping parameters\n    {config[\"m\"]}, {config[\"e\"]}, {config[\"p\"]}, {config[\"q\"]}, {config[\"r\"]}, {config[\"t\"]},\n#endif\n    // Shape parameters\n    {config[\"PAD\"]}, {config[\"U\"]}, {config[\"R\"]}, {config[\"S\"]},\n    {config[\"C\"]}, {config[\"M\"]}, {config[\"W\"]}, {config[\"H\"]},\n    // Quantization scale\n    {convert_log(config[\"input_scale\"] * config[\"weight_scale\"] / config[\"dequantize_scale\"])}\n  );\n\"\"\",\n    # Additional entries go here\n</code></pre> <p>This dictionary maps each TVM operation to a corresponding C-language function call template, dynamically filled using the <code>config</code> dictionary.</p> <p>Each lambda returns a formatted multi-line C function call string, ready to be inserted into generated code. These templates can target: - CPU-only versions (<code>*_cpu</code>) - Normal hardware-accelerated (DLA) versions</p> <p>Success</p> <p>Once all python script are completed, the <code>build_model</code> logic will automatically invoke the appropriate code generation logic from <code>tvm_c_func_call_gen</code>, completing the full compilation and code-emission process for quantized models.</p>"},{"location":"2025sp/lab5/#example-output","title":"Example Output:","text":"<pre><code>qconv2d_relu_maxpool(\n    input, weight, output,\n    bias, output_len, input_len, weight_len,\n    // mapping parameter\n    m, e, p, q, r, t,\n    // shape parameter\n    PAD, U, R, S,\n    C, M, W, H,\n    // quantize scale\n    convert_log(input_scale * weight_scale / dequantize_scale)\n);\n</code></pre>"},{"location":"2025sp/lab5/#lab-54-performance-analysis","title":"Lab 5.4 - Performance Analysis","text":"<p>In this lab, we will explore tools that help analyze and visualize memory usage during program execution. We will use Valgrind\u2019s Massif toolset to profile a simple C program that recursively prints Fibonacci numbers.</p>"},{"location":"2025sp/lab5/#massif-heap-memory-profiler","title":"<code>massif</code>: Heap Memory Profiler","text":"<p>Massif is a heap profiler provided by Valgrind. It helps track memory allocations, identify memory peaks, and analyze memory usage over time.</p> <p>To use Massif, run the following command in the <code>lab</code> directory:</p> <pre><code>make massif\n</code></pre> <p>This will execute the program <code>massif_test</code>, which prints a list of Fibonacci numbers using a recursive function.</p> <p>Massif will trace memory usage at runtime. Internally, it uses the following command:</p> <pre><code>valgrind --tool=massif \\\n  --heap=yes \\\n  --stacks=yes \\\n  --time-unit=i \\\n  --detailed-freq=1 \\\n  --max-snapshots=1000 \\\n  --massif-out-file=massif.out.massif_test ./massif_test\n</code></pre> <p>Example output:</p> <pre><code>==841727== Massif, a heap profiler\n==841727== Command: ./massif_test\n\nFibonacci sequence:\n0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597 2584 4181\nMemory released.\n</code></pre> <p>Note</p> <p>The prefix <code>==841727==</code> refers to the process ID (PID) of the running program.</p>"},{"location":"2025sp/lab5/#ms_print-text-based-memory-usage-graph","title":"<code>ms_print</code>: Text-Based Memory Usage Graph","text":"<p>ms_print is a command-line tool that reads the Massif output and displays a detailed memory usage graph.</p> <p>To generate a visual report:</p> <pre><code>make ms_print\n</code></pre> <p>This will use <code>ms_print</code> to parse the output file <code>massif.out.massif_test</code> and dump the results into <code>massif_output.txt</code>.</p> <p>Here's a sample snippet of the memory usage chart:</p> <pre><code>--------------------------------------------------------------------------------\nCommand:            ./massif_test\nMassif arguments:   --heap=yes --stacks=yes --time-unit=i --detailed-freq=1 --max-snapshots=1000 --massif-out-file=massif.out.massif_test\nms_print arguments: massif.out.massif_test\n--------------------------------------------------------------------------------\n\n\n    KB\n7.219^   #\n     |   #\n     |   #\n     |   #\n     |   #\n     |   #\n     |   #@\n     |   #@\n     |   #@\n     |   #@\n     |   #@\n     |   #@\n     |   #@                                                                 @@\n     |   #@                                                                 @@\n     |   #@                                                                 @@\n     |   #@                                                                 @@\n     |   #@  @@@@@@                                                         @@\n     |   #@ @@@@@@@             @        @            @ @@ @                @@\n     |   #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n     |@@@#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n   0 +-----------------------------------------------------------------------&gt;ki\n     0                                                                   783.4\n\nNumber of snapshots: 533\n Detailed snapshots: [0, 1, 2, 3, 4, 5, 6 (peak), 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532]\n\n--------------------------------------------------------------------------------\n  n        time(i)         total(B)   useful-heap(B) extra-heap(B)    stacks(B)\n--------------------------------------------------------------------------------\n  0              0                0                0             0            0\n00.00% (0B) (heap allocation functions) malloc/new/new[], --alloc-fns, etc.\n\n--------------------------------------------------------------------------------\n  n        time(i)         total(B)   useful-heap(B) extra-heap(B)    stacks(B)\n--------------------------------------------------------------------------------\n  1          1,568              400                0             0          400\n00.00% (0B) (heap allocation functions) malloc/new/new[], --alloc-fns, etc.\n\n... more spanshots\n</code></pre> <p>It includes:</p> <ul> <li>Memory peaks and detailed timeline</li> <li>Number of snapshots (memory samples taken)</li> <li>Stack vs heap usage breakdown</li> </ul>"},{"location":"2025sp/lab5/#massif-visualizer-gui-for-massif-output","title":"<code>massif-visualizer</code>: GUI for Massif Output","text":"<p>massif-visualizer is a graphical interface for visualizing Massif output, making it easier to analyze memory usage trends interactively.</p>"},{"location":"2025sp/lab5/#to-use-massif-visualizer","title":"To use <code>massif-visualizer</code>:","text":"<ol> <li>Ensure X11 forwarding is enabled if you're using SSH.</li> <li>Launch the tool:</li> </ol> <pre><code>massif-visualizer\n</code></pre> <ol> <li>Open the output file:</li> <li>Click Open... </li> <li> <p>Choose <code>massif.out.massif_test</code> </p> </li> <li> <p>View the memory usage graph:    </p> </li> </ol> <p>This visualization helps you easily pinpoint memory-intensive regions and track memory growth over time. There are also gui application called <code>massif-visualizer</code>, it is...</p>"},{"location":"2025sp/lab5/#clean-up","title":"Clean Up","text":"<p>After completing the lab, clean the generated output files by running:</p> <pre><code>make clean\n</code></pre> <p>This removes all intermediate files and prepares the environment for a fresh start.</p>"},{"location":"2025sp/lab5/#practice","title":"Practice","text":"<p>In this lab, you learned how to build a complete AI compiler flow that translates high-level machine learning models into low-level C code suitable for embedded and CPU-only environments. You explored key compiler components such as Relay operator fusion, external code generation, and performance analysis using memory profiling tools like <code>valgrind</code>, <code>massif</code>, and <code>ms_print</code>.</p> <p>In addition, you will implement and complete several essential modules of the compiler pipeline, including:</p> <ul> <li>Writing pattern-matching functions for operator fusion in <code>fuse.py</code>.</li> <li>Completing code generation logic in <code>note.py</code> for different quantized operators.</li> <li>Generating appropriate function calls based on Relay transformations.</li> <li>Performing memory profiling on recursive functions using Massif, and interpreting the results through both CLI (<code>ms_print</code>) and GUI (<code>massif-visualizer</code>).</li> </ul> <p>These tasks are designed to help you understand the end-to-end compilation and deployment process of deep learning models, and how software optimization maps to hardware-aware execution models.</p> <p>By the end of this assignment, you will have hands-on experience with:</p> <ul> <li>Translating Relay models into fused representations</li> <li>Implementing target-specific code generators</li> <li>Profiling and analyzing memory usage at runtime</li> </ul> <p>Make sure to follow the implementation notes and fill in the TODOs as marked by the TAs in each respective file.</p>"},{"location":"2025sp/lab5/#prerequisites","title":"Prerequisites","text":"<p>Before starting the lab, make sure the following setup steps are completed:</p> <ol> <li> <p>Download and extract the lab materials    Get the sample code and report template from Moodle, then decompress the archive:    <pre><code>unzip aoc2025-lab5.zip\n</code></pre></p> </li> <li> <p>Activate the lab environment    Ensure that you are working in the correct Conda environment for this lab:    <pre><code>conda activate tvm-lab\n</code></pre></p> </li> </ol>"},{"location":"2025sp/lab5/#directory-structure","title":"Directory Structure","text":"<p>After unzipped the file downloaded from Moodle, the directory structure will look like below:</p> <pre><code>StudentID_lab5\n\u251c\u2500\u2500 Csource\n\u2502   \u251c\u2500\u2500 color.h\n\u2502   \u251c\u2500\u2500 input.c\n\u2502   \u251c\u2500\u2500 input.h\n\u2502   \u251c\u2500\u2500 utils.c\n\u2502   \u2514\u2500\u2500 utils.h\n\u251c\u2500\u2500 lab\n\u2502   \u251c\u2500\u2500 makefile\n\u2502   \u2514\u2500\u2500 massif_test.c\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 alexnetbn_v2-power2.onnx\n\u251c\u2500\u2500 Python\n\u2502   \u251c\u2500\u2500 build_model.py\n\u2502   \u251c\u2500\u2500 utils\n\u2502   \u2514\u2500\u2500 VisuTVM\n\u251c\u2500\u2500 report.md\n\u251c\u2500\u2500 simulation\n\u2502   \u251c\u2500\u2500 hardware\n\u2502   \u2514\u2500\u2500 software\n\u2514\u2500\u2500 testbench\n    \u251c\u2500\u2500 cpu\n    \u2514\u2500\u2500 dla\n</code></pre>"},{"location":"2025sp/lab5/#1-codegen-with-tvm-compiler","title":"1. Codegen with TVM compiler","text":"<p>Implement the section in the following python files</p> <ol> <li> <p><code>fuse.py</code> fuse.py<pre><code>def fuse_conv2d_bias_add_relu():...\n\ndef fuse_dense_add_relu():...\n\ndef fuse_dense_add():...\n</code></pre></p> </li> <li> <p><code>codegen.py</code> codegen.py<pre><code>def visit_call(self, call):...\n\ndef get_conv_info(self,call):...\n</code></pre></p> </li> <li> <p><code>datagen.py</code></p> </li> <li> <p><code>tvm_c_func_call_gen/note.py</code> tvm_c_func_call_gen/note.py<pre><code>tvm_c_func_call_gen = {\n    ...\n}\n</code></pre></p> </li> </ol> <p>Once you have completed the marked sections above, you can proceed to execute the following steps in order.</p>"},{"location":"2025sp/lab5/#build-the-model","title":"Build the model","text":"<pre><code>make build_model\n</code></pre> <ul> <li>This command will:<ol> <li>load model from onnx format, convert to relay model.</li> <li>parse and fused the relay model, and dump relay model expresion to text file.(Used in visuTVM)</li> <li>Build the model in python script, including traverse composited model, C code generation, weight generation.</li> <li>Parse the CIFAR10 dataset, and convert to custom input binary file.</li> <li>extract the tar generate from TVM, and categorize them into different folders. The Process will traverse the hale model in DFS order, notice that the trace flow is from output layer back to input layer.</li> </ol> </li> </ul>"},{"location":"2025sp/lab5/#visutvm-relay-graph-visualizer","title":"visuTVM: Relay Graph Visualizer","text":"<p><code>visuTVM</code> is a tool used to visualize the structure of a TVM Relay model graph, helping you better understand model transformations during compilation.</p> <p>To generate visualizations of the Relay graph:</p> <pre><code>make visuTVM\n</code></pre> <p>This command produces two SVG images representing the Relay graph:</p> <ul> <li><code>./output/visu_VGG8_relay_ir.svg</code>: The original Relay IR (before the MergeComposite pass)</li> <li><code>./output/visu_VGG8_relay_ir_pass.svg</code>: The Relay IR after pattern fusion and annotation passes</li> </ul>"},{"location":"2025sp/lab5/#2-simulation-and-performance-analysis","title":"2. Simulation and Performance Analysis","text":"<p>In this task, you will analyze the memory usage and runtime performance of the CPU-only version of your model using Massif, a heap memory profiler from Valgrind. Additionally, you will utilize DLA info counters\u2014provided in the Lab 4 runtime library\u2014to evaluate the behavior and efficiency of the simulated accelerator.</p> <p>This dual analysis allows you to compare software-based and hardware-like execution, providing deeper insights into memory bottlenecks and inference performance.</p>"},{"location":"2025sp/lab5/#inference-model-with-cpu-only","title":"Inference model with CPU-only","text":"<p>For quickly demo and test of cpu version:</p> <pre><code>make test_cpu\n</code></pre> <p>you will got a single shot of inference of full model in cpu-only runtime api.</p> <p><pre><code>CC weight.o\nCC input.o\nCC utils.o\nCC runtime_cpu.o\nCC hardware_cpu.o\nCXX main.o\nCXX model.o\nLD main\nmake[1]: Leaving directory '/home/aoc2025/n26130605/work/lab5/testbench/cpu'\n/home/aoc2025/n26130605/work/lab5\nRunning program...\nmake[1]: Entering directory '/home/aoc2025/n26130605/work/lab5/testbench/cpu'\nmkdir -p log\nRun test\n===============[ single test ]===============\nInput file: ../../output/bin/input.bin\nWeight file: ../../output/bin/weight.bin\nClass index: 4\nImage index: 9\n=============================================\nImage Test: 9/10 image class         deer\n\n\n=============================================\n[    airplane]   5.203%\n[  automobile]   0.058%\n[        bird]   0.621%\n[         cat]   0.333%\n[        deer]  20.578%\n[         dog]   0.484%\n[        frog]   0.058%\n[       horse]  71.826%\n[        ship]   0.090%\n[       truck]   0.750%\n=============================================\nmake[1]: Leaving directory '/home/aoc2025/n26130605/work/lab5/testbench/cpu'\n</code></pre> For more config in compiling cpu-only version runtime, move into <code>testbench/cpu</code>, then use <code>make usage</code> for more details about configurations. <pre><code>cd testbench/cpu\nmake usage\n</code></pre> <pre><code>Usage: make [target]\n\nAvailable targets:\n  all                      - Build the project (default target)\n  test     [CLASS][INDEX]  - Run the compiled executable with test input\n  valgrind [CLASS][INDEX]  - Run Valgrind Massif to analyze memory usage\n  test_full                - Run with 100 test input\n  valgrind_full            - Run Valgrind Massifwith 100 test input\n  clean                    - Remove all generated files\n\nEnvironment Variables:\n  CLASS=&lt;num&gt;   - Set class index for testing (default: 4)\n  INDEX=&lt;num&gt;   - Set test index (default: 9)\n</code></pre></p> <p>Notice that it is needed to <code>make clean</code> before any new configuration applied. - <code>make test</code> is the single shot of indecated image. - <code>make test_full</code> will implement 100 images.     <pre><code>================[ full test ]================\nInput file: ../../output/bin/input.bin\nWeight file: ../../output/bin/weight.bin\n=============================================\n'.' is PASS,'&lt;num&gt;' is the wrong prediction\n\n\n=============================================\n[ 0 -        airplane]  . . 2 . . 9 . . . .\n[ 1 -      automobile]  . 9 9 . . . . . . .\n[ 2 -            bird]  . 3 . . . . . . . 5\n[ 3 -             cat]  . . . . . . . . . .\n[ 4 -            deer]  . . . . . 5 . . . 7\n[ 5 -             dog]  . . . . . . . . . 3\n[ 6 -            frog]  . . . . . . . . . 4\n[ 7 -           horse]  . . . . . 3 . . . 4\n[ 8 -            ship]  . . 6 . . . . . . .\n[ 9 -           truck]  . . . . . . . . . .\n\nCorrect/Total: 87/100\n=============================================\n</code></pre></p> <p>Model Accuracy</p> <p>After quantization, it achieves 88.xx% accuracy on the CIFAR-10 dataset. This means that the results seen in this simulation make sense!</p>"},{"location":"2025sp/lab5/#memory-usage-analysis-with-massif","title":"Memory Usage Analysis with Massif","text":"<ul> <li><code>make valgrind</code> and <code>make valgrind_full</code> execute the same tests as <code>make test</code>, but with enhanced memory tracking during runtime.</li> <li>These commands utilize the Valgrind Massif tool to monitor and trace memory usage, saving the output in the <code>massif_out/</code> directory.</li> <li>To visualize and analyze memory usage over time, use massif-visualizer to open the generated output files.</li> </ul>"},{"location":"2025sp/lab5/#inference-model-with-dla","title":"Inference model with DLA","text":"<p>In DLA version test and demo, use <code>make test_dla</code> at top directory will perform a single shot simulation on eyeriss ASIC.</p> <ul> <li>it will takes frew more seconeds to simulation. <pre><code>Run test\n=============================================\nInput file: ../../output/bin/input.bin\nWeight file: ../../output/bin/weight.bin\nClass index: 4\nImage index: 9\n=============================================\nImage Test: 9/10 image class         deer\n\n\n=============================================\n[    airplane]   5.203%\n[  automobile]   0.058%\n[        bird]   0.621%\n[         cat]   0.333%\n[        deer]  20.578%\n[         dog]   0.484%\n[        frog]   0.058%\n[       horse]  71.826%\n[        ship]   0.090%\n[       truck]   0.750%\n=============================================\n</code></pre></li> </ul> <p>Model Misclassification</p> <p>Occasional misclassifications can occur. For example, in the case above, the model mistakenly identified a <code>deer</code> as a <code>horse</code>.</p> <p>Also, for more information and configuraion about DLA version compile and inference is in <code>testbench/dla</code>, use <code>make usage</code> to get details about them.</p> <pre><code>cd testbench/dla\nmake usage\n</code></pre> <pre><code>Usage: make [target]\n\nAvailable targets:\n  all  [DEBUG=?][DLA_INFO=?][USE_VCD=?]      - Build the project (default target)\n  test [CLASS=&lt;num&gt;][INDEX=&lt;num&gt;]            - Run the compiled executable with test input\n  clean      - Remove all generated files\n  nWave      - Launch nWave with logs\n\nEnvironment Variables:\n  DEBUG=1       - Enable debug mode\n  DLA_INFO=1    - Enable DLA info logs\n  USE_VCD=1     - Enable VCD dumping\n  CLASS=&lt;num&gt;   - Set class index for testing (default: 4)\n  INDEX=&lt;num&gt;   - Set test index (default: 9)\n</code></pre> <p>Note</p> <p>Notice that the DLA version did not support <code>test_full</code>, because the 100 images simulation will takes more than one hours, even run on better computer.</p>"},{"location":"2025sp/lab5/#dla-runtime-analysis","title":"DLA runtime analysis","text":"<p>To enable this feature, set the environment variable <code>DLA_INFO=1</code> before running make test. After the test completes, a CSV file will be generated containing statistics and parameters for each task assigned to the DLA.</p> <ol> <li>Run inference on a single image using DLA simulation, and export the statistics as a CSV file. The statistics should include cycle count, time and memory read/write count for each layer. <pre><code>%%{init: {\n    \"themeVariables\": {\n        \"xyChart\": {\n        \"plotColorPalette\": \"#17b55e\"\n        }\n    }\n    }}%%\nxychart-beta\n    title \"Example Chart\"\n    x-axis [\"layer 1\", \"layer 2\", \"layer 3\", \"layer 4\", \"layer 5\"]\n    y-axis \"Y-axis\" 20 --&gt; 300\n    bar [100, 234, 213, 167, 80]</code></pre></li> <li>Compare the results with the predicted values from Lab 2. Do the statistics align with the predictions? If there are discrepancies, where might estimation and actual experimental results diverge? (This is an open-ended question\u2014any reasonable explanation is acceptable.)</li> </ol> <p>VS Code Extension - Markdown Preview Mermaid</p> <p>This extension allows you to preview Mermaid diagrams directly in Markdown files. </p>"},{"location":"2025sp/lab5/#clean-up_1","title":"Clean Up","text":"<ol> <li>To remove output logs and executables in a specific module, run <code>make clean</code> inside <code>testbench/cpu</code> or <code>testbench/dla</code>.</li> <li>To clean up everything, including generated code and testbench executables, run <code>make clean</code> in the root directory of the lab project.</li> </ol>"},{"location":"zh/","title":"FORCE AI","text":"<p>FORCE AI (Fast Optimization for Resource-Constrained and Efficient AI Inference) \u662f\u4e00\u500b\u5c08\u70ba\u6559\u80b2\u76ee\u7684\u6253\u9020\u7684\u958b\u653e\u539f\u59cb\u78bc\u6846\u67b6\uff0c\u5c55\u793a\u5982\u4f55\u5f9e\u96f6\u958b\u59cb\u8a2d\u8a08\u4e00\u500b\u91dd\u5c0d\u7279\u5b9a\u6a21\u578b\u9032\u884c\u6548\u80fd\u512a\u5316\u7684 AI \u52a0\u901f\u5668\u3002\u5c08\u6848\u5167\u5bb9\u6db5\u84cb\u5f9e\u6a21\u578b\u8a13\u7df4\u8207\u91cf\u5316\u3001\u6548\u80fd\u5206\u6790\u3001\u786c\u9ad4\u67b6\u69cb\u8a2d\u8a08\uff0c\u5230\u7de8\u8b6f\u5668\u8207\u57f7\u884c\u968e\u6bb5\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u5e6b\u52a9\u4f60\u7406\u89e3\u4e26\u5be6\u4f5c end-to-end AI \u63a8\u8ad6\u52a0\u901f\u3002</p> <p>\u672c\u6846\u67b6\u4ee5\u53ef\u6559\u5b78\u3001\u53ef\u64f4\u5145\u70ba\u6838\u5fc3\u7cbe\u795e\uff0c\u8a2d\u8a08\u7c21\u6f54\u3001\u6a21\u7d44\u5316\uff0c\u9069\u5408\u7528\u65bc\u8ab2\u7a0b\u5be6\u4f5c\u3001\u81ea\u5b78\u5c08\u6848\u3001\u6216\u4f5c\u70ba\u9032\u968e\u7814\u7a76\u7684\u57fa\u790e\u8d77\u9ede\u3002\u6240\u6709\u5de5\u5177\u8207\u6a21\u7d44\u7686\u57fa\u65bc\u958b\u6e90\u6280\u8853\u69cb\u5efa\uff0c\u5305\u62ec PyTorch\u3001TVM\u3001Verilator \u8207 ONNX\uff0c\u4e26\u642d\u914d\u81ea\u884c\u8a2d\u8a08\u7684 power-of-2 symmetric quantization \u4ee5\u53ca Python-based performance model\uff0c\u7d50\u5408 RTL \u6a21\u64ec\u8207\u786c\u9ad4\u53c3\u6578\u63a2\u7d22\uff0c\u5be6\u73fe\u4ee5\u8a2d\u8a08\u70ba\u5c0e\u5411\u7684 AI \u52a0\u901f\u5668\u958b\u767c\u3002</p>"},{"location":"zh/#_1","title":"\u9069\u5408\u5c0d\u8c61","text":"<ul> <li>AI \u52a0\u901f\u5668\u9818\u57df\u7684\u521d\u5b78\u8005\u3001\u7814\u7a76\u751f\u3001\u5927\u5b78\u9ad8\u5e74\u7d1a\u751f</li> <li>\u5e0c\u671b\u5b78\u7fd2\u5982\u4f55\u4e32\u63a5 AI inference \u8207\u786c\u9ad4\u8a2d\u8a08\u7684\u958b\u767c\u8005</li> <li>\u60f3\u4ee5\u5c0f\u6210\u672c\u5efa\u7acb\u5c6c\u65bc\u81ea\u5df1\u7684\u52a0\u901f\u5668\u5be6\u9a57\u74b0\u5883\u8005</li> </ul>"},{"location":"zh/#_2","title":"\u4f60\u6703\u5b78\u5230","text":"<ul> <li>\u5982\u4f55\u8a13\u7df4\u4e26\u91cf\u5316\u795e\u7d93\u7db2\u8def\u6a21\u578b\u4ee5\u5229\u786c\u9ad4\u90e8\u7f72</li> <li>\u5982\u4f55\u8a2d\u8a08\u8207\u6a21\u64ec\u57fa\u65bc Eyeriss \u7684 CNN \u52a0\u901f\u5668</li> <li>\u5982\u4f55\u4f7f\u7528 Verilator \u505a RTL-level \u6a21\u64ec\u8207\u9a57\u8b49</li> <li>\u5982\u4f55\u5efa\u69cb\u7c21\u6613 inference runtime \u4e26\u4f7f\u7528 TVM \u7de8\u8b6f\u5668\u9032\u884c code generation</li> <li>\u5982\u4f55\u5f9e\u524d\u7aef\u6a21\u578b\u9700\u6c42\u5c0e\u51fa\u9069\u7576\u7684\u786c\u9ad4\u8a2d\u8a08\u898f\u683c</li> </ul>"}]}